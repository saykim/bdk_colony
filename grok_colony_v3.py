import os
from datetime import datetime
from ultralytics import YOLO
import gradio as gr
import torch
from PIL import Image, ImageDraw, ImageFilter, ImageEnhance
import numpy as np
import cv2
import pandas as pd
from pathlib import Path
import time

# íƒ€ì… íŒíŒ… ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ì½”ë“œ (ì‹¤ì œ ì‹¤í–‰ì—ëŠ” ì˜í–¥ ì—†ìŒ)
# pylint: disable=no-member
# mypy: ignore-errors

# AI ê¸°ë°˜ ë¶„í•  ëª¨ë¸ ë¡œë“œ
model_path = 'weights/FastSAM-x.pt'
model = YOLO(model_path)

# ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA, MPS, CPU ìˆœìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else "cpu"
)

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤
class ImagePreprocessHistory:
    def __init__(self):
        self.original_image = None
        self.current_image = None
        self.history = []
        self.current_index = -1

    def set_original(self, image):
        if image is None:
            return None
        if not isinstance(image, Image.Image):
            image = Image.fromarray(np.array(image))
        self.original_image = image
        self.current_image = image
        self.history = [image]
        self.current_index = 0
        return image

    def add_state(self, image):
        if image is None:
            return None
        if not isinstance(image, Image.Image):
            image = Image.fromarray(np.array(image))
        self.history = self.history[:self.current_index + 1]
        self.history.append(image)
        self.current_index = len(self.history) - 1
        self.current_image = image
        return image

    def reset(self):
        if self.original_image is not None:
            self.current_image = self.original_image
            self.current_index = 0
            return self.original_image
        return None

    def undo(self):
        if self.current_index > 0:
            self.current_index -= 1
            self.current_image = self.history[self.current_index]
            return self.current_image
        return self.current_image

image_history = ImagePreprocessHistory()

# Colony ì¹´ìš´í„° í´ë˜ìŠ¤ (ìë™/ìˆ˜ë™ êµ¬ë¶„ ë° ê²°ê³¼ ì´ë¯¸ì§€ì— í†µí•© ìˆ«ì í‘œì‹œ)
class ColonyCounter:
    def __init__(self):
        self.manual_points = []  # ìˆ˜ë™ í¬ì¸íŠ¸
        self.auto_points = []    # ìë™ í¬ì¸íŠ¸
        self.current_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.original_image = None

    def reset(self):
        self.manual_points = []
        self.auto_points = []
        self.current_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.original_image = None

    def set_original_image(self, image):
        self.original_image = np.array(image)

    def toggle_remove_mode(self):
        self.remove_mode = not self.remove_mode
        img_with_points = self.draw_points()
        mode_text = "ğŸ”´ REMOVE MODE" if self.remove_mode else "ğŸŸ¢ ADD MODE"
        return img_with_points, mode_text

    def add_or_remove_point(self, image, evt: gr.SelectData):
        if self.current_image is None and image is not None:
            self.current_image = np.array(image)
        x, y = evt.index
        if self.remove_mode:
            closest_auto = self.find_closest_point(x, y, self.auto_points)
            closest_manual = self.find_closest_point(x, y, self.manual_points)
            if closest_auto is not None:
                self.auto_points.pop(closest_auto)
                self.auto_detected_count = len(self.auto_points)
            elif closest_manual is not None:
                self.manual_points.pop(closest_manual)
        else:
            self.manual_points.append((x, y))
        img_with_points = self.draw_points()
        return img_with_points, self.get_count_text()

    def find_closest_point(self, x, y, points, threshold=20):
        if not points:
            return None
        distances = [(np.sqrt((x - px) ** 2 + (y - py) ** 2), idx) for idx, (px, py) in enumerate(points)]
        closest_dist, closest_idx = min(distances, key=lambda item: item[0])
        return closest_idx if closest_dist < threshold else None

    def remove_last_point(self, image):
        if self.manual_points:
            self.manual_points.pop()
        img_with_points = self.draw_points()
        return img_with_points, self.get_count_text()

    def get_count_text(self):
        total_count = self.auto_detected_count + len(self.manual_points)
        return (f"ì „ì²´ CFU ìˆ˜: {total_count}\n"
                f"ğŸ¤– ìë™ ê°ì§€ëœ CFU: {self.auto_detected_count}\n"
                f"ğŸ‘† ìˆ˜ë™ ì¶”ê°€ëœ CFU: {len(self.manual_points)}")

    def draw_points(self):
        if self.current_image is None:
            return None
        img_with_points = self.current_image.copy()
        overlay = np.zeros_like(img_with_points)
        square_size = 30

        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.7
        font_thickness = 1
        outline_thickness = 3

        # ìë™ ê°ì§€ëœ í¬ì¸íŠ¸ í‘œì‹œ (ë…¹ìƒ‰ ì›)
        for idx, (x, y) in enumerate(self.auto_points, 1):
            cv2.circle(img_with_points, (x, y), 5, (0, 255, 0), -1)
            text = str(idx)
            (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
            text_x = int(x - text_width / 2)
            text_y = int(y - 10)
            
            # ê²€ì€ìƒ‰ ì™¸ê³½ì„  ì¶”ê°€
            for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:
                cv2.putText(img_with_points, text, 
                          (text_x + dx, text_y + dy), 
                          font, font_scale, (0, 0, 0), outline_thickness)
            
            # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸
            cv2.putText(img_with_points, text, (text_x, text_y), font, font_scale, (255, 0, 0), font_thickness)

        # ìˆ˜ë™ ì¶”ê°€ëœ í¬ì¸íŠ¸ í‘œì‹œ (ë¹¨ê°„ ì‚¬ê°í˜•)
        for idx, (x, y) in enumerate(self.manual_points, len(self.auto_points) + 1):
            pt1 = (int(x - square_size / 2), int(y - square_size / 2))
            pt2 = (int(x + square_size / 2), int(y + square_size / 2))
            cv2.rectangle(overlay, pt1, pt2, (0, 0, 255), -1)
            text = str(idx)
            (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
            text_x = int(x - text_width / 2)
            text_y = int(y - 10)
            
            # í•˜ì–€ìƒ‰ ì™¸ê³½ì„  ì¶”ê°€
            for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:
                cv2.putText(img_with_points, text, 
                          (text_x + dx, text_y + dy), 
                          font, font_scale, (255, 255, 255), outline_thickness)
            
            # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸
            cv2.putText(img_with_points, text, (text_x, text_y), font, font_scale, (255, 0, 0), font_thickness)

        cv2.addWeighted(overlay, 0.4, img_with_points, 1.0, 0, img_with_points)

        # ì „ì²´ ì¹´ìš´íŠ¸ í‘œì‹œ (ì™¼ìª½ í•˜ë‹¨)
        total_count = self.auto_detected_count + len(self.manual_points)
        count_text = f"Total: {total_count}"
        text_size = cv2.getTextSize(count_text, font, 1.5, 3)[0]
        margin = 20
        cv2.rectangle(img_with_points, 
                      (10, img_with_points.shape[0] - text_size[1] - margin * 2),
                      (text_size[0] + margin * 2, img_with_points.shape[0]),
                      (0, 0, 0), -1)
        cv2.putText(img_with_points, count_text, 
                    (margin, img_with_points.shape[0] - margin),
                    font, 1.5, (255, 255, 255), 3)

        return img_with_points

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_image(input_image, to_grayscale, binary_threshold, edge_detection, sharpen):
    if input_image is None:
        return None
    img = np.array(input_image)
    if to_grayscale:
        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    if binary_threshold > 0:
        _, img = cv2.threshold(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), binary_threshold, 255, cv2.THRESH_BINARY)
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    if edge_detection:
        img = cv2.Canny(img, 100, 200)
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    if sharpen > 0:
        kernel = np.array([[0, -sharpen, 0], [-sharpen, 1 + 4*sharpen, -sharpen], [0, -sharpen, 0]])
        img = cv2.filter2D(img, -1, kernel)
    return Image.fromarray(img)

# grok_colony_v1ì˜ ê²€ì¶œ ë¡œì§
def segment_and_count_colonies(
    input_image,
    conf_threshold=0.25,
    iou_threshold=0.7,
    circularity_threshold=0.8,
    better_quality=False,
    min_area_percentile=1,
    max_area_percentile=99,
    use_dish_filtering=False,
    dish_overlap_threshold=0.5,
    progress=gr.Progress(),
    draw_contours=True,
    mask_random_color=True,
    input_size=1024
):
    """
    FastSAM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì½œë¡œë‹ˆë¥¼ ê°ì§€í•˜ê³  ì¹´ìš´íŠ¸í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        input_image: ì…ë ¥ ì´ë¯¸ì§€
        conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (0.0~1.0)
        iou_threshold: IOU ì„ê³„ê°’ (0.0~1.0)
        circularity_threshold: ì›í˜• ì„ê³„ê°’ (0.0~1.0)
        better_quality: í–¥ìƒëœ ì‹œê°í™” í’ˆì§ˆ ì‚¬ìš© ì—¬ë¶€
        min_area_percentile: ìµœì†Œ ë©´ì  í¼ì„¼íƒ€ì¼ (0~100)
        max_area_percentile: ìµœëŒ€ ë©´ì  í¼ì„¼íƒ€ì¼ (0~100)
        use_dish_filtering: ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
        dish_overlap_threshold: ë°°ì–‘ ì ‘ì‹œ ê²¹ì¹¨ ì„ê³„ê°’ (0.0~1.0)
        progress: gradio ì§„í–‰ ìƒíƒœ í‘œì‹œ ê°ì²´
        draw_contours: ìœ¤ê³½ì„  ê·¸ë¦¬ê¸° ì—¬ë¶€
        mask_random_color: ë§ˆìŠ¤í¬ ëœë¤ ìƒ‰ìƒ ì‚¬ìš© ì—¬ë¶€
        input_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • ê°’
        
    Returns:
        tuple: (ì²˜ë¦¬ëœ ì´ë¯¸ì§€, ì¹´ìš´íŠ¸ ê²°ê³¼ í…ìŠ¤íŠ¸)
    """
    if input_image is None:
        return None, "ì´ë¯¸ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
    
    progress(0.1, desc="ì´ˆê¸°í™” ì¤‘...")
    counter.reset()
    counter.set_original_image(input_image)
    
    # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥
    original_width, original_height = input_image.size
    
    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë¹„ìœ¨ ìœ ì§€)
    w, h = input_image.size
    scale = input_size / max(w, h)
    new_w, new_h = int(w * scale), int(h * scale)
    input_resized = input_image.resize((new_w, new_h))
    image_np = np.array(input_resized)

    progress(0.3, desc="AI ë¶„ì„ ì¤‘...")
    results = model.predict(
        image_np,
        device=device,
        conf=conf_threshold,
        iou=iou_threshold,
        imgsz=input_size,
        retina_masks=True
    )

    if not results[0].masks:
        return image_np, "CFUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    progress(0.6, desc="ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")
    processed_image = image_np.copy()
    counter.auto_points = []

    # ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì²˜ë¦¬
    dish_mask = None
    dish_idx = -1
    if use_dish_filtering:
        progress(0.65, desc="ë°°ì–‘ ì ‘ì‹œ ê°ì§€ ì¤‘...")
        # ê°€ì¥ í° ë§ˆìŠ¤í¬ë¥¼ ë°°ì–‘ ì ‘ì‹œë¡œ ì‹ë³„
        annotations = results[0].masks.data
        if len(annotations) > 0:
            areas = [np.sum(ann.cpu().numpy()) for ann in annotations]
            dish_idx = np.argmax(areas)
            dish_annotation = annotations[dish_idx].cpu().numpy()
            dish_mask = dish_annotation > 0
            
            # ë°°ì–‘ ì ‘ì‹œ ìœ¤ê³½ì„  ì‹œê°í™” (íŒŒë€ìƒ‰)
            dish_contours, _ = cv2.findContours(dish_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(processed_image, dish_contours, -1, (0, 0, 255), 2)

    # ë§ˆìŠ¤í¬ ë©´ì  ê³„ì‚° ë° í•„í„°ë§
    all_masks = []
    all_areas = []
    
    for idx, mask in enumerate(results[0].masks.data):
        mask_np = mask.cpu().numpy()
        
        # ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì ìš©
        if use_dish_filtering and dish_mask is not None:
            # ë°°ì–‘ ì ‘ì‹œì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if idx == dish_idx:
                continue
                
            # ë°°ì–‘ ì ‘ì‹œ ë‚´ë¶€ ì—¬ë¶€ í™•ì¸ (ë§ˆìŠ¤í¬ êµì§‘í•©)
            overlap_ratio = np.sum(mask_np * dish_mask) / np.sum(mask_np)
            if overlap_ratio < dish_overlap_threshold:
                continue  # ë°°ì–‘ ì ‘ì‹œì™€ ì¶©ë¶„íˆ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
        
        contours, _ = cv2.findContours(mask_np.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            area = cv2.contourArea(contour)
            all_areas.append(area)
            all_masks.append((mask_np, contour, area))
    
    # ë©´ì  ê¸°ì¤€ í•„í„°ë§
    if all_areas:
        min_area = np.percentile(all_areas, min_area_percentile)
        max_area = np.percentile(all_areas, max_area_percentile)
    else:
        min_area = 0
        max_area = float('inf')
    
    # í–¥ìƒëœ í’ˆì§ˆ ì„¤ì • ì ìš©
    if better_quality:
        # ë” ì •êµí•œ ìœ¤ê³½ì„  ê·¸ë¦¬ê¸° (ë‘ê»˜ ì¡°ì •)
        contour_thickness = 3
        # ë” ì„ ëª…í•œ ìƒ‰ìƒ ì‚¬ìš©
        color_min = 120
        color_max = 255
    else:
        contour_thickness = 2
        color_min = 100
        color_max = 200
    
    # í•„í„°ë§ëœ ë§ˆìŠ¤í¬ ì²˜ë¦¬
    for mask_np, contour, area in all_masks:
        if min_area <= area <= max_area:
            perimeter = cv2.arcLength(contour, True)
            if perimeter == 0:
                continue
            circularity = 4 * np.pi * (area / (perimeter * perimeter))
            if circularity >= circularity_threshold:
                M = cv2.moments(contour)
                if M["m00"] != 0:
                    cX = int(M["m10"] / M["m00"])
                    cY = int(M["m01"] / M["m00"])
                    counter.auto_points.append((cX, cY))
                    if draw_contours:
                        color = tuple(np.random.randint(color_min, color_max, 3).tolist()) if mask_random_color else (0, 255, 0)
                        cv2.drawContours(processed_image, [contour], -1, color, contour_thickness)

    counter.auto_detected_count = len(counter.auto_points)
    counter.current_image = processed_image

    progress(1.0, desc="ì™„ë£Œ!")
    img_with_points = counter.draw_points()
    
    # ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë‹¤ì‹œ ì¡°ì •
    img_with_points_pil = Image.fromarray(img_with_points)
    
    # PIL ë²„ì „ì— ë”°ë¥¸ ë¦¬ìƒ˜í”Œë§ í•„í„° ì„¤ì •
    try:
        resampling_filter = Image.Resampling.LANCZOS
    except AttributeError:
        # PIL 9.0.0 ë¯¸ë§Œ ë²„ì „ í˜¸í™˜ì„±
        try:
            resampling_filter = Image.LANCZOS
        except AttributeError:
            # ë” ì˜¤ë˜ëœ PIL ë²„ì „ í˜¸í™˜ì„±
            resampling_filter = Image.ANTIALIAS
    
    img_with_points_resized = img_with_points_pil.resize((original_width, original_height), resampling_filter)
    
    return np.array(img_with_points_resized), counter.get_count_text()

# ê²°ê³¼ ì €ì¥ í•¨ìˆ˜
def save_results(original_image, processed_image):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        original_image: ì›ë³¸ ì´ë¯¸ì§€
        processed_image: ì²˜ë¦¬ëœ ê²°ê³¼ ì´ë¯¸ì§€
        
    Returns:
        str: ì €ì¥ ê²°ê³¼ ë©”ì‹œì§€
    """
    output_dir = "outputs"
    os.makedirs(output_dir, exist_ok=True)
    unique_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    original_path = os.path.join(output_dir, f"original_{unique_id}.png")
    processed_path = os.path.join(output_dir, f"processed_{unique_id}.png")
    original_image.save(original_path)
    Image.fromarray(processed_image).save(processed_path)
    return f"ì €ì¥ ì™„ë£Œ:\n- ì›ë³¸: {original_path}\n- ê²°ê³¼: {processed_path}"

# UI ë””ìì¸ (fastsam_prd_v4_ok ê¸°ë°˜)
css = """
body { font-family: Arial, sans-serif; background-color: #f0f0f0; }
.button-primary { background-color: #4CAF50; color: white; border: none; padding: 10px 20px; border-radius: 5px; }
.button-primary:hover { background-color: #45a049; }
.button-secondary { background-color: #808080; color: white; border: none; padding: 8px 16px; border-radius: 5px; }
.button-secondary:hover { background-color: #707070; }
.result-text { background: #fff; border-left: 5px solid #4CAF50; padding: 10px; }
.section-title { font-size: 18px; font-weight: bold; color: #4CAF50; margin-bottom: 10px; }
"""

counter = ColonyCounter()

with gr.Blocks(theme=gr.themes.Default(), css=css) as demo:
    gr.Markdown("# ğŸ”¬ Colony Counter\nAI ê¸°ë°˜ CFU ìë™ ê°ì§€ ë° ìˆ˜ë™ í¸ì§‘")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("<div class='section-title'>ğŸ“ ì´ë¯¸ì§€ ì—…ë¡œë“œ</div>")
            input_image = gr.Image(type="pil", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ")
            
            with gr.Accordion("ğŸ› ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬", open=False):
                to_grayscale = gr.Checkbox(label="í‘ë°± ë³€í™˜", value=False)
                binary_threshold = gr.Slider(0, 255, 0, label="ë°”ì´ë„ˆë¦¬ ì„ê³„ê°’")
                edge_detection = gr.Checkbox(label="ì—ì§€ ê²€ì¶œ", value=False)
                sharpen = gr.Slider(0, 1, 0, label="ìƒ¤í”ˆ ê°•ë„")
                preprocess_btn = gr.Button("ì „ì²˜ë¦¬ ì ìš©", variant="secondary", elem_classes="button-secondary")
                
                with gr.Row():
                    reset_btn = gr.Button("ì´ˆê¸°í™”", variant="secondary", elem_classes="button-secondary")
                    undo_btn = gr.Button("ì‹¤í–‰ ì·¨ì†Œ", variant="secondary", elem_classes="button-secondary")
            
            analysis_setting = gr.Accordion("âš™ï¸ ë¶„ì„ ì„¤ì •", open=False)
            
            with analysis_setting:
                with gr.Group():
                    gr.Markdown("### FastSAM íŒŒë¼ë¯¸í„°")
                    conf_threshold = gr.Slider(0.0, 1.0, 0.25, label="ì‹ ë¢°ë„ ì„ê³„ê°’", step=0.01)
                    iou_threshold = gr.Slider(0.0, 1.0, 0.7, label="IOU ì„ê³„ê°’", step=0.01)
                    circularity_threshold = gr.Slider(0.0, 1.0, 0.8, label="ì›í˜• ì„ê³„ê°’", step=0.01)
                    better_quality = gr.Checkbox(label="í–¥ìƒëœ ì‹œê°í™” í’ˆì§ˆ", value=True)
                    
                with gr.Group():
                    gr.Markdown("### í•„í„°ë§ ì˜µì…˜")
                    min_area_percentile = gr.Slider(0, 20, 1, label="ìµœì†Œ ë©´ì  í¼ì„¼íƒ€ì¼")
                    max_area_percentile = gr.Slider(80, 100, 99, label="ìµœëŒ€ ë©´ì  í¼ì„¼íƒ€ì¼")
                    
                    use_dish_filtering_checkbox = gr.Checkbox(label="ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì‚¬ìš©", value=True)
                    dish_overlap_threshold = gr.Slider(0.0, 1.0, 0.5, label="ë°°ì–‘ ì ‘ì‹œ ê²¹ì¹¨ ì„ê³„ê°’", step=0.01)
            
            analyze_btn = gr.Button("ğŸ” ë¶„ì„ ì‹œì‘", variant="primary", elem_classes="button-primary")
        
        with gr.Column():
            gr.Markdown("<div class='section-title'>ğŸ“Š ë¶„ì„ ê²°ê³¼</div>")
            output_image = gr.Image(type="numpy", interactive=True, label="ê²°ê³¼ ì´ë¯¸ì§€")
            count_text = gr.Textbox(label="ì¹´ìš´íŠ¸ ê²°ê³¼", elem_classes="result-text")
            
            gr.Markdown("### ìˆ˜ë™ í¸ì§‘")
            gr.Markdown("ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ì—¬ í¬ì¸íŠ¸ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            with gr.Row():
                remove_mode_btn = gr.Button("ğŸ”„ ëª¨ë“œ ì „í™˜", variant="secondary", elem_classes="button-secondary")
                mode_text = gr.Textbox(value="ğŸŸ¢ ADD MODE", interactive=False)
                remove_last_btn = gr.Button("â†©ï¸ ìµœê·¼ í¬ì¸íŠ¸ ì‚­ì œ", variant="secondary", elem_classes="button-secondary")
            
            save_btn = gr.Button("ğŸ’¾ ê²°ê³¼ ì €ì¥", variant="primary", elem_classes="button-primary")
            save_output = gr.Textbox(label="ì €ì¥ ê²°ê³¼", interactive=False, elem_classes="result-text")

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    input_image.upload(
        lambda img: (image_history.set_original(img), "ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ"),
        inputs=input_image,
        outputs=[input_image, count_text]
    )

    preprocess_btn.click(
        preprocess_image,
        inputs=[input_image, to_grayscale, binary_threshold, edge_detection, sharpen],
        outputs=input_image
    ).then(
        lambda img: image_history.add_state(img),
        inputs=input_image,
        outputs=input_image
    )

    reset_btn.click(
        lambda: image_history.reset(),
        inputs=[],
        outputs=input_image
    )

    undo_btn.click(
        lambda: image_history.undo(),
        inputs=[],
        outputs=input_image
    )

    analyze_btn.click(
        segment_and_count_colonies,
        inputs=[
            input_image,
            conf_threshold,
            iou_threshold,
            circularity_threshold,
            better_quality,
            min_area_percentile,
            max_area_percentile,
            use_dish_filtering_checkbox,
            dish_overlap_threshold
        ],
        outputs=[output_image, count_text]
    )

    output_image.select(
        counter.add_or_remove_point,
        inputs=[output_image],
        outputs=[output_image, count_text]
    )

    remove_mode_btn.click(
        counter.toggle_remove_mode,
        inputs=[],
        outputs=[output_image, mode_text]
    )

    remove_last_btn.click(
        counter.remove_last_point,
        inputs=[output_image],
        outputs=[output_image, count_text]
    )

    save_btn.click(
        save_results,
        inputs=[input_image, output_image],
        outputs=save_output
    )
    
    # ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì œëª© ì—…ë°ì´íŠ¸
    def update_title(is_enabled):
        if is_enabled:
            return "âš™ï¸ ë¶„ì„ ì„¤ì • (ë°°ì–‘ì ‘ì‹œ í•„í„°ë§ ì ìš© ì¤‘)"
        else:
            return "âš™ï¸ ë¶„ì„ ì„¤ì •"
    
    # ì´ˆê¸° UI ë¡œë”© ì‹œ ì œëª© ì—…ë°ì´íŠ¸ (ë°°ì–‘ì ‘ì‹œ í•„í„°ë§ì´ ê¸°ë³¸ê°’ìœ¼ë¡œ Trueì´ë¯€ë¡œ)
    analysis_setting.label = update_title(True)
    
    use_dish_filtering_checkbox.change(
        update_title,
        inputs=[use_dish_filtering_checkbox],
        outputs=[analysis_setting]
    )

if __name__ == "__main__":
    demo.launch()