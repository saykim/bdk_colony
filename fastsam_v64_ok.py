import os
from datetime import datetime
from ultralytics import YOLO
import gradio as gr
import torch
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import numpy as np
import cv2

# AI ê¸°ë°˜ ë¶„í•  ëª¨ë¸ ë¡œë“œ
model_path = 'weights/AI_Segmentation_Model.pt'
model = YOLO(model_path)

# ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA, MPS, CPU ìˆœìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else "cpu"
)

class ColonyCounter:
    def __init__(self):
        self.manual_points = []  # ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        self.auto_points = []    # ìë™ìœ¼ë¡œ ê°ì§€ëœ CFU ì¤‘ì‹¬ì  ë¦¬ìŠ¤íŠ¸
        self.current_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.original_image = None
        self.last_method = None
        self.zoom_factor = 1.0  # í™•ëŒ€/ì¶•ì†Œ ë¹„ìœ¨

    def reset(self):
        """ì¹´ìš´í„° ì´ˆê¸°í™”"""
        self.manual_points = []
        self.auto_points = []  # ìë™ í¬ì¸íŠ¸ ì´ˆê¸°í™”
        self.current_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.original_image = None
        self.last_method = None
        self.zoom_factor = 1.0  # í™•ëŒ€/ì¶•ì†Œ ë¹„ìœ¨ ì´ˆê¸°í™”

    def set_original_image(self, image):
        """
        ì›ë³¸ ì´ë¯¸ì§€ ì„¤ì •

        Args:
            image (PIL.Image): ì›ë³¸ ì´ë¯¸ì§€
        """
        self.original_image = np.array(image)

    def toggle_remove_mode(self):
        """í¸ì§‘ ëª¨ë“œ ì „í™˜ (ì¶”ê°€/ì œê±° ëª¨ë“œ)"""
        self.remove_mode = not self.remove_mode
        img_with_points = self.draw_points()
        mode_text = "ğŸ”´ ì œê±° ëª¨ë“œ" if self.remove_mode else "ğŸŸ¢ ì¶”ê°€ ëª¨ë“œ"
        return img_with_points, mode_text

    def add_or_remove_point(self, image, evt: gr.SelectData):
        """
        ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ì—¬ í¬ì¸íŠ¸ ì¶”ê°€ ë˜ëŠ” ì œê±°

        Args:
            image (numpy.ndarray): ì¶œë ¥ ì´ë¯¸ì§€
            evt (gr.SelectData): ì„ íƒ ë°ì´í„° ì´ë²¤íŠ¸

        Returns:
            tuple: ì—…ë°ì´íŠ¸ëœ ì´ë¯¸ì§€ì™€ ì¹´ìš´íŠ¸ í…ìŠ¤íŠ¸
        """
        try:
            if self.current_image is None and image is not None:
                self.current_image = np.array(image)

            x, y = evt.index  # í´ë¦­í•œ ì¢Œí‘œ

            if self.remove_mode:
                # ì œê±° ëª¨ë“œ: ìë™ í¬ì¸íŠ¸ ë˜ëŠ” ìˆ˜ë™ í¬ì¸íŠ¸ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ ì œê±°
                closest_auto = self.find_closest_point(x, y, self.auto_points)
                closest_manual = self.find_closest_point(x, y, self.manual_points)
                removed = False

                if closest_auto is not None:
                    self.auto_points.pop(closest_auto)
                    self.auto_detected_count = len(self.auto_points)
                    removed = True
                elif closest_manual is not None:
                    self.manual_points.pop(closest_manual)
                    removed = True

                if not removed:
                    print("ì œê±°í•  í¬ì¸íŠ¸ê°€ ì¶©ë¶„íˆ ê°€ê¹Œì´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                # ì¶”ê°€ ëª¨ë“œ: ìƒˆë¡œìš´ í¬ì¸íŠ¸ ì¶”ê°€
                self.manual_points.append((x, y))

            img_with_points = self.draw_points()
            return img_with_points, self.get_count_text()
        except Exception as e:
            print(f"í¬ì¸íŠ¸ ì¶”ê°€/ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return image, self.get_count_text()

    def find_closest_point(self, x, y, points, threshold=20):
        """
        ì£¼ì–´ì§„ ì¢Œí‘œì™€ ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            x (int): í´ë¦­í•œ x ì¢Œí‘œ
            y (int): í´ë¦­í•œ y ì¢Œí‘œ
            points (list): í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
            threshold (int, optional): ìµœëŒ€ ê±°ë¦¬ ì„ê³„ê°’. Defaults to 20.

        Returns:
            int or None: ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ì˜ ì¸ë±ìŠ¤ ë˜ëŠ” None
        """
        if not points:
            return None

        distances = []
        for idx, (px, py) in enumerate(points):
            dist = np.sqrt((x - px) ** 2 + (y - py) ** 2)
            distances.append((dist, idx))

        if not distances:
            return None

        closest_dist, closest_idx = min(distances, key=lambda item: item[0])
        return closest_idx if closest_dist < threshold else None

    def remove_last_point(self, image):
        """
        ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ ë§ˆì§€ë§‰ í¬ì¸íŠ¸ ì œê±°

        Args:
            image (numpy.ndarray): ì¶œë ¥ ì´ë¯¸ì§€

        Returns:
            tuple: ì—…ë°ì´íŠ¸ëœ ì´ë¯¸ì§€ì™€ ì¹´ìš´íŠ¸ í…ìŠ¤íŠ¸
        """
        try:
            if self.manual_points:
                self.manual_points.pop()
                img_with_points = self.draw_points()
                return img_with_points, self.get_count_text()
            return image, self.get_count_text()
        except Exception as e:
            print(f"ë§ˆì§€ë§‰ í¬ì¸íŠ¸ ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return image, self.get_count_text()

    def get_count_text(self):
        """
        ì¹´ìš´íŠ¸ ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±

        Returns:
            str: ì¹´ìš´íŠ¸ ê²°ê³¼ í…ìŠ¤íŠ¸
        """
        try:
            method_text = f"ë°©ë²•: {self.last_method}\n" if self.last_method else ""
            return (f"{method_text}ì „ì²´ CFU ìˆ˜: {self.auto_detected_count + len(self.manual_points)}\n"
                    f"ğŸ¤– ìë™ ê°ì§€ëœ CFU: {self.auto_detected_count}\n"
                    f"ğŸ‘† ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ CFU: {len(self.manual_points)}")
        except Exception as e:
            print(f"ì¹´ìš´íŠ¸ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return "ì¹´ìš´íŠ¸ ê³„ì‚° ì˜¤ë¥˜"

    def draw_points(self):
        """
        í˜„ì¬ ì´ë¯¸ì§€ì— í¬ì¸íŠ¸ë¥¼ ê·¸ë ¤ì„œ ë°˜í™˜

        Returns:
            numpy.ndarray: í¬ì¸íŠ¸ê°€ ê·¸ë ¤ì§„ ì´ë¯¸ì§€
        """
        try:
            if self.current_image is None:
                return None

            img_with_points = self.current_image.copy()
            overlay = np.zeros_like(img_with_points)
            square_size = 30  # 25ì—ì„œ 30ìœ¼ë¡œ 20% ì¦ê°€

            # ê¸€ê¼´ ì„¤ì •
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.7 #0.7
            font_thickness = 1
            outline_thickness = 3

            # ìë™ ê°ì§€ëœ CFU ë²ˆí˜¸ í‘œì‹œ (íŒŒë€ìƒ‰)
            for idx, (x, y) in enumerate(self.auto_points, 1):
                text = str(idx)
                (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)

                text_x = int(x - text_width / 2)
                text_y = int(y - 10)

                # ê²€ì€ìƒ‰ ì™¸ê³½ì„ 
                for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:
                    cv2.putText(img_with_points, text,
                                (text_x + dx, text_y + dy),
                                font, font_scale, (0, 0, 0), outline_thickness)

                # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸
                cv2.putText(img_with_points, text,
                            (text_x, text_y),
                            font, font_scale, (255, 0, 0), font_thickness)  # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸

            # ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ í¬ì¸íŠ¸ ì‚¬ê°í˜• ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)
            for x, y in self.manual_points:
                pt1 = (int(x - square_size / 2), int(y - square_size / 2))
                pt2 = (int(x + square_size / 2), int(y + square_size / 2))
                cv2.rectangle(overlay, pt1, pt2, (0, 0, 255), -1)  # ë¹¨ê°„ìƒ‰ ì‚¬ê°í˜•

            # ì˜¤ë²„ë ˆì´ ì ìš©
            cv2.addWeighted(overlay, 0.4, img_with_points, 1.0, 0, img_with_points)

            # ìˆ˜ë™ í¬ì¸íŠ¸ ë²ˆí˜¸ í‘œì‹œ (íŒŒë€ìƒ‰)
            for idx, (x, y) in enumerate(self.manual_points, len(self.auto_points) + 1):
                text = str(idx)
                (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
                text_x = int(x - text_width / 2)
                text_y = int(y - 10)

                # í•˜ì–€ìƒ‰ ì™¸ê³½ì„ 
                for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:
                    cv2.putText(img_with_points, text,
                                (text_x + dx, text_y + dy),
                                font, font_scale, (255, 255, 255), outline_thickness)

                # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸
                cv2.putText(img_with_points, text,
                            (text_x, text_y),
                            font, font_scale, (255, 0, 0), font_thickness)  # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸

            # ì œê±° ëª¨ë“œ í‘œì‹œ
            if self.remove_mode:
                overlay_mode = img_with_points.copy()
                cv2.rectangle(overlay_mode, (0, 0), (img_with_points.shape[1], 40), (255, 0, 0), -1)  # ë¹¨ê°„ìƒ‰ ìƒì
                cv2.addWeighted(overlay_mode, 0.3, img_with_points, 0.7, 0, img_with_points)
                cv2.putText(img_with_points, "ì œê±° ëª¨ë“œ", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)  # í°ìƒ‰ í…ìŠ¤íŠ¸

            # Total Count í‘œì‹œ ì¶”ê°€
            total_count = self.auto_detected_count + len(self.manual_points)
            text = f'Total Count: {total_count}'
            
            # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
            height, width = img_with_points.shape[:2]
            
            # í°íŠ¸ ì„¤ì •
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 1.5 #0.7
            font_thickness = 2
            
            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            (text_width, text_height), baseline = cv2.getTextSize(
                text, font, font_scale, font_thickness
            )
            
            # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚° (ìš°í•˜ë‹¨)
            #text_x = width - text_width - 20  # ìš°ì¸¡ì—ì„œ 20í”½ì…€ ì—¬ë°±
            text_x = 20
            text_y = height - 20  # í•˜ë‹¨ì—ì„œ 20í”½ì…€ ì—¬ë°±
            
            # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸° (ê°€ë…ì„± í–¥ìƒ)
            padding = 5
            cv2.rectangle(
                img_with_points,
                (text_x - padding, text_y - text_height - padding),
                (text_x + text_width + padding, text_y + padding),
                (0, 0, 0),  # ê²€ì • ë°°ê²½
                -1
            )
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            cv2.putText(
                img_with_points,
                text,
                (text_x, text_y),
                font,
                font_scale,
                (0, 255, 255),  # ë…¸ë€ìƒ‰
                font_thickness
            )

            return img_with_points
        
        except Exception as e:
            print(f"í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self.current_image

# ColonyCounter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
counter = ColonyCounter()

def preprocess_image(
    input_image,
    to_grayscale=False,
    binary=False,
    binary_threshold=128,
    edge_detection=False,
    sharpen=False,
    sharpen_amount=1.0
):
    """
    ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        input_image (PIL.Image): ì „ì²˜ë¦¬í•  ì…ë ¥ ì´ë¯¸ì§€
        to_grayscale (bool, optional): í‘ë°± ë³€í™˜ ì—¬ë¶€. Defaults to False.
        binary (bool, optional): ë°”ì´ë„ˆë¦¬ ë³€í™˜ ì—¬ë¶€. Defaults to False.
        binary_threshold (int, optional): ë°”ì´ë„ˆë¦¬ ë³€í™˜ ì„ê³„ê°’. Defaults to 128.
        edge_detection (bool, optional): ì—ì§€ ê²€ì¶œ ì—¬ë¶€. Defaults to False.
        sharpen (bool, optional): ìƒ¤í”ˆ ì—¬ë¶€. Defaults to False.
        sharpen_amount (float, optional): ìƒ¤í”ˆ ê°•ë„. Defaults to 1.0.

    Returns:
        PIL.Image: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€
    """
    try:
        image = input_image.copy()

        # í‘ë°± ë³€í™˜
        if to_grayscale:
            image = image.convert('L').convert('RGB')  # í‘ë°± í›„ RGBë¡œ ë³€í™˜

        # ë°”ì´ë„ˆë¦¬ ë³€í™˜
        if binary:
            image_np = np.array(image)
            gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            _, binary_img = cv2.threshold(gray, binary_threshold, 255, cv2.THRESH_BINARY)
            image = Image.fromarray(binary_img).convert('RGB')

        # ì—ì§€ ê²€ì¶œ
        if edge_detection:
            image_np = np.array(image)
            gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 100, 200)
            image = Image.fromarray(edges).convert('RGB')

        # ìƒ¤í”ˆ
        if sharpen:
            image = image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))  # ê¸°ë³¸ ìƒ¤í”ˆ ì„¤ì •
            if sharpen_amount != 1.0:
                enhancer = ImageEnhance.Sharpness(image)
                image = enhancer.enhance(sharpen_amount)

        return image
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return input_image

def process_image(colony_annotations, dish_annotation, image, device, scale, better_quality, mask_random_color, bbox, use_retina, withContours):
    """
    ë§ˆìŠ¤í¬ ì£¼ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ì—¬ CFUì˜ ìœ¤ê³½ì„ ì„ ê·¸ë¦½ë‹ˆë‹¤.
    
    Args:
        colony_annotations (list): CFUì˜ ë§ˆìŠ¤í¬ ì£¼ì„ ë¦¬ìŠ¤íŠ¸
        dish_annotation (torch.Tensor): ë°°ì–‘ì ‘ì‹œì˜ ë§ˆìŠ¤í¬ ì£¼ì„
        image (PIL.Image): ì›ë³¸ ì´ë¯¸ì§€
        device (torch.device): ë””ë°”ì´ìŠ¤ ì„¤ì •
        scale (float): ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼
        better_quality (bool): í–¥ìƒëœ í’ˆì§ˆ ì—¬ë¶€
        mask_random_color (bool): ë§ˆìŠ¤í¬ì— ëœë¤ ìƒ‰ìƒ ì ìš© ì—¬ë¶€
        bbox (None): ë°”ìš´ë”© ë°•ìŠ¤ (í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        use_retina (bool): Retina ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        withContours (bool): ìœ¤ê³½ì„  í‘œì‹œ ì—¬ë¶€

    Returns:
        PIL.Image: ì²˜ë¦¬ëœ ì´ë¯¸ì§€
    """
    image_np = np.array(image).copy()

    # CFU ë§ˆìŠ¤í¬ ì²˜ë¦¬
    for ann in colony_annotations:
        mask = ann.cpu().numpy()
        if mask.ndim == 2:
            mask = mask > 0
            if mask_random_color:
                color = np.random.randint(100, 200, (3,)).tolist()  # ì¤‘ê°„ ë°ê¸°ì˜ ëœë¤ ìƒ‰ìƒ ìƒì„±
                image_np[mask] = color
            else:
                image_np[mask] = (0, 255, 0)  # ê¸°ë³¸ ë…¹ìƒ‰

        if withContours:
            # ìœ¤ê³½ì„  ì°¾ê¸° ë° ê·¸ë¦¬ê¸°
            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(image_np, contours, -1, (255, 0, 0), 2)  # íŒŒë€ìƒ‰ ìœ¤ê³½ì„ 

    # ë°°ì–‘ì ‘ì‹œ ë§ˆìŠ¤í¬ ì²˜ë¦¬ (ìœ¤ê³½ì„ ë§Œ ê·¸ë¦¬ê¸°)
    if dish_annotation is not None:
        dish_mask = dish_annotation.cpu().numpy()
        if dish_mask.ndim == 2:
            dish_mask = dish_mask > 0
            if withContours:
                contours, _ = cv2.findContours(dish_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(image_np, contours, -1, (0, 0, 255), 3)  # ë¹¨ê°„ìƒ‰ ìœ¤ê³½ì„ 

    processed_image = Image.fromarray(image_np)
    return processed_image

def segment_and_count_colonies(
    preprocessed_image,
    method='AI Detection',
    input_size=1024,
    iou_threshold=0.7,
    conf_threshold=0.25,
    better_quality=False,
    withContours=True,
    mask_random_color=True,
    min_area_percentile=1,
    max_area_percentile=99,
    circularity_threshold=0.8,
    progress=gr.Progress()
):
    """
    ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  CFUë¥¼ ê°ì§€í•˜ì—¬ ì¹´ìš´íŒ…í•©ë‹ˆë‹¤.

    Args:
        preprocessed_image (PIL.Image): ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€
        method (str, optional): ë¶„ì„ ë°©ë²•. Defaults to 'AI Detection'.
        input_size (int, optional): ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°. Defaults to 1024.
        iou_threshold (float, optional): IOU ì„ê³„ê°’. Defaults to 0.7.
        conf_threshold (float, optional): ì‹ ë¢°ë„ ì„ê³„ê°’. Defaults to 0.25.
        better_quality (bool, optional): í–¥ìƒëœ í’ˆì§ˆ ì—¬ë¶€. Defaults to False.
        withContours (bool, optional): ìœ¤ê³½ì„  í‘œì‹œ ì—¬ë¶€. Defaults to True.
        mask_random_color (bool, optional): ë§ˆìŠ¤í¬ì— ëœë¤ ìƒ‰ìƒ ì ìš© ì—¬ë¶€. Defaults to True.
        min_area_percentile (int, optional): ìµœì†Œ ë©´ì  ë°±ë¶„ìœ„ìˆ˜. Defaults to 1.
        max_area_percentile (int, optional): ìµœëŒ€ ë©´ì  ë°±ë¶„ìœ„ìˆ˜. Defaults to 99.
        circularity_threshold (float, optional): ì›í˜•ë„ ì„ê³„ê°’. Defaults to 0.8.
        progress (gr.Progress, optional): ì§„í–‰ ìƒí™© í‘œì‹œ. Defaults to gr.Progress().

    Returns:
        tuple: ë¶„ì„ëœ ì´ë¯¸ì§€ê³¼ ì¹´ìš´íŠ¸ í…ìŠ¤íŠ¸
    """
    try:
        if preprocessed_image is None:
            return None, "ì´ë¯¸ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."

        progress(0.1, desc="ì´ˆê¸°í™” ì¤‘...")
        counter.reset()
        counter.set_original_image(preprocessed_image)
        image_to_use = preprocessed_image

        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë¹„ìœ¨ ìœ ì§€)
        w, h = image_to_use.size
        scale = input_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        input_resized = image_to_use.resize((new_w, new_h))

        progress(0.3, desc="AI ë¶„ì„ ì¤‘...")
        # CFU ê°ì§€
        results = model.predict(
            input_resized,
            device=device,
            conf=conf_threshold,
            iou=iou_threshold,
            imgsz=input_size
        )

        if not results[0].masks:
            return np.array(input_resized), "CFUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        progress(0.6, desc="ë§ˆìŠ¤í¬ ë¦¬ì‚¬ì´ì¦ˆ ì¤‘...")
        # ë§ˆìŠ¤í¬ ë¦¬ì‚¬ì´ì¦ˆ
        annotations = results[0].masks.data
        resized_annotations = []
        for ann in annotations:
            mask_np = ann.cpu().numpy().astype(np.uint8) * 255  # ì´ì§„ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜
            mask_resized = cv2.resize(mask_np, (new_w, new_h), interpolation=cv2.INTER_NEAREST)
            mask_resized = mask_resized > 0  # ë‹¤ì‹œ ë¶ˆë¦¬ì–¸ìœ¼ë¡œ ë³€í™˜
            resized_annotations.append(torch.from_numpy(mask_resized))

        # ì˜ì—­ ê³„ì‚°
        areas = [np.sum(ann.numpy()) for ann in resized_annotations]

        # ê°€ì¥ í° ë§ˆìŠ¤í¬ë¥¼ ë°°ì–‘ì ‘ì‹œë¡œ ì¸ì‹
        dish_idx = np.argmax(areas)
        dish_annotation = resized_annotations[dish_idx]
        colony_annotations = [ann for idx, ann in enumerate(resized_annotations) if idx != dish_idx]

        progress(0.8, desc="ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")
        # ê²°ê³¼ ì‹œê°í™”
        fig = process_image(
            colony_annotations=colony_annotations,
            dish_annotation=dish_annotation,
            image=input_resized,
            device=device,
            scale=1.0,
            better_quality=better_quality,
            mask_random_color=mask_random_color,
            bbox=None,
            use_retina=False,
            withContours=withContours
        )

        counter.current_image = np.array(fig)
        counter.auto_detected_count = len(colony_annotations)

        # ìë™ ê°ì§€ëœ CFUì˜ ì¤‘ì‹¬ì  ê³„ì‚°
        counter.auto_points = []
        for ann in colony_annotations:
            mask = ann.numpy()
            if mask.ndim == 2:
                mask = mask > 0
                y_indices, x_indices = np.where(mask)
                if len(x_indices) > 0 and len(y_indices) > 0:
                    center_x = int(np.mean(x_indices))
                    center_y = int(np.mean(y_indices))
                    counter.auto_points.append((center_x, center_y))

        progress(1.0, desc="ì™„ë£Œ!")
        img_with_points = counter.draw_points()
        return img_with_points, counter.get_count_text()
    except Exception as e:
        error_msg = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(error_msg)
        return np.array(preprocessed_image), error_msg

def save_results(original_image, processed_image):
    """
    ì›ë³¸ ì´ë¯¸ì§€ì™€ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ /outputs í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        original_image (PIL.Image): ì›ë³¸ ì´ë¯¸ì§€
        processed_image (numpy.ndarray): ì²˜ë¦¬ëœ ì´ë¯¸ì§€

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    try:
        # /outputs í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        output_dir = "outputs"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ê³ ìœ í•œ ì‹ë³„ì ìƒì„±
        unique_id = datetime.now().strftime("%Y%m%d_%H%M%S") + f"_{os.getpid()}"

        # íŒŒì¼ëª… ì„¤ì •
        original_filename = f"original_{unique_id}.png"
        processed_filename = f"ì¹´ìš´íŒ…ì™„ë£Œ_{unique_id}.png"

        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        original_path = os.path.join(output_dir, original_filename)
        processed_path = os.path.join(output_dir, processed_filename)

        # ì´ë¯¸ì§€ ì €ì¥
        original_image.save(original_path)
        processed_image_pil = Image.fromarray(processed_image)
        processed_image_pil.save(processed_path)

        return f"ì €ì¥ëœ íŒŒì¼:\n- ì›ë³¸: {original_path}\n- ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {processed_path}"
    except Exception as e:
        error_msg = f"ì´ë¯¸ì§€ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(error_msg)
        return error_msg

# CSS ìŠ¤íƒ€ì¼ë§ ê°œì„ 
css = """
body {
    background-color: #f0f2f5;
}
.container {
    max-width: 1400px;
    margin: auto;
    padding: 20px;
}
.header {
    text-align: center;
    margin-bottom: 30px;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 30px;
    border-radius: 15px;
    color: white;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}
.result-text {
    font-size: 1.2em;
    font-weight: bold;
    padding: 20px;
    background: #ffffff;
    border-radius: 10px;
    border-left: 5px solid #4a90e2;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}
.image-display {
    border: 2px solid #e0e0e0;
    border-radius: 10px;
    overflow: hidden;
    transition: all 0.3s ease;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}
.image-display:hover {
    border-color: #4a90e2;
}
.button-primary {
    background-color: #4a90e2;
    color: white;
}
.button-secondary {
    background-color: #f0f2f5;
    color: #333333;
}
.accordion-header {
    font-weight: bold;
    background-color: #4a90e2;
    color: white;
}
"""

# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
with gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:
    gr.Markdown(
        """
        <div class="header">
            <h1>ğŸ”¬ ê³ ê¸‰ CFU ì¹´ìš´í„°</h1>
            <h3>FastSAMì„ ì´ìš©í•œ ìë™ CFU ê°ì§€ ë° ìˆ˜ë™ ìˆ˜ì •</h3>
        </div>
        """
    )

    with gr.Tab("CFU ì¹´ìš´í„°"):
        with gr.Row():
            with gr.Column(scale=1, min_width=300):
                input_image = gr.Image(
                    label="ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                    type="pil",
                    elem_classes="input-image"
                )

                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •
                with gr.Accordion("ğŸ› ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •", open=False, elem_classes="accordion-header"):
                    to_grayscale = gr.Checkbox(
                        label="í‘ë°± ë³€í™˜",
                        value=False,
                        info="ì´ë¯¸ì§€ë¥¼ í‘ë°±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
                    )
                    binary = gr.Checkbox(
                        label="ë°”ì´ë„ˆë¦¬ ë³€í™˜",
                        value=False,
                        info="ì´ë¯¸ì§€ë¥¼ ë°”ì´ë„ˆë¦¬(ì´ì§„) ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
                    )
                    binary_threshold = gr.Slider(
                        0, 255, 128,
                        step=1,
                        label="ë°”ì´ë„ˆë¦¬ ì„ê³„ê°’",
                        info="ë°”ì´ë„ˆë¦¬ ë³€í™˜ ì‹œ ì‚¬ìš©í•  ì„ê³„ê°’ì…ë‹ˆë‹¤."
                    )
                    edge_detection = gr.Checkbox(
                        label="ì—ì§€ ê²€ì¶œ",
                        value=False,
                        info="ì´ë¯¸ì§€ì˜ ì—ì§€ë¥¼ ê²€ì¶œí•©ë‹ˆë‹¤."
                    )
                    sharpen = gr.Checkbox(
                        label="ìƒ¤í”ˆ",
                        value=False,
                        info="ì´ë¯¸ì§€ì˜ ì„ ëª…ë„ë¥¼ ë†’ì…ë‹ˆë‹¤."
                    )
                    sharpen_amount = gr.Slider(
                        0.5, 5.0, 1.0,
                        step=0.1,
                        label="ìƒ¤í”ˆ ê°•ë„",
                        info="ìƒ¤í”ˆì˜ ê°•ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤."
                    )
                    preprocess_button = gr.Button(
                        "ğŸ”„ ì „ì²˜ë¦¬ ì ìš©",
                        variant="secondary",
                        elem_classes="button-secondary"
                    )

                # Gradioì˜ State ì»´í¬ë„ŒíŠ¸ ì¶”ê°€ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì €ì¥)
                preprocessed_image_state = gr.State()

                with gr.Row():
                    method_select = gr.Radio(
                        choices=['AI Detection'],  # FastSAM ê¸°ë°˜ìœ¼ë¡œ ê°€ì •
                        value='AI Detection',
                        label="íƒì§€ ë°©ë²•",
                        info="AI ê¸°ë°˜ íƒì§€ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”",
                        elem_id="method_select"
                    )
                    segment_button = gr.Button(
                        "ğŸ” ì´ë¯¸ì§€ ë¶„ì„",
                        variant="primary",
                        scale=2,
                        elem_classes="button-primary"
                    )

                # mask_random_color ì •ì˜ë¥¼ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì • ì•„ë˜ë¡œ ì´ë™
                mask_random_color = gr.Checkbox(
                    label="ë§ˆìŠ¤í¬ì— ëœë¤ ìƒ‰ìƒ ì ìš©",
                    value=True,
                    info="ê°ì§€ëœ CFUì˜ ë§ˆìŠ¤í¬ì— ëœë¤ ìƒ‰ìƒì„ ì ìš©í•©ë‹ˆë‹¤."
                )

                with gr.Accordion("âš™ï¸ ë¶„ì„ ì„¤ì •", open=False, elem_classes="accordion-header"):
                    with gr.Tab("ì¼ë°˜"):
                        input_size_slider = gr.Slider(
                            512, 2048, 1024,
                            step=64,
                            label="ì…ë ¥ í¬ê¸°",
                            info="í¬ê¸°ê°€ í´ìˆ˜ë¡ ì •í™•ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ ì†ë„ëŠ” ëŠë ¤ì§‘ë‹ˆë‹¤."
                        )
                        better_quality_checkbox = gr.Checkbox(
                            label="í–¥ìƒëœ í’ˆì§ˆ",
                            value=True,
                            info="ì†ë„ë¥¼ í¬ìƒí•˜ê³  ì¶œë ¥ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
                        )
                        withContours_checkbox = gr.Checkbox(
                            label="ìœ¤ê³½ì„  í‘œì‹œ",
                            value=True,
                            info="CFUì˜ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
                        )

                    with gr.Tab("AI íƒì§€"):
                        iou_threshold_slider = gr.Slider(
                            0.1, 0.9, 0.7,
                            step=0.1,
                            label="IOU ì„ê³„ê°’",
                            info="ë†’ì„ìˆ˜ë¡ ê²¹ì¹¨ ê¸°ì¤€ì´ ì—„ê²©í•´ì§‘ë‹ˆë‹¤."
                        )
                        conf_threshold_slider = gr.Slider(
                            0.1, 0.9, 0.25,
                            step=0.05,
                            label="ì‹ ë¢°ë„ ì„ê³„ê°’",
                            info="ë†’ì„ìˆ˜ë¡ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íƒì§€ë§Œ í‘œì‹œë©ë‹ˆë‹¤."
                        )

                    with gr.Tab("í¬ê¸° í•„í„°"):
                        min_area_percentile_slider = gr.Slider(
                            0, 10, 1,
                            step=1,
                            label="ìµœì†Œ í¬ê¸° ë°±ë¶„ìœ„ìˆ˜",
                            info="ë” ì‘ì€ CFUë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤ (1ì€ ê°€ì¥ ì‘ì€ 1% í•„í„°ë§)."
                        )
                        max_area_percentile_slider = gr.Slider(
                            90, 100, 99,
                            step=1,
                            label="ìµœëŒ€ í¬ê¸° ë°±ë¶„ìœ„ìˆ˜",
                            info="ë” í° ê°ì²´ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤ (99ëŠ” ê°€ì¥ í° 1% í•„í„°ë§)."
                        )

                    with gr.Tab("í˜•íƒœ í•„í„°"):
                        circularity_threshold_slider = gr.Slider(
                            0.0, 1.0, 0.8,
                            step=0.01,
                            label="ì›í˜•ë„ ì„ê³„ê°’",
                            info="ëŒ€ëµì ìœ¼ë¡œ ì›í˜•ì¸ CFUë§Œ ê°ì§€í•©ë‹ˆë‹¤ (1 = ì™„ë²½í•œ ì›)."
                        )

            with gr.Column(scale=1, min_width=300):
                output_image = gr.Image(
                    label="ë¶„ì„ ê²°ê³¼",
                    type="numpy",
                    interactive=True,
                    elem_classes="output-image"
                )
                colony_count_text = gr.Textbox(
                    label="ì¹´ìš´íŠ¸ ê²°ê³¼",
                    lines=3,
                    elem_classes="result-text"
                )

                with gr.Row():
                    with gr.Column(scale=1):
                        remove_mode_button = gr.Button(
                            "ğŸ”„ í¸ì§‘ ëª¨ë“œ ì „í™˜",
                            variant="secondary",
                            elem_classes="button-secondary"
                        )
                        remove_mode_text = gr.Textbox(
                            label="í˜„ì¬ ëª¨ë“œ",
                            value="ğŸŸ¢ ì¶”ê°€ ëª¨ë“œ",
                            lines=1,
                            interactive=False
                        )
                    with gr.Column(scale=1):
                        remove_point_button = gr.Button(
                            "â†©ï¸ ìµœê·¼ í¬ì¸íŠ¸ ì·¨ì†Œ",
                            variant="secondary",
                            elem_classes="button-secondary"
                        )

                # ê²°ê³¼ ì €ì¥ ê¸°ëŠ¥ (ì˜µì…˜)
                save_button = gr.Button("ğŸ’¾ ê²°ê³¼ ì €ì¥", elem_classes="button-primary")
                save_output = gr.Textbox(
                    label="ì €ì¥ ê²°ê³¼",
                    lines=2,
                    interactive=False,
                    elem_classes="result-text"
                )

        with gr.Row(elem_classes="instruction-box"):
            gr.Markdown(
                """
                ### ğŸ“ ë¹ ë¥¸ ê°€ì´ë“œ
                1. **ì´ë¯¸ì§€ ì—…ë¡œë“œ**: ë¶„ì„í•  CFU ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.
                2. **ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •**: í•„ìš”ì— ë”°ë¼ í‘ë°± ë³€í™˜, ë°”ì´ë„ˆë¦¬ ë³€í™˜, ì—ì§€ ê²€ì¶œ, ìƒ¤í”ˆ ì„¤ì •ì„ ì¡°ì ˆí•˜ì„¸ìš”.
                3. **ì „ì²˜ë¦¬ ì ìš©**: "ì „ì²˜ë¦¬ ì ìš©" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì„¤ì •í•œ ì „ì²˜ë¦¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©í•˜ì„¸ìš”.
                4. **íƒì§€ ë°©ë²• ì„ íƒ**: ìë™ ë¶„ì„ì„ ìœ„í•´ AI Detectionì„ ì„ íƒí•˜ì„¸ìš”.
                5. **ì´ë¯¸ì§€ ë¶„ì„**: "ì´ë¯¸ì§€ ë¶„ì„" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”.
                6. **ìˆ˜ë™ ìˆ˜ì •**:
                    - ğŸ‘† ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ì—¬ ëˆ„ë½ëœ CFUë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ìë™ìœ¼ë¡œ ì‹ë³„ëœ CFUë¥¼ ì œê±°í•˜ì„¸ìš”.
                    - ğŸ”„ 'í¸ì§‘ ëª¨ë“œ ì „í™˜' ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€/ì œê±° ëª¨ë“œë¥¼ ì „í™˜í•˜ì„¸ìš”.
                    - â†©ï¸ 'ìµœê·¼ í¬ì¸íŠ¸ ì·¨ì†Œ' ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ìµœê·¼ì— ì¶”ê°€ëœ í¬ì¸íŠ¸ë¥¼ ì œê±°í•˜ì„¸ìš”.
                7. **ë¶„ì„ ì„¤ì • ì¡°ì •**:
                    - **ì…ë ¥ í¬ê¸°**: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì„¤ì •í•˜ì„¸ìš” (í¬ê¸°ê°€ í´ìˆ˜ë¡ ì •í™•ë„ê°€ ì¦ê°€í•˜ì§€ë§Œ ì²˜ë¦¬ ì†ë„ëŠ” ëŠë ¤ì§‘ë‹ˆë‹¤).
                    - **IOU ì„ê³„ê°’**: ê²¹ì¹˜ëŠ” íƒì§€ì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ ì¡°ì •í•˜ì„¸ìš” (ê°’ì´ ë†’ì„ìˆ˜ë¡ ê²¹ì¹¨ ê¸°ì¤€ì´ ì—„ê²©í•´ì§‘ë‹ˆë‹¤).
                    - **ì‹ ë¢°ë„ ì„ê³„ê°’**: íƒì§€ì˜ ì‹ ë¢° ìˆ˜ì¤€ì„ ì„¤ì •í•˜ì„¸ìš” (ê°’ì´ ë†’ì„ìˆ˜ë¡ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íƒì§€ë§Œ í‘œì‹œë©ë‹ˆë‹¤).
                    - **ìµœì†Œ/ìµœëŒ€ í¬ê¸° ë°±ë¶„ìœ„ìˆ˜**: ë„ˆë¬´ ì‘ê±°ë‚˜ í° CFUë¥¼ ì œì™¸í•˜ê¸° ìœ„í•´ í¬ê¸° í•„í„°ë¥¼ ì ìš©í•˜ì„¸ìš”.
                    - **ì›í˜•ë„ ì„ê³„ê°’**: ëŒ€ëµì ìœ¼ë¡œ ì›í˜•ì¸ CFUë§Œ íƒì§€í•˜ê¸° ìœ„í•´ ì›í˜•ë„ í•„í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

                **ğŸ”µ ìë™ ê°ì§€ëœ CFU**ëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, **ğŸ”´ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ CFU**ëŠ” ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤. ì œê±°í•˜ë ¤ë©´ í•´ë‹¹ í¬ì¸íŠ¸ë¥¼ í´ë¦­í•˜ì„¸ìš”.
                """
            )

        # ì „ì²˜ë¦¬ ì ìš© ë²„íŠ¼ ì´ë²¤íŠ¸
        preprocess_button.click(
            preprocess_image,
            inputs=[
                input_image,
                to_grayscale,
                binary,
                binary_threshold,
                edge_detection,
                sharpen,
                sharpen_amount
            ],
            outputs=preprocessed_image_state
        )

        # ì´ë¯¸ì§€ ë¶„ì„ ë²„íŠ¼ ì´ë²¤íŠ¸ ìˆ˜ì •
        segment_button.click(
            fn=lambda orig, pre, method, input_size, iou, conf, bq, wc, mrc, minp, maxp, cir: segment_and_count_colonies(
                pre if pre is not None else orig, 
                method, 
                input_size, 
                iou, 
                conf, 
                bq, 
                wc, 
                mrc, 
                minp, 
                maxp, 
                cir
            ),
            inputs=[
                input_image,                   # original image
                preprocessed_image_state,     # preprocessed image (could be None)
                method_select,
                input_size_slider,
                iou_threshold_slider,
                conf_threshold_slider,
                better_quality_checkbox,
                withContours_checkbox,
                mask_random_color,
                min_area_percentile_slider,
                max_area_percentile_slider,
                circularity_threshold_slider
            ],
            outputs=[output_image, colony_count_text]
        )

        # ìˆ˜ë™ ë° ìë™ í¬ì¸íŠ¸ ì¶”ê°€/ì œê±° ì´ë²¤íŠ¸
        output_image.select(
            counter.add_or_remove_point,
            inputs=[output_image],
            outputs=[output_image, colony_count_text]
        )

        remove_point_button.click(
            counter.remove_last_point,
            inputs=[output_image],
            outputs=[output_image, colony_count_text]
        )

        remove_mode_button.click(
            counter.toggle_remove_mode,
            inputs=[],
            outputs=[output_image, remove_mode_text]
        )

        # ê²°ê³¼ ì €ì¥ ë²„íŠ¼ ì´ë²¤íŠ¸
        save_button.click(
            save_results,
            inputs=[input_image, output_image],
            outputs=save_output
        )

    # ì•± ì‹¤í–‰
    demo.launch()