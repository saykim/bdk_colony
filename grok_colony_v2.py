import os
from datetime import datetime
from ultralytics import YOLO
import gradio as gr
import torch
from PIL import Image, ImageDraw, ImageFilter, ImageEnhance
import numpy as np
import cv2
import pandas as pd
from pathlib import Path
import time

# AI ê¸°ë°˜ ë¶„í•  ëª¨ë¸ ë¡œë“œ (FastSAM-x ì‚¬ìš©)
model_path = 'weights/FastSAM-x.pt'
model = YOLO(model_path)

# ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA, MPS, CPU ìˆœìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else "cpu"
)

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤
class ImagePreprocessHistory:
    def __init__(self):
        self.original_image = None
        self.current_image = None
        self.history = []
        self.current_index = -1

    def set_original(self, image):
        if image is None:
            return None
        if not isinstance(image, Image.Image):
            image = Image.fromarray(np.array(image))
        self.original_image = image
        self.current_image = image
        self.history = [image]
        self.current_index = 0
        return image

    def add_state(self, image):
        if image is None:
            return None
        if not isinstance(image, Image.Image):
            image = Image.fromarray(np.array(image))
        self.history = self.history[:self.current_index + 1]
        self.history.append(image)
        self.current_index = len(self.history) - 1
        self.current_image = image
        return image

    def reset(self):
        if self.original_image is not None:
            self.current_image = self.original_image
            self.current_index = 0
            return self.original_image
        return None

    def undo(self):
        if self.current_index > 0:
            self.current_index -= 1
            self.current_image = self.history[self.current_index]
            return self.current_image
        return self.current_image

image_history = ImagePreprocessHistory()

# Colony ì¹´ìš´í„° í´ë˜ìŠ¤ (ìë™/ìˆ˜ë™ êµ¬ë¶„ ë° ê²°ê³¼ ì´ë¯¸ì§€ì— í†µí•© ìˆ«ì í‘œì‹œ)
class ColonyCounter:
    def __init__(self):
        self.manual_points = []  # ìˆ˜ë™ í¬ì¸íŠ¸
        self.auto_points = []    # ìë™ í¬ì¸íŠ¸
        self.current_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.original_image = None

    def reset(self):
        self.manual_points = []
        self.auto_points = []
        self.current_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.original_image = None

    def set_original_image(self, image):
        self.original_image = np.array(image)

    def toggle_remove_mode(self):
        self.remove_mode = not self.remove_mode
        img_with_points = self.draw_points()
        mode_text = "ğŸ”´ REMOVE MODE" if self.remove_mode else "ğŸŸ¢ ADD MODE"
        return img_with_points, mode_text

    def add_or_remove_point(self, image, evt: gr.SelectData):
        if self.current_image is None and image is not None:
            self.current_image = np.array(image)
        x, y = evt.index
        if self.remove_mode:
            closest_auto = self.find_closest_point(x, y, self.auto_points)
            closest_manual = self.find_closest_point(x, y, self.manual_points)
            if closest_auto is not None:
                self.auto_points.pop(closest_auto)
                self.auto_detected_count = len(self.auto_points)
            elif closest_manual is not None:
                self.manual_points.pop(closest_manual)
        else:
            self.manual_points.append((x, y))
        img_with_points = self.draw_points()
        return img_with_points, self.get_count_text()

    def find_closest_point(self, x, y, points, threshold=20):
        if not points:
            return None
        distances = [(np.sqrt((x - px) ** 2 + (y - py) ** 2), idx) for idx, (px, py) in enumerate(points)]
        closest_dist, closest_idx = min(distances, key=lambda item: item[0])
        return closest_idx if closest_dist < threshold else None

    def remove_last_point(self, image):
        if self.manual_points:
            self.manual_points.pop()
        img_with_points = self.draw_points()
        return img_with_points, self.get_count_text()

    def get_count_text(self):
        total_count = self.auto_detected_count + len(self.manual_points)
        return (f"ì „ì²´ CFU ìˆ˜: {total_count}\n"
                f"ğŸ¤– ìë™ ê°ì§€ëœ CFU: {self.auto_detected_count}\n"
                f"ğŸ‘† ìˆ˜ë™ ì¶”ê°€ëœ CFU: {len(self.manual_points)}")

    def draw_points(self):
        if self.current_image is None:
            return None
        img_with_points = self.current_image.copy()
        overlay = np.zeros_like(img_with_points)
        square_size = 30

        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.7
        font_thickness = 1
        outline_thickness = 3

        # ìë™ ê°ì§€ëœ í¬ì¸íŠ¸ í‘œì‹œ (ë…¹ìƒ‰ ì›)
        for idx, (x, y) in enumerate(self.auto_points, 1):
            cv2.circle(img_with_points, (x, y), 5, (0, 255, 0), -1)
            text = str(idx)
            (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
            text_x = int(x - text_width / 2)
            text_y = int(y - 10)
            
            # ê²€ì€ìƒ‰ ì™¸ê³½ì„  ì¶”ê°€
            for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:
                cv2.putText(img_with_points, text, 
                          (text_x + dx, text_y + dy), 
                          font, font_scale, (0, 0, 0), outline_thickness)
            
            # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸
            cv2.putText(img_with_points, text, (text_x, text_y), font, font_scale, (255, 0, 0), font_thickness)

        # ìˆ˜ë™ ì¶”ê°€ëœ í¬ì¸íŠ¸ í‘œì‹œ (ë¹¨ê°„ ì‚¬ê°í˜•)
        for idx, (x, y) in enumerate(self.manual_points, len(self.auto_points) + 1):
            pt1 = (int(x - square_size / 2), int(y - square_size / 2))
            pt2 = (int(x + square_size / 2), int(y + square_size / 2))
            cv2.rectangle(overlay, pt1, pt2, (0, 0, 255), -1)
            text = str(idx)
            (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
            text_x = int(x - text_width / 2)
            text_y = int(y - 10)
            
            # í•˜ì–€ìƒ‰ ì™¸ê³½ì„  ì¶”ê°€
            for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:
                cv2.putText(img_with_points, text, 
                          (text_x + dx, text_y + dy), 
                          font, font_scale, (255, 255, 255), outline_thickness)
            
            # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸
            cv2.putText(img_with_points, text, (text_x, text_y), font, font_scale, (255, 0, 0), font_thickness)

        cv2.addWeighted(overlay, 0.4, img_with_points, 1.0, 0, img_with_points)

        # ì „ì²´ ì¹´ìš´íŠ¸ í‘œì‹œ (ì™¼ìª½ í•˜ë‹¨)
        total_count = self.auto_detected_count + len(self.manual_points)
        count_text = f"Total: {total_count}"
        text_size = cv2.getTextSize(count_text, font, 1.5, 3)[0]
        margin = 20
        cv2.rectangle(img_with_points, 
                      (10, img_with_points.shape[0] - text_size[1] - margin * 2),
                      (text_size[0] + margin * 2, img_with_points.shape[0]),
                      (0, 0, 0), -1)
        cv2.putText(img_with_points, count_text, 
                    (margin, img_with_points.shape[0] - margin),
                    font, 1.5, (255, 255, 255), 3)

        return img_with_points

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (fastsam_prd_v4_okì—ì„œ ê°€ì ¸ì˜´)
def preprocess_image(input_image, to_grayscale, binary_threshold, edge_detection, sharpen):
    if input_image is None:
        return None
    img = np.array(input_image)
    if to_grayscale:
        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    if binary_threshold > 0:
        _, img = cv2.threshold(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), binary_threshold, 255, cv2.THRESH_BINARY)
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    if edge_detection:
        img = cv2.Canny(img, 100, 200)
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    if sharpen > 0:
        kernel = np.array([[0, -sharpen, 0], [-sharpen, 1 + 4*sharpen, -sharpen], [0, -sharpen, 0]])
        img = cv2.filter2D(img, -1, kernel)
    return Image.fromarray(img)

# grok_colony_v1ì˜ ê²€ì¶œ ë¡œì§ ìœ ì§€
def segment_and_count_colonies(
    input_image,
    conf_threshold=0.25,
    iou_threshold=0.7,
    circularity_threshold=0.8,
    draw_contours=True,
    mask_random_color=True,
    input_size=1024,
    better_quality=False,
    min_area_percentile=1,
    max_area_percentile=99,
    use_dish_filtering=False,  # ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ í™œì„±í™” ì˜µì…˜
    dish_overlap_threshold=0.5,  # ë°°ì–‘ ì ‘ì‹œì™€ ê²¹ì³ì•¼ í•˜ëŠ” ìµœì†Œ ë¹„ìœ¨
    progress=gr.Progress()
):
    if input_image is None:
        return None, "ì´ë¯¸ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
    
    progress(0.1, desc="ì´ˆê¸°í™” ì¤‘...")
    counter.reset()
    counter.set_original_image(input_image)
    
    # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥
    original_width, original_height = input_image.size
    
    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë¹„ìœ¨ ìœ ì§€)
    w, h = input_image.size
    scale = input_size / max(w, h)
    new_w, new_h = int(w * scale), int(h * scale)
    input_resized = input_image.resize((new_w, new_h))
    image_np = np.array(input_resized)

    progress(0.3, desc="AI ë¶„ì„ ì¤‘...")
    results = model.predict(
        image_np,
        device=device,
        conf=conf_threshold,
        iou=iou_threshold,
        imgsz=input_size,
        retina_masks=True
    )

    if not results[0].masks:
        return image_np, "CFUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    progress(0.6, desc="ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")
    processed_image = image_np.copy()
    counter.auto_points = []

    # ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì²˜ë¦¬
    dish_mask = None
    dish_idx = -1
    if use_dish_filtering:
        progress(0.65, desc="ë°°ì–‘ ì ‘ì‹œ ê°ì§€ ì¤‘...")
        # ê°€ì¥ í° ë§ˆìŠ¤í¬ë¥¼ ë°°ì–‘ ì ‘ì‹œë¡œ ì‹ë³„
        annotations = results[0].masks.data
        if len(annotations) > 0:
            areas = [np.sum(ann.cpu().numpy()) for ann in annotations]
            dish_idx = np.argmax(areas)
            dish_annotation = annotations[dish_idx].cpu().numpy()
            dish_mask = dish_annotation > 0
            
            # ë°°ì–‘ ì ‘ì‹œ ìœ¤ê³½ì„  ì‹œê°í™” (íŒŒë€ìƒ‰)
            dish_contours, _ = cv2.findContours(dish_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(processed_image, dish_contours, -1, (0, 0, 255), 2)

    # ë§ˆìŠ¤í¬ ë©´ì  ê³„ì‚° ë° í•„í„°ë§
    all_masks = []
    all_areas = []
    
    for idx, mask in enumerate(results[0].masks.data):
        mask_np = mask.cpu().numpy()
        
        # ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì ìš©
        if use_dish_filtering and dish_mask is not None:
            # ë°°ì–‘ ì ‘ì‹œì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if idx == dish_idx:
                continue
                
            # ë°°ì–‘ ì ‘ì‹œ ë‚´ë¶€ ì—¬ë¶€ í™•ì¸ (ë§ˆìŠ¤í¬ êµì§‘í•©)
            overlap_ratio = np.sum(mask_np * dish_mask) / np.sum(mask_np)
            if overlap_ratio < dish_overlap_threshold:
                continue  # ë°°ì–‘ ì ‘ì‹œì™€ ì¶©ë¶„íˆ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
        
        contours, _ = cv2.findContours(mask_np.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            area = cv2.contourArea(contour)
            all_areas.append(area)
            all_masks.append((mask_np, contour, area))
    
    # ë©´ì  ê¸°ì¤€ í•„í„°ë§
    if all_areas:
        min_area = np.percentile(all_areas, min_area_percentile)
        max_area = np.percentile(all_areas, max_area_percentile)
    else:
        min_area = 0
        max_area = float('inf')
    
    # í–¥ìƒëœ í’ˆì§ˆ ì„¤ì • ì ìš©
    if better_quality:
        # ë” ì •êµí•œ ìœ¤ê³½ì„  ê·¸ë¦¬ê¸° (ë‘ê»˜ ì¡°ì •)
        contour_thickness = 2
        # ë” ì„ ëª…í•œ ìƒ‰ìƒ ì‚¬ìš©
        color_min = 120 if better_quality else 100
        color_max = 255 if better_quality else 200
    else:
        contour_thickness = 2
        color_min = 100
        color_max = 200
    
    # í•„í„°ë§ëœ ë§ˆìŠ¤í¬ ì²˜ë¦¬
    for mask_np, contour, area in all_masks:
        if min_area <= area <= max_area:
            perimeter = cv2.arcLength(contour, True)
            if perimeter == 0:
                continue
            circularity = 4 * np.pi * (area / (perimeter * perimeter))
            if circularity >= circularity_threshold:
                M = cv2.moments(contour)
                if M["m00"] != 0:
                    cX = int(M["m10"] / M["m00"])
                    cY = int(M["m01"] / M["m00"])
                    counter.auto_points.append((cX, cY))
                    if draw_contours:
                        color = tuple(np.random.randint(color_min, color_max, 3).tolist()) if mask_random_color else (0, 255, 0)
                        cv2.drawContours(processed_image, [contour], -1, color, contour_thickness)

    counter.auto_detected_count = len(counter.auto_points)
    counter.current_image = processed_image

    progress(1.0, desc="ì™„ë£Œ!")
    img_with_points = counter.draw_points()
    
    # ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë‹¤ì‹œ ì¡°ì •
    img_with_points_pil = Image.fromarray(img_with_points)
    try:
        # PIL 9.0.0 ì´ìƒ
        resampling_filter = Image.Resampling.LANCZOS
    except AttributeError:
        # PIL 9.0.0 ë¯¸ë§Œ
        resampling_filter = Image.LANCZOS
    img_with_points_resized = img_with_points_pil.resize((original_width, original_height), resampling_filter)
    
    return np.array(img_with_points_resized), counter.get_count_text()

# ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ (ì›ë³¸ê³¼ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥)
def save_results(original_image, processed_image):
    output_dir = "outputs"
    os.makedirs(output_dir, exist_ok=True)
    unique_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    original_path = os.path.join(output_dir, f"original_{unique_id}.png")
    processed_path = os.path.join(output_dir, f"ì¹´ìš´íŒ…ì™„ë£Œ_{unique_id}.png")
    original_image.save(original_path)
    Image.fromarray(processed_image).save(processed_path)
    return f"ì €ì¥ ì™„ë£Œ:\n- ì›ë³¸: {original_path}\n- ê²°ê³¼: {processed_path}"

# UI ë””ìì¸ (fastsam_prd_v4_ok ê¸°ë°˜)
css = """
body { font-family: Arial, sans-serif; background-color: #f0f0f0; }
.button-primary { 
    background-color: #4CAF50; 
    color: white; 
    border: none; 
    padding: 10px 20px; 
    border-radius: 5px; 
    transition: all 0.3s ease;
}
.button-primary:hover { 
    background-color: #45a049; 
    transform: translateY(-2px);
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}
.button-secondary {
    background-color: #f1f1f1;
    color: #333;
    border: 1px solid #ddd;
    padding: 8px 16px;
    border-radius: 4px;
    transition: all 0.3s ease;
}
.button-secondary:hover {
    background-color: #e0e0e0;
    transform: translateY(-1px);
}
.result-text { 
    background: #fff; 
    border-left: 5px solid #4CAF50; 
    padding: 10px; 
    border-radius: 0 4px 4px 0;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
}
.accordion-header {
    font-weight: bold;
    color: #333;
}
.section-title {
    font-size: 1.2em;
    font-weight: bold;
    color: #2E7D32;
    margin-bottom: 10px;
    padding-bottom: 5px;
    border-bottom: 2px solid #4CAF50;
}
.filtering-active {
    background-color: #e8f5e9;
    color: #2E7D32;
    padding: 3px 6px;
    border-radius: 4px;
    font-weight: bold;
    display: inline-block;
    animation: pulse 2s infinite;
}
@keyframes pulse {
    0% { opacity: 0.7; }
    50% { opacity: 1; }
    100% { opacity: 0.7; }
}
"""

counter = ColonyCounter()

with gr.Blocks(theme=gr.themes.Default(), css=css) as demo:
    gr.Markdown("""
    # ğŸ”¬ Colony Counter
    ## AI ê¸°ë°˜ CFU ìë™ ê°ì§€ ë° ìˆ˜ë™ í¸ì§‘
    
    ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ FastSAM AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°°ì–‘ì ‘ì‹œì˜ CFU(Colony Forming Units)ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì¹´ìš´íŒ…í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìëŠ” ìë™ ê°ì§€ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í¸ì§‘í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    
    **ìƒˆë¡œìš´ ê¸°ëŠ¥**: ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ê°€ì¥ í° ë§ˆìŠ¤í¬ë¥¼ ë°°ì–‘ ì ‘ì‹œë¡œ ì‹ë³„í•˜ê³ , ì ‘ì‹œ ë‚´ë¶€ì˜ CFUë§Œ ì¹´ìš´íŒ…í•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ ë°°ì–‘ ì ‘ì‹œ ì™¸ë¶€ì˜ ë…¸ì´ì¦ˆë‚˜ ì˜¤ê°ì§€ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("<div class='section-title'>ğŸ“ ì´ë¯¸ì§€ ì—…ë¡œë“œ</div>")
            input_image = gr.Image(type="pil", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ")
            
            with gr.Accordion("ğŸ› ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬", open=False, elem_classes="accordion-header"):
                gr.Markdown("ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê¸° ì „ì— ì „ì²˜ë¦¬ ì˜µì…˜ì„ ì ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                to_grayscale = gr.Checkbox(label="í‘ë°± ë³€í™˜", value=False, info="ì´ë¯¸ì§€ë¥¼ í‘ë°±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                binary_threshold = gr.Slider(0, 255, 0, label="ë°”ì´ë„ˆë¦¬ ì„ê³„ê°’", info="0ë³´ë‹¤ í¬ë©´ ì´ì§„í™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
                edge_detection = gr.Checkbox(label="ì—ì§€ ê²€ì¶œ", value=False, info="ì´ë¯¸ì§€ì˜ ì—ì§€ë¥¼ ê²€ì¶œí•©ë‹ˆë‹¤.")
                sharpen = gr.Slider(0, 1, 0, label="ìƒ¤í”ˆ ê°•ë„", info="ì´ë¯¸ì§€ì˜ ì„ ëª…ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.")
                
                with gr.Row():
                    preprocess_btn = gr.Button("ì „ì²˜ë¦¬ ì ìš©", variant="secondary", elem_classes="button-secondary")
                    reset_btn = gr.Button("ì´ˆê¸°í™”", variant="secondary", elem_classes="button-secondary")
                    undo_btn = gr.Button("ì‹¤í–‰ ì·¨ì†Œ", variant="secondary", elem_classes="button-secondary")
            
            # FastSAM ë¶„ì„ ì„¤ì • ì¶”ê°€
            with gr.Accordion("âš™ï¸ ë¶„ì„ ì„¤ì •", open=True) as analysis_accordion:
                accordion_label = gr.Textbox(visible=False, value="âš™ï¸ ë¶„ì„ ì„¤ì •")
                
                with gr.Tab("ì¼ë°˜"):
                    input_size_slider = gr.Slider(
                        512, 2048, 1024,
                        step=64,
                        label="ì…ë ¥ í¬ê¸°",
                        info="í¬ê¸°ê°€ í´ìˆ˜ë¡ ì •í™•ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ ì†ë„ëŠ” ëŠë ¤ì§‘ë‹ˆë‹¤."
                    )
                    better_quality_checkbox = gr.Checkbox(
                        label="í–¥ìƒëœ í’ˆì§ˆ",
                        value=False,
                        info="ì†ë„ë¥¼ í¬ìƒí•˜ê³  ì¶œë ¥ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
                    )
                    withContours_checkbox = gr.Checkbox(
                        label="ìœ¤ê³½ì„  í‘œì‹œ",
                        value=True,
                        info="CFUì˜ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
                    )
                    mask_random_color_checkbox = gr.Checkbox(
                        label="ëœë¤ ìƒ‰ìƒ",
                        value=True,
                        info="CFU ë§ˆìŠ¤í¬ì— ëœë¤ ìƒ‰ìƒì„ ì ìš©í•©ë‹ˆë‹¤."
                    )

                with gr.Tab("AI íƒì§€"):
                    iou_threshold_slider = gr.Slider(
                        0.1, 0.9, 0.7,
                        step=0.1,
                        label="IOU ì„ê³„ê°’",
                        info="ë†’ì„ìˆ˜ë¡ ê²¹ì¹¨ ê¸°ì¤€ì´ ì—„ê²©í•´ì§‘ë‹ˆë‹¤."
                    )
                    conf_threshold_slider = gr.Slider(
                        0.1, 0.9, 0.25,
                        step=0.05,
                        label="ì‹ ë¢°ë„ ì„ê³„ê°’",
                        info="ë†’ì„ìˆ˜ë¡ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íƒì§€ë§Œ í‘œì‹œë©ë‹ˆë‹¤."
                    )
                    circularity_threshold_slider = gr.Slider(
                        0.0, 1.0, 0.8,
                        step=0.01,
                        label="ì›í˜•ë„ ì„ê³„ê°’",
                        info="ëŒ€ëµì ìœ¼ë¡œ ì›í˜•ì¸ CFUë§Œ ê°ì§€í•©ë‹ˆë‹¤ (1 = ì™„ë²½í•œ ì›)."
                    )

                with gr.Tab("í¬ê¸° í•„í„°"):
                    min_area_percentile_slider = gr.Slider(
                        0, 10, 1,
                        step=1,
                        label="ìµœì†Œ í¬ê¸° ë°±ë¶„ìœ„ìˆ˜",
                        info="ë” ì‘ì€ CFUë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤ (1ì€ ê°€ì¥ ì‘ì€ 1% í•„í„°ë§)."
                    )
                    max_area_percentile_slider = gr.Slider(
                        90, 100, 99,
                        step=1,
                        label="ìµœëŒ€ í¬ê¸° ë°±ë¶„ìœ„ìˆ˜",
                        info="ë” í° ê°ì²´ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤ (99ëŠ” ê°€ì¥ í° 1% í•„í„°ë§)."
                    )

                with gr.Tab("ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§"):
                    gr.Markdown("""
                    ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ì€ ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ í° ë§ˆìŠ¤í¬ë¥¼ ë°°ì–‘ ì ‘ì‹œë¡œ ì‹ë³„í•˜ê³ , í•´ë‹¹ ë°°ì–‘ ì ‘ì‹œ ë‚´ë¶€ì— ìˆëŠ” CFUë§Œ ì¹´ìš´íŒ…í•©ë‹ˆë‹¤.
                    ì´ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤:
                    - ë°°ê²½ì— ë…¸ì´ì¦ˆê°€ ë§ì€ ì´ë¯¸ì§€
                    - ë°°ì–‘ ì ‘ì‹œ ì™¸ë¶€ì˜ ì˜¤íƒì§€ë¥¼ ì œê±°í•˜ê³  ì‹¶ì„ ë•Œ
                    - ì—¬ëŸ¬ ì ‘ì‹œê°€ í•œ ì´ë¯¸ì§€ì— ìˆëŠ” ê²½ìš° ê°€ì¥ í° ì ‘ì‹œë§Œ ë¶„ì„í•˜ê³  ì‹¶ì„ ë•Œ
                    
                    í™œì„±í™”í•˜ë©´ ë°°ì–‘ ì ‘ì‹œ ìœ¤ê³½ì„ ì´ íŒŒë€ìƒ‰ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
                    """)
                    use_dish_filtering_checkbox = gr.Checkbox(
                        label="ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì‚¬ìš©",
                        value=True,
                        info="ê°€ì¥ í° ì˜ì—­ì„ ë°°ì–‘ ì ‘ì‹œë¡œ ì¸ì‹í•˜ê³  ë‚´ë¶€ CFUë§Œ ê°ì§€í•©ë‹ˆë‹¤."
                    )
                    dish_overlap_threshold_slider = gr.Slider(
                        0.1, 1.0, 0.5,
                        step=0.1,
                        label="ë°°ì–‘ ì ‘ì‹œ ê²¹ì¹¨ ì„ê³„ê°’",
                        info="CFUê°€ ë°°ì–‘ ì ‘ì‹œì™€ ìµœì†Œí•œ ì´ ë¹„ìœ¨ë§Œí¼ ê²¹ì³ì•¼ í•©ë‹ˆë‹¤ (0.5 = 50%)."
                    )
                
            analyze_btn = gr.Button("ğŸ” ë¶„ì„ ì‹œì‘", variant="primary", elem_classes="button-primary")
        
        with gr.Column():
            gr.Markdown("<div class='section-title'>ğŸ“Š ë¶„ì„ ê²°ê³¼</div>")
            output_image = gr.Image(type="numpy", interactive=True, label="ê²°ê³¼ ì´ë¯¸ì§€")
            count_text = gr.Textbox(label="ì¹´ìš´íŠ¸ ê²°ê³¼", elem_classes="result-text")
            
            gr.Markdown("### ìˆ˜ë™ í¸ì§‘")
            gr.Markdown("ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ì—¬ í¬ì¸íŠ¸ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            with gr.Row():
                remove_mode_btn = gr.Button("ğŸ”„ ëª¨ë“œ ì „í™˜", variant="secondary", elem_classes="button-secondary")
                mode_text = gr.Textbox(value="ğŸŸ¢ ADD MODE", interactive=False)
                remove_last_btn = gr.Button("â†©ï¸ ìµœê·¼ í¬ì¸íŠ¸ ì‚­ì œ", variant="secondary", elem_classes="button-secondary")
            
            save_btn = gr.Button("ğŸ’¾ ê²°ê³¼ ì €ì¥", variant="primary", elem_classes="button-primary")
            save_output = gr.Textbox(label="ì €ì¥ ê²°ê³¼", interactive=False, elem_classes="result-text")

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    input_image.upload(
        lambda img: (image_history.set_original(img), "ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ"),
        inputs=input_image,
        outputs=[input_image, count_text]
    )

    preprocess_btn.click(
        preprocess_image,
        inputs=[input_image, to_grayscale, binary_threshold, edge_detection, sharpen],
        outputs=input_image
    ).then(
        lambda img: image_history.add_state(img),
        inputs=input_image,
        outputs=input_image
    )

    reset_btn.click(
        lambda: image_history.reset(),
        inputs=[],
        outputs=input_image
    )

    undo_btn.click(
        lambda: image_history.undo(),
        inputs=[],
        outputs=input_image
    )

    analyze_btn.click(
        segment_and_count_colonies,
        inputs=[
            input_image,
            conf_threshold_slider,
            iou_threshold_slider,
            circularity_threshold_slider,
            withContours_checkbox,
            mask_random_color_checkbox,
            input_size_slider,
            better_quality_checkbox,
            min_area_percentile_slider,
            max_area_percentile_slider,
            use_dish_filtering_checkbox,
            dish_overlap_threshold_slider
        ],
        outputs=[output_image, count_text]
    )

    output_image.select(
        counter.add_or_remove_point,
        inputs=[output_image],
        outputs=[output_image, count_text]
    )

    remove_mode_btn.click(
        counter.toggle_remove_mode,
        inputs=[],
        outputs=[output_image, mode_text]
    )

    remove_last_btn.click(
        counter.remove_last_point,
        inputs=[output_image],
        outputs=[output_image, count_text]
    )

    save_btn.click(
        save_results,
        inputs=[input_image, output_image],
        outputs=save_output
    )

if __name__ == "__main__":
    demo.launch()