import os
import sys
import warnings
from datetime import datetime
from ultralytics import YOLO
import gradio as gr
import torch
from PIL import Image, ImageDraw, ImageFilter, ImageEnhance
import numpy as np
import cv2
import pandas as pd
from pathlib import Path
import time
import glob
import json
import webbrowser
import platform
import subprocess
import gc

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=FutureWarning)

# ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
DEFAULT_OUTPUT_DIR = os.path.join(str(Path.home()), 'colony_counter_results')
os.makedirs(DEFAULT_OUTPUT_DIR, exist_ok=True)

# ëª¨ë¸ ê²½ë¡œ ë° ì¡´ì¬ í™•ì¸ (v6 ì•ˆì •ì„±)
model_path = 'weights/FastSAM-x.pt'
if not os.path.exists(model_path):
    print(f"ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ '{model_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("FastSAM-x.pt íŒŒì¼ì„ weights í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”.")
    sys.exit(1)

# AI ê¸°ë°˜ ë¶„í•  ëª¨ë¸ ë¡œë“œ
try:
    model = YOLO(model_path)
    print(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    sys.exit(1)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else "cpu"
)
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")


class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    @staticmethod
    def clear_gpu_memory():
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
    
    @staticmethod
    def limit_image_size(image, max_dimension=4096):
        """ì´ë¯¸ì§€ í¬ê¸° ì œí•œ"""
        if hasattr(image, 'size'):
            w, h = image.size
            if max(w, h) > max_dimension:
                scale = max_dimension / max(w, h)
                new_w, new_h = int(w * scale), int(h * scale)
                return image.resize((new_w, new_h), Image.LANCZOS)
        return image


class GPUMemoryMonitor:
    """GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
    def __init__(self, threshold_gb=1.0):
        self.threshold_gb = threshold_gb
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    def get_free_memory(self):
        """ì—¬ìœ  GPU ë©”ëª¨ë¦¬ í™•ì¸ (GB)"""
        if not torch.cuda.is_available():
            return float('inf')
        return torch.cuda.mem_get_info()[0] / 1024**3
    
    def check_memory(self):
        """ë©”ëª¨ë¦¬ ì²´í¬ ë° ì •ë¦¬"""
        if self.get_free_memory() < self.threshold_gb:
            self.cleanup()
    
    def cleanup(self):
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()


class ImagePreprocessHistory:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self):
        self.original_image = None
        self.current_image = None
        self.history = []
        self.current_index = -1

    def set_original(self, image):
        """ì›ë³¸ ì´ë¯¸ì§€ ì„¤ì •"""
        if image is None:
            return None
        if not isinstance(image, Image.Image):
            try:
                image = Image.fromarray(image)
            except Exception as e:
                print(f"Error converting image: {str(e)}")
                return None
        self.original_image = image
        self.current_image = image
        self.history = [image]
        self.current_index = 0
        return image

    def add_state(self, image):
        """ìƒˆë¡œìš´ ì´ë¯¸ì§€ ìƒíƒœ ì¶”ê°€"""
        if image is None:
            return None
        if not isinstance(image, Image.Image):
            try:
                image = Image.fromarray(image)
            except Exception as e:
                print(f"Error converting image: {str(e)}")
                return None
        self.history = self.history[:self.current_index + 1]
        self.history.append(image)
        self.current_index = len(self.history) - 1
        self.current_image = image
        return image

    def reset(self):
        """ì›ë³¸ ì´ë¯¸ì§€ë¡œ ë³µì›"""
        if self.original_image is not None:
            self.current_image = self.original_image
            self.current_index = 0
            return self.original_image
        return None

    def undo(self):
        """ì´ì „ ìƒíƒœë¡œ ë³µì›"""
        if self.current_index > 0:
            self.current_index -= 1
            self.current_image = self.history[self.current_index]
            return self.current_image
        return self.current_image


def calculate_circularity_robust(mask):
    """í–¥ìƒëœ ì›í˜•ë„ ê³„ì‚°"""
    contours, _ = cv2.findContours(
        mask.astype(np.uint8), 
        cv2.RETR_EXTERNAL, 
        cv2.CHAIN_APPROX_SIMPLE
    )
    
    if not contours:
        return 0.0
    
    # ëª¨ë“  ì™¸ë¶€ contour ë³‘í•© (êµ¬ë© ìˆëŠ” ì½œë¡œë‹ˆ ëŒ€ì‘)
    if len(contours) > 1:
        # ê°€ì¥ í° contour ì°¾ê¸°
        largest_contour = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(largest_contour)
        perimeter = cv2.arcLength(largest_contour, True)
    else:
        area = cv2.contourArea(contours[0])
        perimeter = cv2.arcLength(contours[0], True)
    
    if perimeter == 0:
        return 0.0
    
    # ê¸°ë³¸ ì›í˜•ë„
    circularity = 4 * np.pi * area / (perimeter ** 2)
    
    # Convex Hullì„ ì´ìš©í•œ ë³´ì •
    if contours:
        hull = cv2.convexHull(contours[0])
        hull_area = cv2.contourArea(hull)
        solidity = area / hull_area if hull_area > 0 else 0
        
        # ì›í˜•ë„ì™€ solidityë¥¼ ì¡°í•©
        adjusted_circularity = circularity * 0.7 + solidity * 0.3
        return min(adjusted_circularity, 1.0)
    
    return circularity


def identify_petri_dish(annotations, image_shape):
    """í–¥ìƒëœ ë°°ì–‘ì ‘ì‹œ ì‹ë³„ ì•Œê³ ë¦¬ì¦˜"""
    if len(annotations) < 2:
        return None, list(range(len(annotations)))
    
    h, w = image_shape[:2]
    image_area = h * w
    candidates = []
    
    for idx, ann in enumerate(annotations):
        mask = ann.cpu().numpy()
        if mask.ndim == 3:
            mask = mask[0]
        mask = mask > 0
        
        # ê¸°ë³¸ ì†ì„± ê³„ì‚°
        area = np.sum(mask)
        area_ratio = area / image_area
        
        # ì¤‘ì‹¬ì ê³¼ ì´ë¯¸ì§€ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬
        y_indices, x_indices = np.where(mask)
        if len(x_indices) == 0:
            continue
            
        center_x = np.mean(x_indices)
        center_y = np.mean(y_indices)
        img_center_x, img_center_y = w / 2, h / 2
        center_distance = np.sqrt((center_x - img_center_x)**2 + (center_y - img_center_y)**2)
        center_distance_ratio = center_distance / np.sqrt(w**2 + h**2)
        
        # ì›í˜•ë„ ê³„ì‚°
        circularity = calculate_circularity_robust(mask)
        
        # ìµœì†Œ ì™¸ì ‘ì› ê³„ì‚°
        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        fill_ratio = 0
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            (x, y), radius = cv2.minEnclosingCircle(largest_contour)
            enclosing_circle_area = np.pi * radius ** 2
            fill_ratio = area / enclosing_circle_area if enclosing_circle_area > 0 else 0
        
        # ë°°ì–‘ì ‘ì‹œ ì ìˆ˜ ê³„ì‚°
        score = 0
        
        # 1. ë©´ì  ë¹„ìœ¨ (ì „ì²´ ì´ë¯¸ì§€ì˜ 20-80%)
        if 0.2 < area_ratio < 0.8:
            score += 30
        
        # 2. ì¤‘ì‹¬ ê·¼ì ‘ë„ (ì´ë¯¸ì§€ ì¤‘ì‹¬ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        if center_distance_ratio < 0.2:
            score += 25
        elif center_distance_ratio < 0.3:
            score += 15
        
        # 3. ì›í˜•ë„ (0.7 ì´ìƒ)
        if circularity > 0.7:
            score += 25
        elif circularity > 0.5:
            score += 15
        
        # 4. ì±„ì›€ ë¹„ìœ¨ (ì›í˜• ëŒ€ë¹„ ì‹¤ì œ ë©´ì )
        if fill_ratio > 0.8:
            score += 20
        
        candidates.append({
            'idx': idx,
            'score': score,
            'area': area,
            'circularity': circularity,
            'center_distance_ratio': center_distance_ratio
        })
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    candidates.sort(key=lambda x: x['score'], reverse=True)
    
    # ìµœê³  ì ìˆ˜ê°€ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ë§Œ ë°°ì–‘ì ‘ì‹œë¡œ ì¸ì •
    if candidates and candidates[0]['score'] >= 60:
        dish_idx = candidates[0]['idx']
        colony_indices = [c['idx'] for c in candidates if c['idx'] != dish_idx]
        return dish_idx, colony_indices
    
    # ë°°ì–‘ì ‘ì‹œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
    return None, list(range(len(annotations)))


class ColonyCounter:
    """ì½œë¡œë‹ˆ ì¹´ìš´íŒ… ë° í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤ (í†µí•© ë²„ì „)"""
    def __init__(self):
        # í¬ì¸íŠ¸ ê´€ë¦¬
        self.manual_points = []  # ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ í¬ì¸íŠ¸
        self.auto_points = []    # ìë™ìœ¼ë¡œ ê°ì§€ëœ í¬ì¸íŠ¸
        self.removed_history = []  # ì‚­ì œëœ í¬ì¸íŠ¸ ê¸°ë¡ (ë³µì›ìš©)
        
        # ì´ë¯¸ì§€ ê´€ë¦¬
        self.current_image = None  # í˜„ì¬ í‘œì‹œ ì¤‘ì¸ ì´ë¯¸ì§€
        self.original_image = None  # ì›ë³¸ ì´ë¯¸ì§€
        self.base_image = None     # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì´ë¯¸ì§€
        
        # ë§ˆìŠ¤í¬ ë° ì£¼ì„ ë°ì´í„°
        self.auto_annotations = []  # ìë™ ê°ì§€ëœ ì½œë¡œë‹ˆ ì• ë…¸í…Œì´ì…˜
        self.dish_annotation = None  # í˜íŠ¸ë¦¬ ì ‘ì‹œ ì• ë…¸í…Œì´ì…˜
        self.colony_masks = []      # ì½œë¡œë‹ˆ ë§ˆìŠ¤í¬ ë¦¬ìŠ¤íŠ¸
        self.dish_mask = None       # ë°°ì–‘ ì ‘ì‹œ ë§ˆìŠ¤í¬
        
        # ìƒíƒœ ê´€ë¦¬
        self.auto_detected_count = 0  # ìë™ ê°ì§€ëœ ì½œë¡œë‹ˆ ìˆ˜
        self.remove_mode = False      # ì œê±° ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        self.scale_factor = 1.0       # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ë¹„ìœ¨
        self.last_method = "FastSAM"  # ë§ˆì§€ë§‰ ì‚¬ìš© ë°©ë²•

    def reset(self):
        """ëª¨ë“  ìƒíƒœ ì´ˆê¸°í™”"""
        self.manual_points = []
        self.auto_points = []
        self.removed_history = []
        self.auto_annotations = []
        self.colony_masks = []
        self.dish_annotation = None
        self.dish_mask = None
        self.current_image = None
        self.base_image = None
        self.original_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.scale_factor = 1.0
        self.last_method = "FastSAM"

    def set_original_image(self, image):
        """ì›ë³¸ ì´ë¯¸ì§€ ì„¤ì •"""
        if isinstance(image, Image.Image):
            self.original_image = np.array(image)
            self.base_image = self.original_image.copy()
        else:
            self.original_image = image
            self.base_image = image.copy() if image is not None else None

    def toggle_remove_mode(self):
        """í¸ì§‘ ëª¨ë“œ ì „í™˜ (ì¶”ê°€/ì œê±° ëª¨ë“œ)"""
        self.remove_mode = not self.remove_mode
        current_img = self.draw_points()
        mode_html = self._get_mode_html()
        return current_img, mode_html

    def _get_mode_html(self):
        """í˜„ì¬ ëª¨ë“œë¥¼ HTML í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
        if self.remove_mode:
            return "<span style='color: red; font-weight: bold;'>ğŸ”´ REMOVE MODE</span>"
        else:
            return "<span style='color: green; font-weight: bold;'>ğŸŸ¢ ADD MODE</span>"

    def add_or_remove_point(self, image, evt: gr.SelectData):
        """ì•ˆì „í•œ ì¸ë±ìŠ¤ ì²˜ë¦¬ë¡œ í¬ì¸íŠ¸ ì¶”ê°€/ì œê±°"""
        try:
            if self.base_image is None and image is not None:
                self.base_image = np.array(image)
                self.current_image = self.base_image.copy()

            x, y = evt.index

            if self.remove_mode:
                closest_idx, is_auto = self.find_closest_point(x, y)
                
                if closest_idx is not None:
                    if is_auto:
                        # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
                        if 0 <= closest_idx < len(self.auto_points):
                            removed_point = self.auto_points[closest_idx]
                            removed_annotation = None
                            
                            # ê° ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ ë…ë¦½ì ìœ¼ë¡œ í™•ì¸
                            if closest_idx < len(self.auto_annotations):
                                removed_annotation = self.auto_annotations[closest_idx]
                                del self.auto_annotations[closest_idx]
                            
                            if closest_idx < len(self.colony_masks):
                                del self.colony_masks[closest_idx]
                            
                            self.removed_history.append(("auto", closest_idx, removed_point, removed_annotation))
                            del self.auto_points[closest_idx]
                            self.auto_detected_count = max(0, self.auto_detected_count - 1)
                    else:
                        # ìˆ˜ë™ í¬ì¸íŠ¸ ì¸ë±ìŠ¤ ê²€ì¦
                        manual_idx = closest_idx - len(self.auto_points)
                        if 0 <= manual_idx < len(self.manual_points):
                            removed_point = self.manual_points[manual_idx]
                            self.removed_history.append(("manual", manual_idx, removed_point, None))
                            del self.manual_points[manual_idx]
            else:
                self.manual_points.append((x, y))

            img_with_points = self.draw_points()
            return img_with_points, self.get_count_text()
        except Exception as e:
            print(f"Error in add_or_remove_point: {str(e)}")
            import traceback
            traceback.print_exc()
            return image, self.get_count_text()

    def find_closest_point(self, x, y, threshold=30):
        """ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ ì°¾ê¸°"""
        all_points = self.auto_points + self.manual_points
        if not all_points:
            return None, None
        
        distances = []
        for idx, (px, py) in enumerate(all_points):
            dist = np.sqrt((x - px) ** 2 + (y - py) ** 2)
            distances.append((dist, idx))

        if not distances:
            return None, None

        closest_dist, closest_idx = min(distances, key=lambda item: item[0])
        if closest_dist < threshold:
            is_auto = (closest_idx < len(self.auto_points))
            return closest_idx, is_auto
        return None, None

    def remove_last_point(self, image):
        """ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ê°€ëœ ìˆ˜ë™ í¬ì¸íŠ¸ ì œê±°"""
        try:
            if self.manual_points:
                removed_point = self.manual_points.pop()
                self.removed_history.append(('manual', len(self.manual_points), removed_point, None))
                img_with_points = self.draw_points()
                return img_with_points, self.get_count_text()
            return image, self.get_count_text()
        except Exception as e:
            print(f"Error in remove_last_point: {str(e)}")
            return image, self.get_count_text()

    def undo_last_removal(self, image):
        """ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚­ì œëœ í¬ì¸íŠ¸ ë³µì› (v90 ê¸°ëŠ¥)"""
        try:
            if not self.removed_history:
                return image, self.get_count_text() + "\në³µì›í•  í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            last_removal = self.removed_history.pop()
            removal_type, index, point, annotation = last_removal
            
            if removal_type == "auto":
                # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì‚½ì…
                if 0 <= index <= len(self.auto_points):
                    self.auto_points.insert(index, point)
                else:
                    self.auto_points.append(point)
                
                if annotation is not None:
                    if 0 <= index <= len(self.auto_annotations):
                        self.auto_annotations.insert(index, annotation)
                    else:
                        self.auto_annotations.append(annotation)
                self.auto_detected_count += 1
            else:
                # ìˆ˜ë™ í¬ì¸íŠ¸ ë³µì›
                if 0 <= index <= len(self.manual_points):
                    self.manual_points.insert(index, point)
                else:
                    self.manual_points.append(point)
            
            img_with_points = self.draw_points()
            return img_with_points, self.get_count_text() + "\ní¬ì¸íŠ¸ê°€ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"Error in undo_last_removal: {str(e)}")
            return image, self.get_count_text() + f"\në³µì› ì¤‘ ì˜¤ë¥˜: {str(e)}"

    def get_count_text(self):
        """ì¹´ìš´íŠ¸ ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±"""
        total = self.auto_detected_count + len(self.manual_points)
        return (f"ì´ ì½œë¡œë‹ˆ ìˆ˜: {total}\n"
                f"ğŸ¤– ìë™ ê°ì§€: {self.auto_detected_count}\n"
                f"ğŸ‘† ìˆ˜ë™ ì¶”ê°€: {len(self.manual_points)}")

    def draw_points(self):
        """í¬ì¸íŠ¸ì™€ ë²ˆí˜¸ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
        try:
            if self.base_image is None:
                return None

            img_with_points = self.base_image.copy()
            overlay = np.zeros_like(img_with_points)

            # ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥í•œ ë³€ìˆ˜ë“¤
            square_size = 30
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.8
            font_thickness = 2
            outline_thickness = 4

            # ìƒ‰ìƒ ì„¤ì • (B, G, R)
            AUTO_TEXT_COLOR = (255, 255, 255)
            AUTO_OUTLINE_COLOR = (0, 0, 0)
            MANUAL_RECT_COLOR = (0, 0, 255)
            MANUAL_BORDER_COLOR = (255, 255, 0)
            MANUAL_TEXT_COLOR = (255, 0, 0)
            OVERLAY_OPACITY = 0.5

            # ìë™ ê°ì§€ëœ í¬ì¸íŠ¸ í‘œì‹œ
            for idx, (x, y) in enumerate(self.auto_points, 1):
                text = str(idx)
                (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
                text_x = int(x - text_width / 2)
                text_y = int(y - 10)

                # ì™¸ê³½ì„ 
                for dx in [-1, 0, 1]:
                    for dy in [-1, 0, 1]:
                        if dx != 0 or dy != 0:
                            cv2.putText(img_with_points, text,
                                      (text_x + dx, text_y + dy),
                                      font, font_scale, AUTO_OUTLINE_COLOR, outline_thickness)

                # í…ìŠ¤íŠ¸
                cv2.putText(img_with_points, text,
                          (text_x, text_y),
                          font, font_scale, AUTO_TEXT_COLOR, font_thickness)

            # ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ í¬ì¸íŠ¸ í‘œì‹œ
            for idx, (x, y) in enumerate(self.manual_points, len(self.auto_points) + 1):
                pt1 = (int(x - square_size / 2), int(y - square_size / 2))
                pt2 = (int(x + square_size / 2), int(y + square_size / 2))
                
                cv2.rectangle(overlay, pt1, pt2, MANUAL_RECT_COLOR, -1)
                cv2.rectangle(overlay, pt1, pt2, MANUAL_BORDER_COLOR, 2)

                text = str(idx)
                (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
                text_x = int(x - text_width / 2)
                text_y = int(y - 10)

                # ì™¸ê³½ì„ 
                for dx in [-1, 0, 1]:
                    for dy in [-1, 0, 1]:
                        if dx != 0 or dy != 0:
                            cv2.putText(overlay, text,
                                      (text_x + dx, text_y + dy),
                                      font, font_scale, AUTO_OUTLINE_COLOR, outline_thickness)

                cv2.putText(img_with_points, text,
                          (text_x, text_y),
                          font, font_scale, MANUAL_TEXT_COLOR, font_thickness)

            cv2.addWeighted(overlay, OVERLAY_OPACITY, img_with_points, 1.0, 0, img_with_points)

            # ì œê±° ëª¨ë“œ í‘œì‹œ
            if self.remove_mode:
                mode_overlay = img_with_points.copy()
                h, w = mode_overlay.shape[:2]
                gradient = np.zeros((50, w, 3), dtype=np.uint8)
                for i in range(50):
                    alpha = 1.0 - (i / 50.0) * 0.3
                    gradient[i, :] = (0, 0, 255 * alpha)
                
                mode_overlay[:50, :] = cv2.addWeighted(mode_overlay[:50, :], 0.6, gradient, 0.8, 0)
                cv2.addWeighted(mode_overlay, 0.4, img_with_points, 0.6, 0, img_with_points)
                cv2.putText(img_with_points, "REMOVE MODE", (10, 35),
                          font, 1.2, (255, 255, 255), 2)

            # ì „ì²´ ì¹´ìš´íŠ¸ í‘œì‹œ
            total_count = self.auto_detected_count + len(self.manual_points)
            count_text = f"Total Count: {total_count}"
            (text_width, text_height), baseline = cv2.getTextSize(count_text, font, 1.0, 2)
            margin = 15
            cv2.rectangle(img_with_points,
                         (margin, img_with_points.shape[0] - margin - text_height - baseline),
                         (margin + text_width + margin, img_with_points.shape[0] - margin + baseline),
                         (0, 0, 0), -1)
            cv2.putText(img_with_points, count_text,
                      (margin + margin // 2, img_with_points.shape[0] - margin - baseline // 2),
                      font, 1.0, (255, 255, 255), 2)

            return img_with_points
        except Exception as e:
            print(f"Error in draw_points: {str(e)}")
            return self.base_image


# ì „ì—­ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°
gpu_monitor = GPUMemoryMonitor(threshold_gb=1.5)

# ì „ì—­ ê°ì²´ ìƒì„±
image_history = ImagePreprocessHistory()
counter = ColonyCounter()


def preprocess_image(
    input_image,
    to_grayscale=False,
    binary=False,
    binary_threshold=128,
    edge_detection=False,
    sharpen=False,
    sharpen_amount=1.0
):
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        if not isinstance(input_image, Image.Image):
            return input_image
        
        # ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•œ ì´ë¯¸ì§€ í¬ê¸° ì œí•œ
        image = MemoryManager.limit_image_size(input_image.copy())

        if to_grayscale:
            image = image.convert('L').convert('RGB')
        
        if binary:
            image_np = np.array(image.convert('L'))
            _, binary_img = cv2.threshold(image_np, binary_threshold, 255, cv2.THRESH_BINARY)
            image = Image.fromarray(binary_img).convert('RGB')
        
        if edge_detection:
            image_np = np.array(image.convert('L'))
            edges = cv2.Canny(image_np, 100, 200)
            image = Image.fromarray(edges).convert('RGB')
        
        if sharpen:
            image = image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))
            if sharpen_amount != 1.0:
                enhancer = ImageEnhance.Sharpness(image)
                image = enhancer.enhance(sharpen_amount)

        return image
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return input_image


def fast_process(colony_annotations, dish_annotation, image, mask_random_color, withContours, original_size=None):
    """ë§ˆìŠ¤í¬ ì£¼ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        image_np = np.array(image).copy()

        # ì½œë¡œë‹ˆ ë§ˆìŠ¤í¬ ì²˜ë¦¬
        for ann in colony_annotations:
            ann_cpu = ann.cpu().numpy()
            if ann_cpu.ndim == 3 and ann_cpu.shape[0] == 1:
                ann_cpu = ann_cpu[0]
            
            mask = ann_cpu > 0
            if mask.ndim == 2:
                if mask_random_color:
                    color = np.random.randint(100, 200, (3,)).tolist()
                    image_np[mask] = color
                else:
                    image_np[mask] = (0, 255, 0)

                if withContours:
                    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    cv2.drawContours(image_np, contours, -1, (255, 0, 0), 2)

        # ë°°ì–‘ ì ‘ì‹œ ë§ˆìŠ¤í¬ ì²˜ë¦¬
        if dish_annotation is not None and withContours:
            dish_mask = dish_annotation.cpu().numpy()
            if dish_mask.ndim == 3 and dish_mask.shape[0] == 1:
                dish_mask = dish_mask[0]
            if dish_mask.ndim == 2:
                dish_mask = dish_mask > 0
                contours, _ = cv2.findContours(dish_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(image_np, contours, -1, (0, 0, 255), 3)

        processed_image = Image.fromarray(image_np)
        
        if original_size is not None:
            processed_image = processed_image.resize(original_size, Image.LANCZOS)
            
        return processed_image
    except Exception as e:
        print(f"Error in fast_process: {str(e)}")
        return image


def segment_and_count_colonies(
    input_image,
    input_size=1024,
    iou_threshold=0.7,
    conf_threshold=0.25,
    better_quality=False,
    withContours=True,
    mask_random_color=True,
    circularity_threshold=0.8,
    # ë©´ì  í•„í„°ë§ (v6 ê¸°ëŠ¥)
    enable_area_filter=False,
    min_area_percentile=1,
    max_area_percentile=99,
    # ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ (grok_v6 ê¸°ëŠ¥)
    use_dish_filtering=True,
    dish_overlap_threshold=0.5,
    progress=gr.Progress()
):
    """ë©”ëª¨ë¦¬ ì•ˆì „ ì´ë¯¸ì§€ ë¶„ì„"""
    global counter
    
    try:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        MemoryManager.clear_gpu_memory()
        
        if input_image is None:
            return None, "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        # ì´ë¯¸ì§€ í¬ê¸° ì œí•œ
        input_image = MemoryManager.limit_image_size(input_image)
        
        counter.reset()
        
        progress(0.1, desc="ì´ë¯¸ì§€ ì¤€ë¹„ ì¤‘...")
        counter.set_original_image(input_image)
        
        # ì›ë³¸ í¬ê¸° ì €ì¥
        original_size = input_image.size
        
        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•
        w, h = input_image.size
        scale = input_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        input_resized = input_image.resize((new_w, new_h))
        counter.scale_factor = scale

        progress(0.3, desc="AI ë¶„ì„ ì¤‘...")
        
        # ëª¨ë¸ ì˜ˆì¸¡ ì „ ë©”ëª¨ë¦¬ ì²´í¬
        if torch.cuda.is_available():
            free_memory = torch.cuda.mem_get_info()[0] / 1024**3  # GB
            if free_memory < 1.0:
                print(f"Warning: Low GPU memory ({free_memory:.2f}GB)")
                MemoryManager.clear_gpu_memory()
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        with torch.no_grad():
            results = model.predict(
                source=np.array(input_resized),
                device=device,
                conf=conf_threshold,
                iou=iou_threshold,
                imgsz=input_size,
                retina_masks=True
            )

        annotations = getattr(results[0].masks, 'data', None)
        if annotations is None or len(annotations) == 0:
            result_image = Image.fromarray(np.array(input_resized)).resize(original_size)
            counter.current_image = np.array(result_image)
            # ëŒ€ìš©ëŸ‰ ê°ì²´ ëª…ì‹œì  í•´ì œ
            del results
            MemoryManager.clear_gpu_memory()
            return np.array(result_image), "ì½œë¡œë‹ˆê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        progress(0.6, desc="ë§ˆìŠ¤í¬ ì²˜ë¦¬ ì¤‘...")
        
        # í–¥ìƒëœ ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì²˜ë¦¬
        dish_mask = None
        dish_annotation = None
        colony_annotations = list(annotations)
        
        if use_dish_filtering and len(annotations) > 1:
            # í–¥ìƒëœ ë°°ì–‘ì ‘ì‹œ ì‹ë³„ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
            dish_idx, colony_indices = identify_petri_dish(annotations, input_resized.size[::-1])
            
            if dish_idx is not None:
                dish_annotation = annotations[dish_idx]
                dish_mask_np = dish_annotation.cpu().numpy()
                if dish_mask_np.ndim == 3 and dish_mask_np.shape[0] == 1:
                    dish_mask_np = dish_mask_np[0]
                dish_mask = dish_mask_np > 0
                
                # ë°°ì–‘ ì ‘ì‹œë¥¼ ì œì™¸í•œ ì½œë¡œë‹ˆ ë§ˆìŠ¤í¬
                colony_annotations = [annotations[idx] for idx in colony_indices]

        # ì½œë¡œë‹ˆ í•„í„°ë§
        valid_colony_annotations = []
        counter.auto_points = []
        
        # ë©´ì  í•„í„°ë§ ì¤€ë¹„
        if enable_area_filter and colony_annotations:
            colony_areas = [np.sum(ann.cpu().numpy() > 0) for ann in colony_annotations]
            if colony_areas:
                min_area = np.percentile(colony_areas, min_area_percentile)
                max_area = np.percentile(colony_areas, max_area_percentile)
            else:
                min_area, max_area = 0, float('inf')
        else:
            min_area, max_area = 0, float('inf')

        for ann in colony_annotations:
            ann_cpu = ann.cpu().numpy()
            if ann_cpu.ndim == 3 and ann_cpu.shape[0] == 1:
                ann_cpu = ann_cpu[0]
            mask = ann_cpu > 0
            area = np.sum(mask)

            # ë©´ì  í•„í„°ë§
            if enable_area_filter and (area < min_area or area > max_area):
                continue

            # ë°°ì–‘ ì ‘ì‹œ ë‚´ë¶€ í™•ì¸
            if use_dish_filtering and dish_mask is not None:
                overlap_ratio = np.sum(mask * dish_mask) / np.sum(mask)
                if overlap_ratio < dish_overlap_threshold:
                    continue

            # í–¥ìƒëœ ì›í˜•ë„ ê³„ì‚°
            circularity = calculate_circularity_robust(mask)
            if circularity >= circularity_threshold:
                valid_colony_annotations.append(ann)
                # ì¤‘ì‹¬ì  ê³„ì‚°
                y_indices, x_indices = np.where(mask)
                if len(x_indices) > 0 and len(y_indices) > 0:
                    center_x = int(np.mean(x_indices))
                    center_y = int(np.mean(y_indices))
                    # ì›ë³¸ í¬ê¸° ì¢Œí‘œë¡œ ë³€í™˜
                    original_x = int(center_x / scale)
                    original_y = int(center_y / scale)
                    counter.auto_points.append((original_x, original_y))

        progress(0.8, desc="ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        processed_image = fast_process(
            colony_annotations=valid_colony_annotations,
            dish_annotation=dish_annotation,
            image=input_resized,
            mask_random_color=mask_random_color,
            withContours=withContours,
            original_size=original_size
        )

        # counter ì—…ë°ì´íŠ¸
        counter.colony_masks = valid_colony_annotations
        counter.dish_mask = dish_mask
        counter.base_image = np.array(processed_image)
        counter.auto_detected_count = len(valid_colony_annotations)
        counter.current_image = counter.draw_points()

        # ëŒ€ìš©ëŸ‰ ê°ì²´ ëª…ì‹œì  í•´ì œ
        del results
        MemoryManager.clear_gpu_memory()

        progress(1.0, desc="ì™„ë£Œ!")
        return counter.current_image, counter.get_count_text()

    except torch.cuda.OutOfMemoryError:
        MemoryManager.clear_gpu_memory()
        return None, "GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
    except Exception as e:
        error_msg = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        MemoryManager.clear_gpu_memory()
        if input_image is not None:
            return np.array(input_image), error_msg
        return None, error_msg


def save_results(original_image, processed_image):
    """ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join(DEFAULT_OUTPUT_DIR, f"result_{timestamp}")
        os.makedirs(save_dir, exist_ok=True)

        # íŒŒì¼ ì €ì¥
        original_path = os.path.join(save_dir, "original.png")
        result_path = os.path.join(save_dir, "result.png")
        
        if isinstance(original_image, Image.Image):
            original_image.save(original_path)
        
        if isinstance(processed_image, np.ndarray):
            Image.fromarray(processed_image).save(result_path)
        
        # ê²°ê³¼ ì •ë³´ ì €ì¥
        result_info = {
            'datetime': datetime.now().isoformat(),
            'auto_count': counter.auto_detected_count,
            'manual_count': len(counter.manual_points),
            'total_count': counter.auto_detected_count + len(counter.manual_points),
            'analysis_method': counter.last_method,
            'device': str(device)
        }
        
        # JSON ì €ì¥
        json_path = os.path.join(save_dir, "result_info.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result_info, f, indent=4, ensure_ascii=False)
        
        # í…ìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        txt_path = os.path.join(save_dir, "count_results.txt")
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(counter.get_count_text())
            f.write(f"\n\nì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            f.write(f"\në¶„ì„ ë°©ë²•: {counter.last_method}")

        return f"ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n{save_dir}"
    except Exception as e:
        return f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def process_single_file(file, **params):
    """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        filename = os.path.basename(file.name)
        image = Image.open(file.name).convert("RGB")
        
        # ë¶„ì„ ìˆ˜í–‰
        result_image, count_text = segment_and_count_colonies(
            image,
            **params
        )
        
        # ê²°ê³¼ ì´ë¯¸ì§€ê°€ numpy ë°°ì—´ì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        gallery_item = None
        if result_image is not None:
            try:
                # numpy ë°°ì—´ì„ PIL Imageë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ìœ ì§€
                # Gradioì—ì„œ numpy ë°°ì—´ì„ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
                gallery_item = (result_image, f"{filename}\n{count_text}")
            except Exception as e:
                print(f"ê°¤ëŸ¬ë¦¬ ì•„ì´í…œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                gallery_item = None
        
        # ì¹´ìš´íŠ¸ ê°’ë“¤ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        total_count = int(counter.auto_detected_count + len(counter.manual_points))
        auto_count = int(counter.auto_detected_count)
        manual_count = int(len(counter.manual_points))
        
        return {
            'filename': filename,
            'total_count': total_count,
            'auto_count': auto_count,
            'manual_count': manual_count,
            'status': 'success',
            'gallery_item': gallery_item
        }
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ìƒì„¸ ì˜¤ë¥˜ ({file.name}): {error_detail}")
        
        return {
            'filename': os.path.basename(file.name) if hasattr(file, 'name') else 'unknown',
            'total_count': 0,
            'auto_count': 0,
            'manual_count': 0,
            'status': 'error',
            'error': str(e),
            'gallery_item': None
        }


def batch_process_optimized(files, chunk_size=5, **params):
    """ì²­í¬ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        if not files:
            return "ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", []
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        batch_dir = os.path.join(DEFAULT_OUTPUT_DIR, f"batch_{timestamp}")
        os.makedirs(batch_dir, exist_ok=True)
        
        total_files = len(files)
        chunks = [files[i:i+chunk_size] for i in range(0, total_files, chunk_size)]
        
        results = []
        gallery_items = []
        
        progress = params.get('progress', lambda x, desc="": None)
        
        for chunk_idx, chunk in enumerate(chunks):
            # ì²­í¬ ì²˜ë¦¬ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
            gpu_monitor.cleanup()
            
            for file_idx, file in enumerate(chunk):
                global_idx = chunk_idx * chunk_size + file_idx
                progress((global_idx + 1) / total_files, desc=f"ì²˜ë¦¬ ì¤‘ ({global_idx + 1}/{total_files})")
                
                try:
                    # íŒŒì¼ ì²˜ë¦¬
                    result = process_single_file(file, **params)
                    
                    # JSON ì•ˆì „ ë²„ì „ì˜ ê²°ê³¼ ìƒì„± (gallery_item ì œì™¸)
                    json_safe_result = {
                        'filename': result['filename'],
                        'total_count': int(result['total_count']),  # numpy intë¥¼ Python intë¡œ ë³€í™˜
                        'auto_count': int(result['auto_count']),
                        'manual_count': int(result['manual_count']),
                        'status': result['status']
                    }
                    
                    if result['status'] == 'error' and 'error' in result:
                        json_safe_result['error'] = str(result['error'])
                    
                    results.append(json_safe_result)
                    
                    # ê°¤ëŸ¬ë¦¬ ì•„ì´í…œì€ ë³„ë„ë¡œ ê´€ë¦¬
                    if result.get('gallery_item'):
                        gallery_items.append(result['gallery_item'])
                    
                    # ì„±ê³µí•œ ê²½ìš° íŒŒì¼ ì €ì¥
                    if result['status'] == 'success':
                        filename = result['filename']
                        img_dir = os.path.join(batch_dir, os.path.splitext(filename)[0])
                        os.makedirs(img_dir, exist_ok=True)
                        
                        # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                        Image.open(file.name).save(os.path.join(img_dir, "original.png"))
                        
                        # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ (numpy ë°°ì—´ ì²˜ë¦¬)
                        if result.get('gallery_item') and result['gallery_item'][0] is not None:
                            result_image = result['gallery_item'][0]
                            if isinstance(result_image, np.ndarray):
                                Image.fromarray(result_image).save(os.path.join(img_dir, "result.png"))
                            elif isinstance(result_image, Image.Image):
                                result_image.save(os.path.join(img_dir, "result.png"))
                        
                        # ì¹´ìš´íŠ¸ ê²°ê³¼ ì €ì¥
                        count_info = f"ì´ ì½œë¡œë‹ˆ ìˆ˜: {result['total_count']}\nìë™ ê°ì§€: {result['auto_count']}\nìˆ˜ë™ ì¶”ê°€: {result['manual_count']}"
                        with open(os.path.join(img_dir, "count.txt"), 'w', encoding='utf-8') as f:
                            f.write(count_info)
                    
                except Exception as e:
                    error_result = {
                        'filename': os.path.basename(file.name),
                        'total_count': 0,
                        'auto_count': 0,
                        'manual_count': 0,
                        'status': 'error',
                        'error': str(e)
                    }
                    results.append(error_result)
                    print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({file.name}): {str(e)}")
                
                # íŒŒì¼ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²´í¬
                gpu_monitor.check_memory()
            
            # ì²­í¬ ì™„ë£Œ í›„ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            gpu_monitor.cleanup()
            time.sleep(0.1)  # GPU ë™ê¸°í™” ëŒ€ê¸°

        # CSVë¡œ ê²°ê³¼ ì €ì¥ (JSON ì•ˆì „ ë²„ì „ ì‚¬ìš©)
        try:
            df = pd.DataFrame(results)
            df.to_csv(os.path.join(batch_dir, "results.csv"), index=False, encoding='utf-8')
        except Exception as e:
            print(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # JSONìœ¼ë¡œ ìš”ì•½ ì €ì¥ (JSON ì•ˆì „ ë°ì´í„°ë§Œ í¬í•¨)
        summary = {
            'timestamp': timestamp,
            'total_files': int(total_files),
            'successful': int(len([r for r in results if r['status'] == 'success'])),
            'failed': int(len([r for r in results if r['status'] == 'error'])),
            'results': results  # ì´ë¯¸ JSON ì•ˆì „ ë²„ì „
        }
        
        try:
            with open(os.path.join(batch_dir, "summary.json"), 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=4, ensure_ascii=False)
        except Exception as e:
            print(f"JSON ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # JSON ì €ì¥ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
            with open(os.path.join(batch_dir, "summary.txt"), 'w', encoding='utf-8') as f:
                f.write(f"ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½\n")
                f.write(f"ì²˜ë¦¬ ì‹œê°„: {timestamp}\n")
                f.write(f"ì´ íŒŒì¼ ìˆ˜: {total_files}\n")
                f.write(f"ì„±ê³µ: {summary['successful']}\n")
                f.write(f"ì‹¤íŒ¨: {summary['failed']}\n")

        summary_text = f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\n"
        summary_text += f"ì´ {total_files}ê°œ íŒŒì¼ ì¤‘ {summary['successful']}ê°œ ì„±ê³µ\n"
        summary_text += f"ì‹¤íŒ¨: {summary['failed']}ê°œ\n"
        summary_text += f"ê²°ê³¼ ì €ì¥: {batch_dir}"
        
        return summary_text, gallery_items

    except Exception as e:
        import traceback
        error_msg = f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, []


def batch_process(
    files,
    input_size=1024,
    iou_threshold=0.7,
    conf_threshold=0.25,
    better_quality=False,
    withContours=True,
    circularity_threshold=0.8,
    enable_area_filter=False,
    min_area_percentile=1,
    max_area_percentile=99,
    use_dish_filtering=True,
    dish_overlap_threshold=0.5,
    progress=gr.Progress()
):
    """ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ ë˜í¼"""
    return batch_process_optimized(
        files=files,
        chunk_size=3,  # ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ ì²­í¬ í¬ê¸° ê°ì†Œ
        input_size=input_size,
        iou_threshold=iou_threshold,
        conf_threshold=conf_threshold,
        better_quality=better_quality,
        withContours=withContours,
        mask_random_color=True,
        circularity_threshold=circularity_threshold,
        enable_area_filter=enable_area_filter,
        min_area_percentile=min_area_percentile,
        max_area_percentile=max_area_percentile,
        use_dish_filtering=use_dish_filtering,
        dish_overlap_threshold=dish_overlap_threshold,
        progress=progress
    )


# CSS ìŠ¤íƒ€ì¼ (v99 ë””ìì¸)
css = """
/* ê¸°ë³¸ ì„¤ì • */
:root {
    --primary-color: #3b82f6;
    --primary-dark: #1d4ed8;
    --primary-light: #93c5fd;
    --secondary-color: #10b981;
    --accent-color: #f59e0b;
    --danger-color: #ef4444;
    --text-color: #1f2937;
    --text-light: #6b7280;
    --bg-color: #f9fafb;
    --card-bg: #ffffff;
    --border-color: #e5e7eb;
    --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
    --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
    --radius-md: 0.375rem;
    --radius-lg: 0.5rem;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    color: var(--text-color);
    background-color: var(--bg-color);
}

/* í—¤ë” ìŠ¤íƒ€ì¼ */
.header {
    background: linear-gradient(135deg, var(--primary-color), var(--primary-dark));
    color: white;
    padding: 2rem;
    border-radius: var(--radius-lg);
    margin-bottom: 2rem;
    box-shadow: var(--shadow-md);
    text-align: center;
}

.header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 0.5rem;
}

.header h3 {
    font-size: 1.25rem;
    font-weight: 500;
    opacity: 0.9;
}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
button.primary {
    background: linear-gradient(to right, var(--primary-color), var(--primary-dark));
    color: white;
    font-weight: 600;
    border-radius: var(--radius-md);
    padding: 0.75rem 1.5rem;
    transition: all 0.3s ease;
    box-shadow: var(--shadow-sm);
    border: none;
}

button.primary:hover {
    box-shadow: var(--shadow-md);
    transform: translateY(-2px);
}

button.secondary {
    background-color: var(--card-bg);
    color: var(--primary-color);
    font-weight: 600;
    border: 1px solid var(--primary-color);
    border-radius: var(--radius-md);
    padding: 0.75rem 1.5rem;
    transition: all 0.3s ease;
}

button.secondary:hover {
    background-color: var(--primary-light);
    color: var(--primary-dark);
}

/* ì„¹ì…˜ íƒ€ì´í‹€ */
.section-title {
    font-size: 1.25rem;
    font-weight: 600;
    color: var(--primary-dark);
    margin-bottom: 1rem;
    display: flex;
    align-items: center;
    gap: 0.5rem;
}

/* ì¹´ë“œ ìŠ¤íƒ€ì¼ */
.card {
    background: var(--card-bg);
    border-radius: var(--radius-lg);
    padding: 1.5rem;
    box-shadow: var(--shadow-sm);
    border: 1px solid var(--border-color);
    transition: all 0.3s ease;
}

.card:hover {
    box-shadow: var(--shadow-md);
    transform: translateY(-2px);
}

/* ì´ë¯¸ì§€ ì»¨í…Œì´ë„ˆ */
.input-image, .output-image {
    border: 2px solid var(--border-color);
    border-radius: var(--radius-lg);
    background-color: var(--card-bg);
    transition: all 0.3s ease;
    box-shadow: var(--shadow-sm);
    overflow: hidden;
}

.input-image:hover, .output-image:hover {
    box-shadow: var(--shadow-md);
    border-color: var(--primary-light);
}

/* ê²°ê³¼ í…ìŠ¤íŠ¸ */
.result-text {
    font-family: 'Roboto Mono', monospace;
    background-color: var(--card-bg);
    border-radius: var(--radius-md);
    border-left: 4px solid var(--secondary-color);
    padding: 1rem;
    box-shadow: var(--shadow-sm);
}

/* ì•„ì½”ë””ì–¸ */
.accordion-header {
    background-color: var(--card-bg);
    color: var(--primary-dark);
    font-weight: 600;
    padding: 1rem;
    border-radius: var(--radius-md);
    border: 1px solid var(--border-color);
    transition: all 0.2s ease;
}

.accordion-header:hover {
    border-color: var(--primary-color);
    box-shadow: var(--shadow-sm);
}

/* ëª¨ë“œ ì¸ë””ì¼€ì´í„° */
.mode-indicator {
    text-align: center;
    padding: 0.5rem;
    border-radius: var(--radius-md);
    font-weight: 600;
}

/* í‘¸í„° */
.footer {
    text-align: center;
    padding: 2rem;
    margin-top: 3rem;
    color: var(--text-light);
    border-top: 1px solid var(--border-color);
}

/* ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ */
.progress-bar {
    background: linear-gradient(90deg, var(--primary-light), var(--primary-color), var(--primary-dark));
    background-size: 200% 100%;
    animation: gradient-animation 2s ease infinite;
}

@keyframes gradient-animation {
    0% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
    100% { background-position: 0% 50%; }
}
"""

# Gradio UI êµ¬ì„±
with gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:
    gr.Markdown(
        """
        <div class="header">
            <h1>ğŸ”¬ Colony Counter</h1>
            <h3>AI ê¸°ë°˜ ì½œë¡œë‹ˆ ê²€ì¶œ ë° ì¹´ìš´íŒ…</h3>
        </div>
        """
    )

    with gr.Tabs():
        # ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ íƒ­
        with gr.Tab("ğŸ” ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„"):
            with gr.Row():
                # ì…ë ¥ ì»¬ëŸ¼
                with gr.Column(scale=1):
                    gr.Markdown("<div class='section-title'>ğŸ“¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ</div>")
                    input_image = gr.Image(
                        type="pil",
                        elem_classes=["input-image"],
                        show_label=False
                    )
                    
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    with gr.Accordion("ğŸ› ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬", open=False, elem_classes="accordion-header"):
                        to_grayscale = gr.Checkbox(label="í‘ë°± ë³€í™˜", value=False)
                        binary = gr.Checkbox(label="ë°”ì´ë„ˆë¦¬ ë³€í™˜", value=False)
                        binary_threshold = gr.Slider(0, 255, 128, step=1, label="ë°”ì´ë„ˆë¦¬ ì„ê³„ê°’")
                        edge_detection = gr.Checkbox(label="ì—ì§€ ê²€ì¶œ", value=False)
                        sharpen = gr.Checkbox(label="ìƒ¤í”ˆ", value=False)
                        sharpen_amount = gr.Slider(0.5, 5.0, 1.0, step=0.1, label="ìƒ¤í”ˆ ê°•ë„")
                        
                        with gr.Row():
                            preprocess_button = gr.Button("ğŸ”„ ì „ì²˜ë¦¬ ì ìš©", variant="secondary")
                            reset_button = gr.Button("â†º ì›ë³¸ ë³µì›", variant="secondary")
                            undo_button = gr.Button("â†¶ ì‹¤í–‰ ì·¨ì†Œ", variant="secondary")
                    
                    # ë¶„ì„ ì„¤ì •
                    with gr.Accordion("âš™ï¸ ë¶„ì„ ì„¤ì •", open=False, elem_classes="accordion-header"):
                        with gr.Tab("ê¸°ë³¸"):
                            input_size_slider = gr.Slider(512, 2048, 1024, step=64, label="ì…ë ¥ í¬ê¸°")
                            withContours_checkbox = gr.Checkbox(label="ìœ¤ê³½ì„  í‘œì‹œ", value=True)
                            better_quality_checkbox = gr.Checkbox(label="í–¥ìƒëœ í’ˆì§ˆ", value=False)
                            mask_random_color = gr.Checkbox(label="ëœë¤ ìƒ‰ìƒ", value=True)
                        
                        with gr.Tab("AI íƒì§€"):
                            iou_threshold_slider = gr.Slider(0.1, 0.9, 0.7, step=0.1, label="IOU ì„ê³„ê°’")
                            conf_threshold_slider = gr.Slider(0.1, 0.9, 0.25, step=0.05, label="ì‹ ë¢°ë„ ì„ê³„ê°’")
                            circularity_threshold_slider = gr.Slider(0.0, 1.0, 0.8, step=0.01, label="ì›í˜•ë„ ì„ê³„ê°’")
                        
                        with gr.Tab("ë©´ì  í•„í„°"):
                            enable_area_filter = gr.Checkbox(label="ë©´ì  í•„í„°ë§ ì‚¬ìš©", value=False)
                            min_area_percentile_slider = gr.Slider(0, 10, 1, step=1, label="ìµœì†Œ ë©´ì  ë°±ë¶„ìœ„")
                            max_area_percentile_slider = gr.Slider(90, 100, 99, step=1, label="ìµœëŒ€ ë©´ì  ë°±ë¶„ìœ„")
                        
                        with gr.Tab("ë°°ì–‘ ì ‘ì‹œ í•„í„°"):
                            use_dish_filtering = gr.Checkbox(label="ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§ ì‚¬ìš©", value=True)
                            dish_overlap_threshold = gr.Slider(0.1, 1.0, 0.5, step=0.1, label="ì ‘ì‹œ ê²¹ì¹¨ ì„ê³„ê°’")
                    
                    segment_button = gr.Button("ğŸ” ì´ë¯¸ì§€ ë¶„ì„", variant="primary", scale=2)

                # ì¶œë ¥ ì»¬ëŸ¼
                with gr.Column(scale=1):
                    gr.Markdown("<div class='section-title'>ğŸ“Š ë¶„ì„ ê²°ê³¼</div>")
                    output_image = gr.Image(
                        type="numpy",
                        interactive=True,
                        elem_classes=["output-image"],
                        show_label=False
                    )
                    colony_count_text = gr.Textbox(
                        label="ì¹´ìš´íŠ¸ ê²°ê³¼",
                        lines=3,
                        elem_classes="result-text"
                    )
                    
                    # ìˆ˜ë™ í¸ì§‘
                    with gr.Row():
                        with gr.Column(scale=1):
                            remove_mode_button = gr.Button("ğŸ”„ í¸ì§‘ ëª¨ë“œ ì „í™˜", variant="secondary")
                            remove_mode_indicator = gr.Markdown(
                                value="<span style='color: green; font-weight: bold;'>ğŸŸ¢ ADD MODE</span>",
                                elem_classes="mode-indicator"
                            )
                        with gr.Column(scale=1):
                            remove_point_button = gr.Button("â†©ï¸ ë§ˆì§€ë§‰ í¬ì¸íŠ¸ ì·¨ì†Œ", variant="secondary")
                            undo_removal_button = gr.Button("â™»ï¸ ì‚­ì œ ë³µì›", variant="secondary")
                    
                    # ì €ì¥
                    save_button = gr.Button("ğŸ’¾ ê²°ê³¼ ì €ì¥", variant="primary")
                    save_output = gr.Textbox(label="ì €ì¥ ê²°ê³¼", lines=2, elem_classes="result-text")

        # ë°°ì¹˜ ì²˜ë¦¬ íƒ­
        with gr.Tab("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("<div class='section-title'>ğŸ“ íŒŒì¼ ì„ íƒ</div>")
                    batch_files = gr.File(
                        label="ì´ë¯¸ì§€ íŒŒì¼ ì„ íƒ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
                        file_count="multiple",
                        file_types=["image"],
                        elem_classes=["card"]
                    )
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
                    with gr.Accordion("âš™ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •", open=True, elem_classes="accordion-header"):
                        with gr.Row():
                            batch_input_size = gr.Slider(512, 2048, 1024, step=64, label="ì…ë ¥ í¬ê¸°")
                            batch_iou_threshold = gr.Slider(0.1, 0.9, 0.7, step=0.1, label="IOU ì„ê³„ê°’")
                        
                        with gr.Row():
                            batch_conf_threshold = gr.Slider(0.1, 0.9, 0.25, step=0.05, label="ì‹ ë¢°ë„ ì„ê³„ê°’")
                            batch_circularity = gr.Slider(0.0, 1.0, 0.8, step=0.01, label="ì›í˜•ë„ ì„ê³„ê°’")
                        
                        with gr.Row():
                            batch_enable_area_filter = gr.Checkbox(label="ë©´ì  í•„í„°ë§", value=False)
                            batch_use_dish_filtering = gr.Checkbox(label="ë°°ì–‘ ì ‘ì‹œ í•„í„°ë§", value=True)
                        
                        with gr.Row():
                            batch_min_area = gr.Slider(0, 10, 1, step=1, label="ìµœì†Œ ë©´ì  ë°±ë¶„ìœ„")
                            batch_max_area = gr.Slider(90, 100, 99, step=1, label="ìµœëŒ€ ë©´ì  ë°±ë¶„ìœ„")
                        
                        batch_dish_overlap = gr.Slider(0.1, 1.0, 0.5, step=0.1, label="ì ‘ì‹œ ê²¹ì¹¨ ì„ê³„ê°’")
                    
                    batch_process_button = gr.Button("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘", variant="primary")

                with gr.Column(scale=1):
                    gr.Markdown("<div class='section-title'>ğŸ“Š ì²˜ë¦¬ ê²°ê³¼</div>")
                    batch_summary = gr.Textbox(
                        label="ì²˜ë¦¬ ìš”ì•½",
                        lines=8,
                        elem_classes="result-text"
                    )
                    
                    # ê²°ê³¼ ê°¤ëŸ¬ë¦¬
                    batch_gallery = gr.Gallery(
                        label="ì²˜ë¦¬ëœ ì´ë¯¸ì§€",
                        show_label=False,
                        columns=3,
                        height=400,
                        elem_classes=["card"]
                    )
                    
                    open_output_button = gr.Button("ğŸ“‚ ê²°ê³¼ í´ë” ì—´ê¸°", variant="secondary")

    # í‘¸í„°
    gr.Markdown(
        """
        <div class="footer">
            <div>ğŸ”¬ Colony Counter</div>
            <div>Produced By BDK &copy; 2025</div>
        </div>
        """
    )

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
    
    # ì´ë¯¸ì§€ ì—…ë¡œë“œ
    input_image.upload(
        lambda img: (image_history.set_original(img), "ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."),
        inputs=[input_image],
        outputs=[input_image, colony_count_text]
    )
    
    # ì „ì²˜ë¦¬
    preprocess_button.click(
        lambda img, gs, b, bt, ed, sh, sa: image_history.add_state(
            preprocess_image(img, gs, b, bt, ed, sh, sa)
        ),
        inputs=[input_image, to_grayscale, binary, binary_threshold, edge_detection, sharpen, sharpen_amount],
        outputs=[input_image]
    )
    
    reset_button.click(
        lambda: image_history.reset(),
        outputs=[input_image]
    )
    
    undo_button.click(
        lambda: image_history.undo(),
        outputs=[input_image]
    )
    
    # ë¶„ì„
    segment_button.click(
        segment_and_count_colonies,
        inputs=[
            input_image,
            input_size_slider,
            iou_threshold_slider,
            conf_threshold_slider,
            better_quality_checkbox,
            withContours_checkbox,
            mask_random_color,
            circularity_threshold_slider,
            enable_area_filter,
            min_area_percentile_slider,
            max_area_percentile_slider,
            use_dish_filtering,
            dish_overlap_threshold
        ],
        outputs=[output_image, colony_count_text]
    )
    
    # ìˆ˜ë™ í¸ì§‘
    output_image.select(
        counter.add_or_remove_point,
        inputs=[output_image],
        outputs=[output_image, colony_count_text]
    )
    
    remove_mode_button.click(
        counter.toggle_remove_mode,
        outputs=[output_image, remove_mode_indicator]
    )
    
    remove_point_button.click(
        counter.remove_last_point,
        inputs=[output_image],
        outputs=[output_image, colony_count_text]
    )
    
    undo_removal_button.click(
        counter.undo_last_removal,
        inputs=[output_image],
        outputs=[output_image, colony_count_text]
    )
    
    # ì €ì¥
    save_button.click(
        save_results,
        inputs=[input_image, output_image],
        outputs=[save_output]
    )
    
    # ë°°ì¹˜ ì²˜ë¦¬
    batch_process_button.click(
        batch_process,
        inputs=[
            batch_files,
            batch_input_size,
            batch_iou_threshold,
            batch_conf_threshold,
            gr.Checkbox(value=False, visible=False),  # better_quality
            gr.Checkbox(value=True, visible=False),   # withContours
            batch_circularity,
            batch_enable_area_filter,
            batch_min_area,
            batch_max_area,
            batch_use_dish_filtering,
            batch_dish_overlap
        ],
        outputs=[batch_summary, batch_gallery]
    )
    
    # ê²°ê³¼ í´ë” ì—´ê¸°
    def open_folder():
        try:
            if platform.system() == "Windows":
                os.startfile(DEFAULT_OUTPUT_DIR)
            elif platform.system() == "Darwin":
                subprocess.run(["open", DEFAULT_OUTPUT_DIR])
            else:
                subprocess.run(["xdg-open", DEFAULT_OUTPUT_DIR])
            return f"í´ë” ì—´ê¸°: {DEFAULT_OUTPUT_DIR}"
        except Exception as e:
            return f"í´ë” ì—´ê¸° ì‹¤íŒ¨: {str(e)}"
    
    open_output_button.click(
        open_folder,
        outputs=[batch_summary]
    )

if __name__ == "__main__":
    demo.launch(share=False)