from ultralytics import YOLO
import gradio as gr
import torch
from PIL import Image
import numpy as np
import cv2
import os
from datetime import datetime
import pandas as pd
from pathlib import Path
import json
import sys
import warnings
import glob
import platform
import subprocess

# FutureWarning ë¬´ì‹œ (ì˜µì…˜, ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
warnings.filterwarnings("ignore", category=FutureWarning)

# ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
DEFAULT_OUTPUT_DIR = os.path.join(str(Path.home()), 'colony_counter_results')
os.makedirs(DEFAULT_OUTPUT_DIR, exist_ok=True)

# FastSAM ëª¨ë¸ ë¡œë“œ
try:
    model = YOLO('./weights/FastSAM-x.pt')
except Exception as e:
    print(f"Error loading YOLO model: {str(e)}")
    sys.exit(1)

# ì¥ì¹˜ ì„¤ì •
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else "cpu"
)

def fast_process(colony_annotations, dish_annotation, image, mask_random_color, withContours, original_size=None):
    """
    ë§ˆìŠ¤í¬ ì£¼ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³ , í˜íŠ¸ë¦¬ ì ‘ì‹œëŠ” ì™¸ê³½ì„ ë§Œ ê·¸ë¦¬ë©° ì½œë¡œë‹ˆëŠ” ì±„ìš°ê³  ì™¸ê³½ì„ ì„ ê·¸ë¦½ë‹ˆë‹¤.
    
    Args:
        colony_annotations: ì½œë¡œë‹ˆ ë§ˆìŠ¤í¬ ì£¼ì„
        dish_annotation: í˜íŠ¸ë¦¬ ì ‘ì‹œ ë§ˆìŠ¤í¬ ì£¼ì„
        image: ì²˜ë¦¬í•  ì´ë¯¸ì§€
        mask_random_color: ë§ˆìŠ¤í¬ì— ëœë¤ ìƒ‰ìƒ ì ìš© ì—¬ë¶€
        withContours: ì™¸ê³½ì„  í‘œì‹œ ì—¬ë¶€
        original_size: ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° (width, height). Noneì´ë©´ ë¦¬ì‚¬ì´ì¦ˆí•˜ì§€ ì•ŠìŒ
    """
    try:
        image_np = np.array(image).copy()

        # ì½œë¡œë‹ˆ ë§ˆìŠ¤í¬ ì²˜ë¦¬
        for ann in colony_annotations:
            ann_cpu = ann.cpu().numpy()
            if ann_cpu.ndim == 3 and ann_cpu.shape[0] == 1:
                ann_cpu = ann_cpu[0]  # (1, H, W) -> (H, W)
            
            mask = ann_cpu > 0
            if mask.ndim == 2:
                if mask_random_color:
                    color = np.random.randint(0, 255, (3,)).tolist()
                    image_np[mask] = color
                else:
                    image_np[mask] = (0, 255, 0)  # ê¸°ë³¸ ì´ˆë¡ìƒ‰

            if withContours:
                contours = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                contours = contours[0] if len(contours) == 2 else contours[1]
                cv2.drawContours(image_np, contours, -1, (255, 0, 0), 2)  # íŒŒë€ìƒ‰ ê²½ê³„ì„ 

        # í˜íŠ¸ë¦¬ ì ‘ì‹œ ë§ˆìŠ¤í¬ ì²˜ë¦¬ (ì™¸ê³½ì„ ë§Œ)
        if dish_annotation is not None:
            dish_mask = dish_annotation.cpu().numpy()
            if dish_mask.ndim == 3 and dish_mask.shape[0] == 1:
                dish_mask = dish_mask[0]
            if dish_mask.ndim == 2:
                dish_mask = dish_mask > 0
                if withContours:
                    contours = cv2.findContours(dish_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    contours = contours[0] if len(contours) == 2 else contours[1]
                    cv2.drawContours(image_np, contours, -1, (0, 0, 255), 3)  # ë¹¨ê°„ìƒ‰ ì™¸ê³½ì„ 

        processed_image = Image.fromarray(image_np)
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë³µì› (original_sizeê°€ ì œê³µëœ ê²½ìš°)
        if original_size is not None:
            processed_image = processed_image.resize(original_size, Image.LANCZOS)
            
        return processed_image
    except Exception as e:
        print(f"Error in fast_process: {str(e)}")
        return image

class ColonyCounter:
    """
    ì½œë¡œë‹ˆ ìˆ˜ë™ ì¹´ìš´íŒ… ë„êµ¬ í´ë˜ìŠ¤
    - ìë™ ê°ì§€ëœ í¬ì¸íŠ¸ì™€ ìˆ˜ë™ ì¶”ê°€ í¬ì¸íŠ¸ë¥¼ ê´€ë¦¬
    - ì´ë¯¸ì§€ì— í¬ì¸íŠ¸ë¥¼ ì‹œê°í™”
    - ì œê±° ëª¨ë“œ ì§€ì›
    """
    def __init__(self):
        self.manual_points = []  # ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ í¬ì¸íŠ¸ ëª©ë¡
        self.auto_points = []  # ìë™ìœ¼ë¡œ ê°ì§€ëœ í¬ì¸íŠ¸ ëª©ë¡
        self.auto_annotations = []  # ìë™ìœ¼ë¡œ ê°ì§€ëœ ì½œë¡œë‹ˆ ì• ë…¸í…Œì´ì…˜
        self.dish_annotation = None  # í˜íŠ¸ë¦¬ ì ‘ì‹œ ì• ë…¸í…Œì´ì…˜
        self.current_image = None  # í˜„ì¬ í‘œì‹œ ì¤‘ì¸ ì´ë¯¸ì§€
        self.original_image = None  # ì›ë³¸ ì´ë¯¸ì§€ (ë¦¬ì‚¬ì´ì§• í›„)
        self.base_image = None  # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì´ë¯¸ì§€ (ì˜¤ë²„ë ˆì´ ì „)
        self.auto_detected_count = 0  # ìë™ ê°ì§€ëœ ì½œë¡œë‹ˆ ìˆ˜
        self.remove_mode = False  # ì œê±° ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        self.last_method = None  # ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©í•œ ë¶„ì„ ë°©ë²•
        self.scale_factor = 1.0  # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ë¹„ìœ¨ ì €ì¥ ë³€ìˆ˜ ì¶”ê°€
        self.removed_history = []  # ì‚­ì œëœ í¬ì¸íŠ¸ ê¸°ë¡ (ì›ë³µ ê¸°ëŠ¥ì„ ìœ„í•¨)

    def reset(self):
        """ëª¨ë“  ìƒíƒœ ì´ˆê¸°í™”"""
        self.manual_points = []
        self.auto_points = []
        self.auto_annotations = []
        self.dish_annotation = None
        self.current_image = None
        self.base_image = None
        self.auto_detected_count = 0
        self.remove_mode = False
        self.original_image = None
        self.last_method = None
        self.scale_factor = 1.0  # ë¦¬ì…‹ ì‹œ scale_factorë„ ì´ˆê¸°í™”
        self.removed_history = []  # ì‚­ì œ ê¸°ë¡ë„ ì´ˆê¸°í™”

    def set_original_image(self, image):
        """ì›ë³¸ ì´ë¯¸ì§€ ì„¤ì •"""
        if isinstance(image, Image.Image):
            self.original_image = np.array(image)
            # base_imageë„ í•¨ê»˜ ì„¤ì • (ì´ˆê¸°ì—ëŠ” ì›ë³¸ê³¼ ë™ì¼)
            self.base_image = self.original_image.copy()
        else:
            self.original_image = image
            self.base_image = image.copy() if image is not None else None

    def toggle_remove_mode(self):
        """ì œê±° ëª¨ë“œ í† ê¸€"""
        self.remove_mode = not self.remove_mode
        # ì œê±° ëª¨ë“œê°€ ë³€ê²½ë˜ë©´ í˜„ì¬ ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œ ê·¸ë¦¬ê¸°
        current_img = self.draw_points()
        mode_text = "ğŸ”´ Remove Mode" if self.remove_mode else "ğŸŸ¢ Add Mode"
        return current_img, mode_text

    def find_closest_point(self, x, y, threshold=30):
        # ìë™ í¬ì¸íŠ¸ì™€ ìˆ˜ë™ í¬ì¸íŠ¸ ëª¨ë‘ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì  ì°¾ê¸°
        all_points = self.auto_points + self.manual_points
        if not all_points:
            return None, None
        
        # í´ë¦­ ì¢Œí‘œëŠ” ì´ë¯¸ UI ì¢Œí‘œê³„ì´ë¯€ë¡œ, ë¹„êµë¥¼ ìœ„í•´ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•  í•„ìš” ì—†ìŒ
        scaled_x = x
        scaled_y = y

        distances = []
        for idx, (px, py) in enumerate(all_points):
            dist = np.sqrt((scaled_x - px) ** 2 + (scaled_y - py) ** 2)
            distances.append((dist, idx))

        if not distances:
            return None, None

        closest_dist, closest_idx = min(distances, key=lambda item: item[0])
        if closest_dist < threshold:
            # auto_points ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìë™/ìˆ˜ë™ì„ êµ¬ë¶„
            is_auto = (closest_idx < len(self.auto_points))
            return closest_idx, is_auto
        return None, None

    def debug_find_closest(self, x, y, threshold=30):
        """ë””ë²„ê¹…ìš© í•¨ìˆ˜: ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ ì°¾ê¸° ê³¼ì •ì„ ìì„¸íˆ ì¶œë ¥"""
        all_points = self.auto_points + self.manual_points
        if not all_points:
            print("í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
        
        # í´ë¦­ ì¢Œí‘œëŠ” ì´ë¯¸ UI ì¢Œí‘œê³„ì´ë¯€ë¡œ, ë¹„êµë¥¼ ìœ„í•´ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•  í•„ìš” ì—†ìŒ
        scaled_x = x
        scaled_y = y
        print(f"í´ë¦­ ì¢Œí‘œ: ({x}, {y}) -> ë³€í™˜ ì¢Œí‘œ: ({scaled_x}, {scaled_y})")
        print(f"ìŠ¤ì¼€ì¼ íŒ©í„°: {self.scale_factor}")
        
        if len(self.auto_points) > 0:
            print(f"ìë™ í¬ì¸íŠ¸ ê°œìˆ˜: {len(self.auto_points)}")
            for i, (px, py) in enumerate(self.auto_points[:5]):
                dist = np.sqrt((scaled_x - px) ** 2 + (scaled_y - py) ** 2)
                print(f"  ìë™ í¬ì¸íŠ¸ {i}: ({px}, {py}), ê±°ë¦¬: {dist}")
            if len(self.auto_points) > 5:
                print(f"  ... ì™¸ {len(self.auto_points)-5}ê°œ")
        
        if len(self.manual_points) > 0:
            print(f"ìˆ˜ë™ í¬ì¸íŠ¸ ê°œìˆ˜: {len(self.manual_points)}")
            for i, (px, py) in enumerate(self.manual_points[:5]):
                dist = np.sqrt((scaled_x - px) ** 2 + (scaled_y - py) ** 2)
                print(f"  ìˆ˜ë™ í¬ì¸íŠ¸ {i}: ({px}, {py}), ê±°ë¦¬: {dist}")
            if len(self.manual_points) > 5:
                print(f"  ... ì™¸ {len(self.manual_points)-5}ê°œ")

        distances = []
        for idx, (px, py) in enumerate(all_points):
            dist = np.sqrt((scaled_x - px) ** 2 + (scaled_y - py) ** 2)
            distances.append((dist, idx))

        if not distances:
            print("ê±°ë¦¬ ê³„ì‚° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None

        closest_dist, closest_idx = min(distances, key=lambda item: item[0])
        print(f"ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ ì¸ë±ìŠ¤: {closest_idx}, ê±°ë¦¬: {closest_dist}")
        print(f"ì„ê³„ê°’: {threshold}")
        
        if closest_dist < threshold:
            is_auto = (closest_idx < len(self.auto_points))
            type_str = "ìë™" if is_auto else "ìˆ˜ë™"
            print(f"ì„ íƒëœ í¬ì¸íŠ¸: {type_str} í¬ì¸íŠ¸ {closest_idx}, ì„ê³„ê°’ ë²”ìœ„ ë‚´ì— ìˆìŒ")
            return closest_idx, is_auto
        else:
            print(f"ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ë„ ì„ê³„ê°’ë³´ë‹¤ ë©€ë¦¬ ìˆìŠµë‹ˆë‹¤.")
        return None, None

    def add_or_remove_point(self, image, evt: gr.SelectData):
        """
        ì´ë¯¸ì§€ì— í¬ì¸íŠ¸ ì¶”ê°€ ë˜ëŠ” ì œê±°í•˜ëŠ” í•¨ìˆ˜
        - í´ë¦­í•œ ìœ„ì¹˜ì— í¬ì¸íŠ¸ ì¶”ê°€(ì¼ë°˜ ëª¨ë“œ) ë˜ëŠ” ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ ì œê±°(ì œê±° ëª¨ë“œ)
        """
        try:
            if self.base_image is None and image is not None:
                self.base_image = np.array(image)
                self.current_image = self.base_image.copy()

            x, y = evt.index

            if self.remove_mode:
                # ì œê±° ëª¨ë“œì¸ ê²½ìš°, ê°€ì¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸ ì°¾ê¸°
                closest_idx, is_auto = self.find_closest_point(x, y)
                
                if closest_idx is not None:
                    if is_auto:
                        # ìë™ ê°ì§€ í¬ì¸íŠ¸ ì œê±° ì „ ì •ë³´ ì €ì¥
                        removed_point = self.auto_points[closest_idx]
                        removed_annotation = None
                        if len(self.auto_annotations) > closest_idx:
                            removed_annotation = self.auto_annotations[closest_idx]
                        # ì‚­ì œ ì •ë³´ ì €ì¥: (ìœ í˜•, ì¸ë±ìŠ¤, ì¢Œí‘œ, ì• ë…¸í…Œì´ì…˜)
                        self.removed_history.append(("auto", closest_idx, removed_point, removed_annotation))
                        
                        # ìë™ ê°ì§€ í¬ì¸íŠ¸ ì œê±°
                        del self.auto_points[closest_idx]
                        # í•´ë‹¹ ì• ë…¸í…Œì´ì…˜ë„ í•¨ê»˜ ì œê±°
                        if len(self.auto_annotations) > closest_idx:
                            del self.auto_annotations[closest_idx]
                        self.auto_detected_count -= 1
                    else:
                        # ìˆ˜ë™ ì¶”ê°€ í¬ì¸íŠ¸ ì œê±° ì „ ì •ë³´ ì €ì¥
                        manual_idx = closest_idx - len(self.auto_points)
                        removed_point = self.manual_points[manual_idx]
                        # ì‚­ì œ ì •ë³´ ì €ì¥: (ìœ í˜•, ì¸ë±ìŠ¤, ì¢Œí‘œ)
                        self.removed_history.append(("manual", manual_idx, removed_point, None))
                        
                        # ìˆ˜ë™ ì¶”ê°€ í¬ì¸íŠ¸ ì œê±°
                        del self.manual_points[manual_idx]
            else:
                # ì¼ë°˜ ëª¨ë“œì¸ ê²½ìš°, í´ë¦­ ìœ„ì¹˜ì— ìˆ˜ë™ í¬ì¸íŠ¸ ì¶”ê°€
                # í´ë¦­ ì¢Œí‘œëŠ” ì´ë¯¸ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ì „ë‹¬ë˜ë¯€ë¡œ ë³€í™˜ ë¶ˆí•„ìš”
                self.manual_points.append((x, y))

            # í¬ì¸íŠ¸ ë°˜ì˜í•œ ì´ë¯¸ì§€ ë‹¤ì‹œ ê·¸ë¦¬ê¸°
            img_with_points = self.draw_points()
            return img_with_points, self.get_count_text()
        except Exception as e:
            print(f"Error in add_or_remove_point: {str(e)}")
            import traceback
            traceback.print_exc()
            return image, self.get_count_text()

    def remove_last_point(self, image):
        try:
            if self.manual_points:
                self.manual_points.pop()
                img_with_points = self.draw_points()
                return img_with_points, self.get_count_text()
            return image, self.get_count_text()
        except Exception as e:
            print(f"Error in remove_last_point: {str(e)}")
            return image, self.get_count_text()

    def get_count_text(self):
        try:
            method_text = f"Method: {self.last_method}\n" if self.last_method != "NONE" else ""
            total = self.auto_detected_count + len(self.manual_points)
            return (f"{method_text}Total Colony Count: {total}\n"
                    f"ğŸ¤– Auto detected: {self.auto_detected_count}\n"
                    f"ğŸ‘† Manually added: {len(self.manual_points)}")
        except Exception as e:
            print(f"Error in get_count_text: {str(e)}")
            return "Error calculating count"

    def draw_points(self):
        """
        ì´ë¯¸ì§€ì— ì½œë¡œë‹ˆ í¬ì¸íŠ¸ì™€ ë²ˆí˜¸ë¥¼ ê·¸ë¦¬ëŠ” ë©”ì„œë“œ
        
        ê¸°ëŠ¥:
        1. ìë™ ê°ì§€ëœ ì½œë¡œë‹ˆì— ë²ˆí˜¸ í‘œì‹œ (í°ìƒ‰ í…ìŠ¤íŠ¸ + ê²€ì€ìƒ‰ ì™¸ê³½ì„ )
        2. ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ í¬ì¸íŠ¸ì— ì‚¬ê°í˜•ê³¼ ë²ˆí˜¸ í‘œì‹œ (ë¹¨ê°„ìƒ‰)
        3. ì œê±° ëª¨ë“œì¼ ë•Œ ìƒë‹¨ì— í‘œì‹œ
        
        [ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥í•œ ë¶€ë¶„]
        1. ë§ˆì»¤ í¬ê¸°:
           - square_size: ìˆ˜ë™ í¬ì¸íŠ¸ì˜ ì‚¬ê°í˜• í¬ê¸° (ê¸°ë³¸ê°’: 25)
           - font: í°íŠ¸ ì¢…ë¥˜ (ê¸°ë³¸ê°’: cv2.FONT_HERSHEY_SIMPLEX)
           - font_scale: í…ìŠ¤íŠ¸ í¬ê¸° (ê¸°ë³¸ê°’: 0.77)
           - font_thickness: ë‚´ë¶€ í…ìŠ¤íŠ¸ ë‘ê»˜ (ê¸°ë³¸ê°’: 1)
           - outline_thickness: ì™¸ê³½ì„  ë‘ê»˜ (ê¸°ë³¸ê°’: 4)

        2. ìƒ‰ìƒ ì„¤ì •:
           - ìë™ ê°ì§€ ì½œë¡œë‹ˆ:
             * í…ìŠ¤íŠ¸ ìƒ‰ìƒ: (255, 255, 255) # í°ìƒ‰
             * ì™¸ê³½ì„  ìƒ‰ìƒ: (0, 0, 0) # ê²€ì€ìƒ‰
           - ìˆ˜ë™ ì¶”ê°€ ì½œë¡œë‹ˆ:
             * ì‚¬ê°í˜• ìƒ‰ìƒ: (255, 0, 0) # ë¹¨ê°„ìƒ‰
             * í…Œë‘ë¦¬ ìƒ‰ìƒ: (0, 255, 255) # ì‹œì•ˆìƒ‰
             * í…ìŠ¤íŠ¸ ìƒ‰ìƒ: (255, 0, 0) # ë¹¨ê°„ìƒ‰

        3. íˆ¬ëª…ë„ ì„¤ì •:
           - overlay_opacity: ì˜¤ë²„ë ˆì´ íˆ¬ëª…ë„ (ê¸°ë³¸ê°’: 0.4)
           - remove_mode_opacity: ì œê±° ëª¨ë“œ ë°°ë„ˆ íˆ¬ëª…ë„ (ê¸°ë³¸ê°’: 0.3)
        
        ë°˜í™˜ê°’:
        - í¬ì¸íŠ¸ì™€ ë²ˆí˜¸ê°€ í‘œì‹œëœ ì´ë¯¸ì§€
        - ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
        """
        try:
            if self.base_image is None:
                return None

            # ê¸°ë°˜ ì´ë¯¸ì§€(ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ë²ˆ ìƒˆë¡­ê²Œ ê·¸ë¦¬ê¸°
            img_with_points = self.base_image.copy()
            overlay = np.zeros_like(img_with_points)

            ###########################################
            # Clear previous drawings by creating a fresh copy of the original image
            img_with_points = self.base_image.copy()
            overlay = np.zeros_like(img_with_points)

            # [ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥í•œ ë³€ìˆ˜ë“¤]
            ###########################################
            # 1. ë§ˆì»¤ í¬ê¸° ë° í°íŠ¸ ì„¤ì •
            square_size = 25  # ìˆ˜ë™ í¬ì¸íŠ¸ì˜ ì‚¬ê°í˜• í¬ê¸°
            font = cv2.FONT_HERSHEY_SIMPLEX  # í°íŠ¸ ì¢…ë¥˜
            font_scale = 0.8  # í…ìŠ¤íŠ¸ í¬ê¸° (ì•½ê°„ í‚¤ì›€)
            font_thickness = 2  # ë‚´ë¶€ í…ìŠ¤íŠ¸ ë‘ê»˜ (ë‘ê»ê²Œ ë³€ê²½)
            outline_thickness = 4  # ì™¸ê³½ì„  ë‘ê»˜

            # 2. ìƒ‰ìƒ ì„¤ì • (B, G, R í˜•ì‹)
            AUTO_TEXT_COLOR = (255, 255, 255)  # ìë™ ê°ì§€ í…ìŠ¤íŠ¸ ìƒ‰ìƒ (í°ìƒ‰)
            AUTO_OUTLINE_COLOR = (0, 0, 0)     # ìë™ ê°ì§€ ì™¸ê³½ì„  ìƒ‰ìƒ (ê²€ì€ìƒ‰)
            MANUAL_RECT_COLOR = (255, 0, 0)    # ìˆ˜ë™ ì¶”ê°€ ì‚¬ê°í˜• ìƒ‰ìƒ (ë¹¨ê°„ìƒ‰)
            MANUAL_BORDER_COLOR = (255, 255, 0)  # ìˆ˜ë™ ì¶”ê°€ í…Œë‘ë¦¬ ìƒ‰ìƒ (ì‹œì•ˆìƒ‰)
            MANUAL_TEXT_COLOR = (255, 0, 0)    # ìˆ˜ë™ ì¶”ê°€ í…ìŠ¤íŠ¸ ìƒ‰ìƒ (ë¹¨ê°„ìƒ‰)
            MANUAL_OUTLINE_COLOR = (0, 255, 255)  # ìˆ˜ë™ ì¶”ê°€ ì™¸ê³½ì„  ìƒ‰ìƒ (ì‹œì•ˆìƒ‰)

            # 3. íˆ¬ëª…ë„ ì„¤ì •
            OVERLAY_OPACITY = 0.5  # ì˜¤ë²„ë ˆì´ íˆ¬ëª…ë„ (ì•½ê°„ ë” ì„ ëª…í•˜ê²Œ)
            REMOVE_MODE_OPACITY = 0.4  # ì œê±° ëª¨ë“œ ë°°ë„ˆ íˆ¬ëª…ë„ (ì•½ê°„ ë” ì„ ëª…í•˜ê²Œ)

            ###########################################
            # 1. ìë™ ê°ì§€ëœ ì½œë¡œë‹ˆ ë²ˆí˜¸ í‘œì‹œ
            ###########################################
            for idx, (x, y) in enumerate(self.auto_points, 1):
                # ì¢Œí‘œëŠ” ì´ë¯¸ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ì´ë¯€ë¡œ í˜„ì¬ í™”ë©´ í¬ê¸°ì— ë§ê²Œ ì§ì ‘ í‘œì‹œ
                display_x = x
                display_y = y
                
                text = str(idx)
                # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°í•˜ì—¬ ì¤‘ì•™ ì •ë ¬
                (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
                text_x = int(display_x - text_width / 2)
                text_y = int(display_y - 10)

                # [ì¤‘ìš”] 8ë°©í–¥ ê²€ì€ìƒ‰ ì™¸ê³½ì„ ìœ¼ë¡œ í…ìŠ¤íŠ¸ ê°€ì‹œì„± í–¥ìƒ
                # dx, dyë¡œ 8ë°©í–¥ì˜ ì˜¤í”„ì…‹ì„ ì§€ì •í•˜ì—¬ ì™¸ê³½ì„  ìƒì„±
                for dx, dy in [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]:
                    cv2.putText(img_with_points, text,
                              (text_x + dx, text_y + dy),
                              font, font_scale, AUTO_OUTLINE_COLOR, outline_thickness)

                # í°ìƒ‰ í…ìŠ¤íŠ¸ë¥¼ ì™¸ê³½ì„  ìœ„ì— ê·¸ë¦¬ê¸°
                cv2.putText(img_with_points, text,
                          (text_x, text_y),
                          font, font_scale, AUTO_TEXT_COLOR, font_thickness)

            ###########################################
            # 2. ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€ëœ í¬ì¸íŠ¸ í‘œì‹œ
            ###########################################
            for idx, (x, y) in enumerate(self.manual_points, len(self.auto_points) + 1):
                # ì¢Œí‘œëŠ” ì´ë¯¸ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ì´ë¯€ë¡œ í˜„ì¬ í™”ë©´ í¬ê¸°ì— ë§ê²Œ ì§ì ‘ í‘œì‹œ
                display_x = x
                display_y = y
                
                # ì‚¬ê°í˜• ì¢Œí‘œ ê³„ì‚°
                pt1 = (int(display_x - square_size / 2), int(display_y - square_size / 2))
                pt2 = (int(display_x + square_size / 2), int(display_y + square_size / 2))
                
                # [ì¤‘ìš”] ë°˜íˆ¬ëª… ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
                cv2.rectangle(overlay, pt1, pt2, MANUAL_RECT_COLOR, -1)  # ìƒ‰ìƒ ì±„ìš°ê¸°
                cv2.rectangle(overlay, pt1, pt2, MANUAL_BORDER_COLOR, 2)  # í…Œë‘ë¦¬

                # ë²ˆí˜¸ í…ìŠ¤íŠ¸ ì¶”ê°€
                text = str(idx)
                (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
                text_x = int(display_x - text_width / 2)
                text_y = int(display_y - 10)

                # 8ë°©í–¥ ì‹œì•ˆìƒ‰ ì™¸ê³½ì„  (ë³€ê²½ë¨)
                for dx, dy in [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]:
                    cv2.putText(overlay, text,
                              (text_x + dx, text_y + dy),
                              font, font_scale, MANUAL_OUTLINE_COLOR, outline_thickness)

                # ë¹¨ê°„ìƒ‰ í…ìŠ¤íŠ¸
                cv2.putText(img_with_points, text,
                          (text_x, text_y),
                          font, font_scale, MANUAL_TEXT_COLOR, font_thickness)

            # [ì¤‘ìš”] ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ê³¼ ë¸”ë Œë”©
            cv2.addWeighted(overlay, OVERLAY_OPACITY, img_with_points, 1.0, 0, img_with_points)

            ###########################################
            # 3. ì œê±° ëª¨ë“œ í‘œì‹œ
            ###########################################
            if self.remove_mode:
                # ìƒë‹¨ì— ê·¸ë¼ë°ì´ì…˜ íš¨ê³¼ê°€ ìˆëŠ” ë¹¨ê°„ìƒ‰ ë°°ë„ˆ ì¶”ê°€
                overlay = img_with_points.copy()
                h, w = overlay.shape[:2]
                
                # ê·¸ë¼ë°ì´ì…˜ íš¨ê³¼ë¥¼ ìœ„í•œ ë§ˆìŠ¤í¬ ìƒì„±
                gradient = np.zeros((40, w, 3), dtype=np.uint8)
                for i in range(40):
                    alpha = 1.0 - (i / 40.0) * 0.3  # ìœ„ì—ì„œ ì•„ë˜ë¡œ ì ì  íˆ¬ëª…í•´ì§€ëŠ” íš¨ê³¼
                    gradient[i, :] = (0, 0, 255 * alpha)  # ë¹¨ê°„ìƒ‰ ê·¸ë¼ë°ì´ì…˜
                
                # ë°°ë„ˆ ì˜ì—­ì— ê·¸ë¼ë°ì´ì…˜ ì ìš©
                overlay[:40, :] = cv2.addWeighted(overlay[:40, :], 0.6, gradient, 0.8, 0)
                
                # ì „ì²´ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´ ì ìš©
                cv2.addWeighted(overlay, REMOVE_MODE_OPACITY, img_with_points, 0.7, 0, img_with_points)
                
                # "REMOVE MODE" í…ìŠ¤íŠ¸ í‘œì‹œ (ë” ëˆˆì— ë„ê²Œ)
                cv2.putText(img_with_points, "REMOVE MODE", (10, 30),
                          font, 1, (255, 255, 255), 2)  # í°ìƒ‰ í…ìŠ¤íŠ¸

            return img_with_points
        except Exception as e:
            print(f"Error in draw_points: {str(e)}")
            return self.base_image

    def save_results(self, output_dir, img_filename):
        """ê²°ê³¼ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì €ì¥"""
        try:
            image_name = os.path.splitext(img_filename)[0]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = os.path.join(output_dir, f"{image_name}_{timestamp}")
            os.makedirs(save_dir, exist_ok=True)
            
            if self.original_image is not None:
                original_path = os.path.join(save_dir, "original.png")
                cv2.imwrite(original_path, cv2.cvtColor(self.original_image, cv2.COLOR_RGB2BGR))
            
            # í¬ì¸íŠ¸ê°€ ê·¸ë ¤ì§„ í˜„ì¬ ì´ë¯¸ì§€ ì €ì¥
            current_img = self.draw_points()
            if current_img is not None:
                result_path = os.path.join(save_dir, "result.png")
                cv2.imwrite(result_path, cv2.cvtColor(current_img, cv2.COLOR_RGB2BGR))
            
            result_txt = os.path.join(save_dir, "count_results.txt")
            with open(result_txt, 'w') as f:
                f.write(self.get_count_text())
                
            return f"Results saved to {save_dir}"
        except Exception as e:
            return f"Error saving results: {str(e)}"

    def undo_last_removal(self, image):
        """
        ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚­ì œëœ í¬ì¸íŠ¸ë¥¼ ë³µì›í•˜ëŠ” í•¨ìˆ˜
        - ì‚­ì œ ê¸°ë¡ì´ ì—†ìœ¼ë©´ í˜„ì¬ ì´ë¯¸ì§€ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        - ì‚­ì œ ê¸°ë¡ì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ì‚­ì œëœ í¬ì¸íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì™€ ë³µì›
        """
        try:
            # ì‚­ì œ ê¸°ë¡ì´ ì—†ìœ¼ë©´ ë³µì›í•  ê²ƒì´ ì—†ìŒ
            if not self.removed_history:
                return image, self.get_count_text() + "\nì‚­ì œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ë§ˆì§€ë§‰ ì‚­ì œ ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            last_removal = self.removed_history.pop()
            removal_type, index, point, annotation = last_removal
            
            # ì‚­ì œëœ í¬ì¸íŠ¸ ìœ í˜•ì— ë”°ë¼ ë³µì›
            if removal_type == "auto":
                # ìë™ í¬ì¸íŠ¸ ë³µì›
                self.auto_points.insert(index, point)
                if annotation is not None:
                    # ì• ë…¸í…Œì´ì…˜ë„ í•¨ê»˜ ë³µì›
                    self.auto_annotations.insert(index, annotation)
                self.auto_detected_count += 1
            else:
                # ìˆ˜ë™ í¬ì¸íŠ¸ ë³µì›
                self.manual_points.insert(index, point)
            
            # í¬ì¸íŠ¸ê°€ ë°˜ì˜ëœ ì´ë¯¸ì§€ ë‹¤ì‹œ ê·¸ë¦¬ê¸°
            img_with_points = self.draw_points()
            return img_with_points, self.get_count_text() + "\në§ˆì§€ë§‰ ì‚­ì œëœ í¬ì¸íŠ¸ê°€ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"Error in undo_last_removal: {str(e)}")
            import traceback
            traceback.print_exc()
            return image, self.get_count_text() + f"\në³µì› ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

class ImagePreprocessHistory:
    def __init__(self):
        self.original_image = None
        self.current_image = None
        self.history = []
        self.current_index = -1

    def set_original(self, image):
        """ì›ë³¸ ì´ë¯¸ì§€ ì„¤ì •"""
        if image is None:
            return None
            
        # PIL Imageë¡œ ë³€í™˜
        if not isinstance(image, Image.Image):
            try:
                image = Image.fromarray(image)
            except Exception as e:
                print(f"Error converting image: {str(e)}")
                return None
        
        self.original_image = image
        self.current_image = image
        self.history = [image]
        self.current_index = 0
        return image

    def add_state(self, image):
        """ìƒˆë¡œìš´ ì´ë¯¸ì§€ ìƒíƒœ ì¶”ê°€"""
        if image is None:
            return None
            
        # PIL Imageë¡œ ë³€í™˜
        if not isinstance(image, Image.Image):
            try:
                image = Image.fromarray(image)
            except Exception as e:
                print(f"Error converting image: {str(e)}")
                return None
        
        # í˜„ì¬ ì¸ë±ìŠ¤ ì´í›„ì˜ íˆìŠ¤í† ë¦¬ëŠ” ì‚­ì œ
        self.history = self.history[:self.current_index + 1]
        self.history.append(image)
        self.current_index = len(self.history) - 1
        self.current_image = image
        return image

    def reset(self):
        """ì›ë³¸ ì´ë¯¸ì§€ë¡œ ë³µì›"""
        if self.original_image is not None:
            self.current_image = self.original_image
            self.current_index = 0
            return self.original_image
        return None

    def redo(self):
        """ì´ì „ ìƒíƒœë¡œ ë³µì›"""
        if self.current_index > 0:
            self.current_index -= 1
            self.current_image = self.history[self.current_index]
            return self.current_image
        return self.current_image

# ì „ì—­ ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ ê°ì²´
image_history = ImagePreprocessHistory()

def segment_and_count_colonies(
    input_image,
    method='fastsam',
    input_size=1024,
    iou_threshold=0.7,
    conf_threshold=0.25,
    better_quality=False,
    withContours=True,
    mask_random_color=True,
    min_area_percentile=1,
    max_area_percentile=99,
    circularity_threshold=0.8
):
    try:
        if input_image is None:
            return None, "No input image provided.", None

        # ìƒˆë¡œìš´ counter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        new_counter = ColonyCounter()
        new_counter.reset()
        new_counter.set_original_image(input_image)
        new_counter.last_method = method.upper()

        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥
        original_size = input_image.size

        # ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
        input_size = int(input_size)
        w, h = input_image.size
        scale = input_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        input_resized = input_image.resize((new_w, new_h))
        
        # ìŠ¤ì¼€ì¼ ë¹„ìœ¨ ì €ì¥
        new_counter.scale_factor = scale

        # FastSAM ëª¨ë¸ ì˜ˆì¸¡
        input_array = np.array(input_resized)
        results = model.predict(
            source=input_array,
            device=device,
            conf=conf_threshold,
            iou=iou_threshold,
            imgsz=input_size,
            retina_masks=True  # FastSAM ë§ˆìŠ¤í¬ë¥¼ ìœ„í•´ í•„ìš”
        )

        annotations = getattr(results[0].masks, 'data', None)
        if annotations is None or len(annotations) == 0:
            new_counter.current_image = np.array(input_resized)
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            result_image = Image.fromarray(new_counter.current_image).resize(original_size)
            new_counter.current_image = np.array(result_image)
            return np.array(result_image), "No colonies detected", new_counter

        # ê° ë§ˆìŠ¤í¬ ë©´ì 
        areas = [np.sum(ann.cpu().numpy() > 0) for ann in annotations]

        # ê°€ì¥ í° ë§ˆìŠ¤í¬ = í˜íŠ¸ë¦¬ ì ‘ì‹œ(dish)
        dish_idx = np.argmax(areas)
        dish_annotation = annotations[dish_idx]
        colony_annotations = [ann for idx, ann in enumerate(annotations) if idx != dish_idx]

        # í•„í„°ë§ + ì›í˜•ë„ ê³„ì‚°
        valid_colony_annotations = []
        new_counter.auto_points = []  # ìë™ ê°ì§€ëœ ì  ì´ˆê¸°í™”

        for ann in colony_annotations:
            ann_cpu = ann.cpu().numpy()
            if ann_cpu.ndim == 3 and ann_cpu.shape[0] == 1:
                ann_cpu = ann_cpu[0]
            mask = ann_cpu > 0
            area = np.sum(mask)

            contours = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            contours = contours[0] if len(contours) == 2 else contours[1]
            if contours and len(contours) > 0:
                perimeter = cv2.arcLength(contours[0], True)
                circularity = 4 * np.pi * (area / (perimeter ** 2)) if perimeter > 0 else 0
                if circularity >= circularity_threshold:
                    valid_colony_annotations.append(ann)
                    # ë§ˆìŠ¤í¬ ì¤‘ì‹¬ì 
                    y_indices, x_indices = np.where(mask)
                    center_x = int(np.mean(x_indices))
                    center_y = int(np.mean(y_indices))
                    
                    # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                    # scale_factorëŠ” ë¦¬ì‚¬ì´ì¦ˆ ë¹„ìœ¨ì´ë¯€ë¡œ, ì›ë³¸ ì¢Œí‘œ = ë¦¬ì‚¬ì´ì¦ˆ ì¢Œí‘œ / scale_factor
                    original_center_x = int(center_x / scale)
                    original_center_y = int(center_y / scale)
                    
                    # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ì €ì¥
                    new_counter.auto_points.append((original_center_x, original_center_y))

        if valid_colony_annotations:
            processed_image = fast_process(
                colony_annotations=valid_colony_annotations,
                dish_annotation=dish_annotation,
                image=input_resized,
                mask_random_color=mask_random_color,
                withContours=withContours,
                original_size=original_size
            )
        else:
            processed_image = input_resized
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            if original_size is not None:
                processed_image = processed_image.resize(original_size)

        # counter ê°ì²´ì— ê²°ê³¼ ë°˜ì˜
        if isinstance(processed_image, Image.Image):
            new_counter.base_image = np.array(processed_image)
        else:
            new_counter.base_image = processed_image

        new_counter.auto_detected_count = len(new_counter.auto_points)
        new_counter.current_image = new_counter.draw_points()

        if new_counter is not None:
            counter.auto_points = new_counter.auto_points
            counter.auto_annotations = new_counter.auto_annotations
            counter.auto_detected_count = new_counter.auto_detected_count
            counter.current_image = new_counter.current_image
            counter.base_image = new_counter.base_image
            counter.original_image = new_counter.original_image
            counter.last_method = new_counter.last_method
            counter.scale_factor = new_counter.scale_factor

        return new_counter.current_image, new_counter.get_count_text(), new_counter
    except Exception as e:
        print(f"Error in segment_and_count_colonies: {str(e)}")
        if input_image is not None:
            return np.array(input_image), f"Error processing image: {str(e)}", None
        return None, f"Error processing image: {str(e)}", None

def batch_process(
    input_dir,
    output_dir='',
    method='fastsam',
    input_size=1024,
    iou_threshold=0.7,
    conf_threshold=0.25,
    better_quality=False,
    withContours=True,
    mask_random_color=True,
    min_area_percentile=1,
    max_area_percentile=99,
    circularity_threshold=0.8
):
    """ì—¬ëŸ¬ ì´ë¯¸ì§€ ì¼ê´„ ì²˜ë¦¬"""
    try:
        if not isinstance(input_dir, str) or not os.path.isdir(input_dir):
            return "Invalid input directory.", []

        if not output_dir or output_dir.strip() == "":
            output_dir = DEFAULT_OUTPUT_DIR

        try:
            os.makedirs(output_dir, exist_ok=True)
        except Exception as e:
            return f"Error creating output directory: {str(e)}", []

        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')
        image_files = [f for f in os.listdir(input_dir) 
                       if f.lower().endswith(valid_extensions)]
        
        if not image_files:
            return "No valid image files found in the input directory.", []

        results = []
        progress_text = (f"Processing images from: {input_dir}\n"
                         f"Saving results to: {output_dir}\n\n")
        gallery_items = []
        
        for idx, img_file in enumerate(image_files, 1):
            try:
                progress_text += f"Processing image {idx}/{len(image_files)}: {img_file}\n"
                img_path = os.path.join(input_dir, img_file)

                try:
                    input_image = Image.open(img_path).convert('RGB')
                except Exception as e:
                    raise Exception(f"Error loading image: {str(e)}")
                
                # ì´ë¯¸ì§€ ë¶„ì„
                result_image, count_text, counter = segment_and_count_colonies(
                    input_image=input_image,
                    method=method,
                    input_size=input_size,
                    iou_threshold=iou_threshold,
                    conf_threshold=conf_threshold,
                    better_quality=better_quality,
                    withContours=withContours,
                    mask_random_color=mask_random_color,
                    min_area_percentile=min_area_percentile,
                    max_area_percentile=max_area_percentile,
                    circularity_threshold=circularity_threshold
                )
                
                if counter is not None:
                    save_result = counter.save_results(output_dir, img_file)
                else:
                    save_result = "Failed to process image."

                results.append({
                    'filename': img_file,
                    'count_text': count_text,
                    'save_result': save_result
                })
                
                if result_image is not None:
                    gallery_items.append({
                        'image': result_image,
                        'caption': f"{img_file}\n{count_text}"
                    })
                else:
                    gallery_items.append({
                        'image': None,
                        'caption': f"{img_file}\n{count_text}"
                    })
                
                progress_text += f"Processed {img_file}: {count_text}\n{save_result}\n\n"
            except Exception as e:
                error_msg = f"Error processing {img_file}: {str(e)}\n"
                print(error_msg)
                results.append({
                    'filename': img_file,
                    'count_text': f"Error: {str(e)}",
                    'save_result': "Failed to save results"
                })
                gallery_items.append({
                    'image': None,
                    'caption': f"{img_file}\nError: {str(e)}"
                })
                progress_text += error_msg

        summary_path = os.path.join(output_dir, "batch_summary.csv")
        try:
            pd.DataFrame(results).to_csv(summary_path, index=False)
        except Exception as e:
            progress_text += f"Error saving summary CSV: {str(e)}\n"

        # summary.json ìƒì„±
        try:
            create_summary(results, output_dir)
            progress_text += f"\nBatch processing complete. Summary saved to {summary_path} and summary.json"
        except Exception as e:
            progress_text += f"\nError creating summary: {str(e)}"

        return progress_text, gallery_items

    except Exception as e:
        return f"Error in batch processing: {str(e)}", []

def get_output_subdir(output_dir):
    date_str = datetime.now().strftime("%Y-%m-%d")
    time_str = datetime.now().strftime("%H-%M-%S")
    return os.path.join(output_dir, date_str, time_str)

def create_summary(results, output_dir):
    summary = {
        'date': datetime.now().strftime("%Y-%m-%d"),
        'time': datetime.now().strftime("%H:%M:%S"),
        'total_images': len(results),
        'successful': sum(1 for r in results if 'Error' not in r['count_text']),
        'failed': sum(1 for r in results if 'Error' in r['count_text']),
        'results': results
    }
    
    summary_json_path = os.path.join(output_dir, "summary.json")
    with open(summary_json_path, 'w') as f:
        json.dump(summary, f, indent=4)
        
    df = pd.DataFrame(results)
    csv_path = os.path.join(output_dir, "results.csv")
    df.to_csv(csv_path, index=False)

def preprocess_image(image, method='none'):
    """
    ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
    Args:
        image: PIL Image ë˜ëŠ” numpy array
        method: ì „ì²˜ë¦¬ ë°©ë²• ('grayscale', 'binary', 'edge', 'sharpen', 'none')
    Returns:
        ì²˜ë¦¬ëœ PIL Image
    """
    try:
        # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
        if isinstance(image, Image.Image):
            image_np = np.array(image)
        else:
            image_np = image.copy()

        if method == 'none':
            return Image.fromarray(image_np)

        if method == 'grayscale':
            if len(image_np.shape) == 3:
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
                return Image.fromarray(cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB))
            return Image.fromarray(image_np)

        elif method == 'binary':
            if len(image_np.shape) == 3:
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            else:
                gray = image_np
            _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
            return Image.fromarray(cv2.cvtColor(binary, cv2.COLOR_GRAY2RGB))

        elif method == 'edge':
            if len(image_np.shape) == 3:
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
            else:
                gray = image_np
            edges = cv2.Canny(gray, 100, 200)
            return Image.fromarray(cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB))

        elif method == 'sharpen':
            kernel = np.array([[-1,-1,-1],
                             [-1, 9,-1],
                             [-1,-1,-1]])
            sharpened = cv2.filter2D(image_np, -1, kernel)
            return Image.fromarray(sharpened)

        return Image.fromarray(image_np)
    except Exception as e:
        print(f"Error in preprocess_image: {str(e)}")
        return image

# CSS ìŠ¤íƒ€ì¼ ìˆ˜ì •
css = """
/* ê¸°ë³¸ ë ˆì´ì•„ì›ƒ ë° ìƒ‰ìƒ ì²´ê³„ */
:root {
    --primary-color: #3b82f6;
    --primary-dark: #1d4ed8;
    --primary-light: #93c5fd;
    --secondary-color: #10b981;
    --accent-color: #f59e0b;
    --danger-color: #ef4444;
    --text-color: #1f2937;
    --text-light: #6b7280;
    --bg-color: #f9fafb;
    --card-bg: #ffffff;
    --border-color: #e5e7eb;
    --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
    --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
    --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
    --radius-sm: 0.25rem;
    --radius-md: 0.375rem;
    --radius-lg: 0.5rem;
}

body {
    font-family: 'Inter', 'Segoe UI', system-ui, -apple-system, sans-serif;
    color: var(--text-color);
    background-color: var(--bg-color);
}

.container {
    max-width: 1200px;
    margin: auto;
    padding: 1.5rem;
}

/* í—¤ë” ìŠ¤íƒ€ì¼ */
.header {
    text-align: center;
    margin-bottom: 2rem;
    padding: 1.5rem;
    background: linear-gradient(135deg, var(--primary-color), var(--primary-dark));
    color: white;
    border-radius: var(--radius-lg);
    box-shadow: var(--shadow-md);
}

.header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 0.5rem;
}

.header h3 {
    font-size: 1.25rem;
    font-weight: 500;
    opacity: 0.9;
    margin-bottom: 1rem;
}

/* í—¤ë” ë°°ì§€ ìŠ¤íƒ€ì¼ */
.header-badges {
    display: flex;
    justify-content: center;
    gap: 0.75rem;
    margin-top: 1rem;
}

.badge {
    display: inline-block;
    padding: 0.35rem 0.75rem;
    background-color: rgba(255, 255, 255, 0.2);
    border-radius: 9999px;
    font-size: 0.875rem;
    font-weight: 600;
    color: white;
    box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
    transition: all 0.2s ease;
}

.badge:hover {
    background-color: rgba(255, 255, 255, 0.3);
    transform: translateY(-1px);
}

/* ì¹´ë“œ ë° ì„¹ì…˜ ìŠ¤íƒ€ì¼ */
.gradio-container {
    background-color: transparent !important;
}

.gradio-tab-nav {
    background-color: var(--card-bg);
    border-radius: var(--radius-md);
    box-shadow: var(--shadow-sm);
    overflow: hidden;
    border: 1px solid var(--border-color);
}

.gradio-tab-nav button {
    font-weight: 600;
    padding: 0.75rem 1.25rem;
    transition: all 0.2s ease;
}

.gradio-tab-nav button.selected {
    background-color: var(--primary-color);
    color: white;
}

.gradio-tab-nav button:hover:not(.selected) {
    background-color: var(--primary-light);
    color: var(--primary-dark);
}

/* ì´ë¯¸ì§€ ì»¨í…Œì´ë„ˆ */
.input-image, .output-image {
    border: 2px solid var(--border-color);
    border-radius: var(--radius-md);
    background-color: var(--card-bg);
    transition: all 0.3s ease;
    box-shadow: var(--shadow-sm);
    overflow: hidden;
    max-height: 600px;
}

.input-image:hover, .output-image:hover {
    box-shadow: var(--shadow-md);
    border-color: var(--primary-light);
}

/* ì´ë¯¸ì§€ ë ˆì´ë¸” */
.image-label {
    font-size: 1.1rem;
    font-weight: 600;
    margin-bottom: 0.75rem;
    color: var(--primary-dark);
    border-left: 4px solid var(--primary-color);
    padding-left: 0.75rem;
}

/* ê²°ê³¼ í…ìŠ¤íŠ¸ */
.result-text {
    font-family: 'Roboto Mono', 'Consolas', monospace;
    font-size: 1.1rem;
    font-weight: 500;
    background-color: var(--card-bg);
    border-radius: var(--radius-md);
    border-left: 4px solid var(--secondary-color);
    padding: 1rem;
    box-shadow: var(--shadow-sm);
}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
button.primary {
    background: linear-gradient(to right, var(--primary-color), var(--primary-dark));
    color: white;
    font-weight: 600;
    border-radius: var(--radius-md);
    padding: 0.75rem 1.5rem;
    transition: all 0.3s ease;
    box-shadow: var(--shadow-sm);
    border: none;
}

button.primary:hover {
    box-shadow: var(--shadow-md);
    transform: translateY(-2px);
}

button.secondary {
    background-color: var(--card-bg);
    color: var(--primary-color);
    font-weight: 600;
    border: 1px solid var(--primary-color);
    border-radius: var(--radius-md);
    padding: 0.75rem 1.5rem;
    transition: all 0.3s ease;
}

button.secondary:hover {
    background-color: var(--primary-light);
    color: var(--primary-dark);
}

/* ìŠ¬ë¼ì´ë” ë° ì²´í¬ë°•ìŠ¤ */
.gradio-slider input[type=range] {
    accent-color: var(--primary-color);
}

.gradio-checkbox input[type=checkbox] {
    accent-color: var(--primary-color);
}

/* ì•„ì½”ë””ì–¸ */
.gradio-accordion {
    border: 1px solid var(--border-color);
    border-radius: var(--radius-md);
    overflow: hidden;
    box-shadow: var(--shadow-sm);
    margin: 1rem 0;
}

.gradio-accordion .label {
    font-weight: 600;
    padding: 1rem;
    background-color: var(--card-bg);
    border-bottom: 1px solid var(--border-color);
    cursor: pointer;
    transition: all 0.2s ease;
}

.gradio-accordion .label:hover {
    background-color: var(--primary-light);
    color: var(--primary-dark);
}

/* ë°°ì¹˜ ì²˜ë¦¬ ê°¤ëŸ¬ë¦¬ */
.gradio-gallery {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
    gap: 1rem;
    margin-top: 1rem;
}

.gradio-gallery .thumbnail {
    border-radius: var(--radius-md);
    overflow: hidden;
    box-shadow: var(--shadow-sm);
    transition: all 0.3s ease;
}

.gradio-gallery .thumbnail:hover {
    transform: scale(1.03);
    box-shadow: var(--shadow-md);
}

/* í‘¸í„° */
.footer {
    text-align: center;
    padding: 2rem;
    margin-top: 3rem;
    color: var(--text-light);
    font-size: 0.9rem;
    border-top: 1px solid var(--border-color);
    background: linear-gradient(to right, var(--bg-color), var(--card-bg), var(--bg-color));
}

.footer-content {
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 1rem;
}

.footer-logo {
    font-size: 1.25rem;
    font-weight: 600;
    color: var(--primary-color);
}

.footer-links {
    display: flex;
    gap: 1.5rem;
    margin: 0.5rem 0;
}

.footer-link {
    color: var(--primary-color);
    text-decoration: none;
    font-weight: 500;
    transition: all 0.2s ease;
    position: relative;
}

.footer-link:after {
    content: '';
    position: absolute;
    width: 0;
    height: 2px;
    bottom: -4px;
    left: 0;
    background-color: var(--primary-color);
    transition: width 0.3s ease;
}

.footer-link:hover {
    color: var(--primary-dark);
}

.footer-link:hover:after {
    width: 100%;
}

.footer-copyright {
    margin-top: 0.5rem;
    color: var(--text-light);
}

/* ëª¨ë°”ì¼ ë°˜ì‘í˜• */
@media (max-width: 768px) {
    .header h1 {
        font-size: 1.75rem;
    }
    
    .header h3 {
        font-size: 1rem;
    }
    
    .container {
        padding: 1rem;
    }
    
    .footer-links {
        flex-direction: column;
        gap: 0.75rem;
    }
}

/* ë‹¤í¬ ëª¨ë“œ ì§€ì› (ì„ íƒì ) */
@media (prefers-color-scheme: dark) {
    :root {
        --primary-color: #3b82f6;
        --primary-dark: #1d4ed8;
        --primary-light: #93c5fd;
        --text-color: #f3f4f6;
        --text-light: #d1d5db;
        --bg-color: #111827;
        --card-bg: #1f2937;
        --border-color: #374151;
    }
}

/* íŠ¹ìˆ˜ íš¨ê³¼ */
.instruction-box {
    background: linear-gradient(135deg, #000000, #333333);
    color: #ffffff;
    border-radius: var(--radius-md);
    padding: 1.5rem;
    margin: 1.5rem 0;
    box-shadow: var(--shadow-md);
}

/* í˜¸ë²„ íš¨ê³¼ */
button, .gradio-checkbox, .gradio-radio, .gradio-slider, .gradio-dropdown {
    transition: transform 0.2s ease, box-shadow 0.2s ease;
}

button:hover, .gradio-checkbox:hover, .gradio-radio:hover, .gradio-slider:hover, .gradio-dropdown:hover {
    transform: translateY(-2px);
    box-shadow: var(--shadow-md);
}

/* ì…ë ¥ í•„ë“œ í¬ì»¤ìŠ¤ íš¨ê³¼ */
input:focus, textarea:focus {
    border-color: var(--primary-color) !important;
    box-shadow: 0 0 0 2px var(--primary-light) !important;
    outline: none !important;
}

/* ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ ê°œì„  */
.progress-bar {
    background: linear-gradient(90deg, var(--primary-light), var(--primary-color), var(--primary-dark));
    background-size: 200% 100%;
    animation: gradient-animation 2s ease infinite;
}

@keyframes gradient-animation {
    0% {
        background-position: 0% 50%;
    }
    50% {
        background-position: 100% 50%;
    }
    100% {
        background-position: 0% 50%;
    }
}
"""

# ì „ì—­ counter ê°ì²´
counter = ColonyCounter()

with gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:
    gr.Markdown(
        """
        <div class="header">
            <h1>ğŸ”¬ Advanced Colony Counter</h1>
            <h3>AI-Powered Colony Detection & Analysis</h3>
            <div class="header-badges">
                <span class="badge">v10</span>
                <span class="badge">AI-Powered with BDK</span>
            </div>
        </div>
        """
    )

    with gr.Tabs():
        with gr.Tab("Colony Counter"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown(
                        """<p class="image-label">ğŸ“¸ Upload Image</p>""",
                        elem_classes="image-label"
                    )
                    input_image = gr.Image(
                        label="",
                        type="pil",
                        elem_classes="input-image",
                        show_label=False,
                        show_download_button=False
                    )
                    
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì˜µì…˜
                    with gr.Row():
                        preprocess_method = gr.Radio(
                            choices=['none', 'grayscale', 'binary', 'edge', 'sharpen'],
                            value='none',
                            label="ğŸ¨ Image Preprocessing",
                            info="Select preprocessing method"
                        )
                    
                    with gr.Row():
                        preprocess_button = gr.Button(
                            "ğŸ¨ Apply Preprocessing",
                            variant="secondary",
                            scale=1
                        )
                        reset_button = gr.Button(
                            "â†º Reset to Original",
                            variant="secondary",
                            scale=1
                        )
                        redo_button = gr.Button(
                            "â†¶ Undo Preprocessing",
                            variant="secondary",
                            scale=1
                        )

                    with gr.Row():
                        method_select = gr.Radio(
                            choices=['AI Detection'],
                            value='AI Detection',
                            label="ğŸ§  Detection Method",
                            info="Advanced AI-based colony detection"
                        )
                        segment_button = gr.Button(
                            "ğŸ” Analyze Image",
                            variant="primary",
                            scale=2
                        )

                    # ë¶„ì„ ì„¤ì • ì¶”ê°€
                    with gr.Accordion("âš™ï¸ Analysis Settings", open=False):
                        with gr.Row():
                            input_size_slider = gr.Slider(
                                512, 1024, 1024,
                                step=64,
                                label='ğŸ“ Input Size',
                                info='Larger size = better accuracy but slower'
                            )
                            better_quality_checkbox = gr.Checkbox(
                                label="âœ¨ Enhanced Quality",
                                value=True,
                                info="Improve output quality"
                            )
                        
                        with gr.Row():
                            withContours_checkbox = gr.Checkbox(
                                label="ğŸ”² Show Contours",
                                value=True,
                                info="Display colony boundaries"
                            )
                            iou_threshold_slider = gr.Slider(
                                0.1, 0.9, 0.7,
                                label="ğŸ¯ IOU Threshold",
                                info="Higher = stricter detection"
                            )
                        
                        with gr.Row():
                            conf_threshold_slider = gr.Slider(
                                0.1, 0.9, 0.25,
                                label="ğŸ”’ Confidence Threshold",
                                info="Higher = more confident detection"
                            )
                            circularity_threshold_slider = gr.Slider(
                                0.0, 1.0, 0.8,
                                label="â­• Circularity",
                                info="Higher = more circular colonies"
                            )
                        
                        with gr.Row():
                            min_area_percentile_slider = gr.Slider(
                                0, 10, 1,
                                label="ğŸ“‰ Min Size %",
                                info="Filter smaller colonies"
                            )
                            max_area_percentile_slider = gr.Slider(
                                90, 100, 99,
                                label="ğŸ“ˆ Max Size %",
                                info="Filter larger objects"
                            )

                    # ì €ì¥ ê¸°ëŠ¥ UI ì¶”ê°€
                    with gr.Row():
                        save_dir = gr.Textbox(label="ì €ì¥ ê²½ë¡œ", placeholder="ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ", value="")
                        save_button = gr.Button("ê²°ê³¼ ì €ì¥", variant="primary")
                    save_result_text = gr.Textbox(label="ì €ì¥ ê²°ê³¼", value="")

                with gr.Column(scale=1):
                    gr.Markdown(
                        """<p class="image-label">ğŸ”¬ Analysis Result</p>""",
                        elem_classes="image-label"
                    )
                    output_image = gr.Image(
                        label="",
                        type="numpy",
                        interactive=True,
                        elem_classes="output-image",
                        show_label=False,
                        show_download_button=True
                    )
                    colony_count_text = gr.Textbox(
                        label="ğŸ“Š Count Result",
                        lines=3,
                        elem_classes="result-text"
                    )

                    with gr.Row():
                        with gr.Column(scale=1):
                            remove_mode_button = gr.Button(
                                "ğŸ”„ Toggle Edit Mode",
                                variant="secondary"
                            )
                            remove_mode_text = gr.Textbox(
                                label="ğŸ”¹ Current Mode",
                                value="ğŸŸ¢ Add Mode",
                                lines=1
                            )
                        with gr.Column(scale=1):
                            remove_point_button = gr.Button(
                                "â†©ï¸ Undo Last Point",
                                variant="secondary"
                            )
                            undo_removal_button = gr.Button(
                                "â™»ï¸ ì‚­ì œ ë³µì›",
                                variant="secondary"
                            )

        with gr.Tab("Batch Processing"):
            with gr.Row():
                input_dir = gr.Textbox(
                    label="ğŸ“ Input Directory",
                    placeholder="Enter the full path to the input directory",
                    lines=1
                )
                output_dir = gr.Textbox(
                    label="ğŸ“‚ Output Directory",
                    value=DEFAULT_OUTPUT_DIR,
                    info="Results will be saved here",
                    placeholder="Enter path or use default",
                    lines=1
                )
            
            with gr.Row():
                batch_process_btn = gr.Button("ğŸ” Process All Images", scale=2, variant="primary")
                open_folder_btn = gr.Button("ğŸ“‚ Open Output Folder", scale=1, variant="secondary")

            with gr.Row():
                batch_progress = gr.Textbox(
                    label="ğŸ“‹ Progress",
                    lines=10,
                    max_lines=15,
                    elem_classes="result-text"
                )
            
            with gr.Row():
                batch_gallery = gr.Gallery(
                    label="ğŸ–¼ï¸ Batch Processing Results",
                    show_label=True,
                    elem_id="gallery",
                    columns=3
                )

            with gr.Row():
                with gr.Accordion("ğŸ“‹ Batch Processing Instructions", open=False):
                    gr.Markdown("""
                    ### How to use batch processing:
                    1. Enter the full path to the input directory containing your images.
                    2. Enter the output directory path or use the default.
                    3. Click 'Process All Images' to start.
                    4. Progress will be shown in the text box below.
                    5. Results for each image will be saved in separate folders and displayed in the gallery.
                    6. A summary CSV and JSON file will be created in the output directory.
                    """)

    # Footer ì¶”ê°€
    gr.Markdown(
        """
        <div class="footer">
            <div class="footer-content">
                <div class="footer-logo">ğŸ”¬ Advanced Colony Counter</div>
                <div class="footer-links">
                    <a href="#" class="footer-link">Documentation</a>
                    <a href="#" class="footer-link">GitHub</a>
                    <a href="#" class="footer-link">Report Issues</a>
                </div>
                <div class="footer-copyright">Produced By BDK&copy; 2023</div>
            </div>
        </div>
        """
    )

    # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
    def handle_image_upload(image):
        """ì´ë¯¸ì§€ ì—…ë¡œë“œ ì²˜ë¦¬ ìµœì í™”"""
        if image is None:
            return None, None
        
        try:
            # PIL Imageë¡œ ë³€í™˜
            if not isinstance(image, Image.Image):
                image = Image.fromarray(image)
            
            # ì´ë¯¸ì§€ í¬ê¸° ì œí•œ
            max_size = 1024
            w, h = image.size
            if max(w, h) > max_size:
                scale = max_size / max(w, h)
                new_w, new_h = int(w * scale), int(h * scale)
                image = image.resize((new_w, new_h), Image.LANCZOS)
            
            # ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ì— ì €ì¥
            processed_image = image_history.set_original(image)
            return processed_image, "Image uploaded successfully"
        except Exception as e:
            print(f"Error in handle_image_upload: {str(e)}")
            return None, f"Error uploading image: {str(e)}"

    # ì „ì²˜ë¦¬ í•¨ìˆ˜ ìˆ˜ì •
    def apply_preprocessing(image, method):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì ìš©"""
        if image is None:
            return None
        
        try:
            processed = preprocess_image(image, method)
            if processed is not None:
                return image_history.add_state(processed)
            return image
        except Exception as e:
            print(f"Error in apply_preprocessing: {str(e)}")
            return image

    # Reset í•¨ìˆ˜
    def reset_image():
        """ì´ë¯¸ì§€ ì´ˆê¸°í™”"""
        try:
            return image_history.reset()
        except Exception as e:
            print(f"Error in reset_image: {str(e)}")
            return None

    # Redo í•¨ìˆ˜
    def redo_image():
        """ì´ì „ ìƒíƒœë¡œ ë³µì›"""
        try:
            return image_history.redo()
        except Exception as e:
            print(f"Error in redo_image: {str(e)}")
            return None

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
    input_image.upload(
        handle_image_upload,
        inputs=[input_image],
        outputs=[input_image, colony_count_text]
    )

    preprocess_button.click(
        apply_preprocessing,
        inputs=[input_image, preprocess_method],
        outputs=[input_image]
    )

    reset_button.click(
        reset_image,
        inputs=[],
        outputs=[input_image]
    )

    redo_button.click(
        redo_image,
        inputs=[],
        outputs=[input_image]
    )

    # analyze_image í•¨ìˆ˜ ìˆ˜ì •
    def analyze_image(
        input_image, method, input_size=1024, iou_threshold=0.7,
        conf_threshold=0.25, better_quality=True, withContours=True,
        min_area_percentile=1, max_area_percentile=99,
        circularity_threshold=0.8
    ):
        """ì´ë¯¸ì§€ ë¶„ì„ í•¨ìˆ˜ ìµœì í™”"""
        if input_image is None:
            return None, "No input image provided."

        try:
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥
            original_image = input_image
            
            # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”
            if isinstance(input_image, Image.Image):
                w, h = input_image.size
                if max(w, h) > input_size:
                    scale = input_size / max(w, h)
                    new_w, new_h = int(w * scale), int(h * scale)
                    input_image = input_image.resize((new_w, new_h), Image.LANCZOS)

            # FastSAMì„ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©
            method = 'fastsam'
            processed_image, count_text, new_counter = segment_and_count_colonies(
                input_image=input_image,
                method=method,
                input_size=input_size,
                iou_threshold=iou_threshold,
                conf_threshold=conf_threshold,
                better_quality=better_quality,
                withContours=withContours,
                mask_random_color=True,
                min_area_percentile=min_area_percentile,
                max_area_percentile=max_area_percentile,
                circularity_threshold=circularity_threshold
            )

            if new_counter is not None:
                counter.auto_points = new_counter.auto_points
                counter.auto_annotations = new_counter.auto_annotations
                counter.auto_detected_count = new_counter.auto_detected_count
                counter.current_image = new_counter.current_image
                counter.base_image = new_counter.base_image
                counter.original_image = new_counter.original_image
                counter.last_method = new_counter.last_method
                counter.scale_factor = new_counter.scale_factor

            return processed_image, count_text
        except Exception as e:
            print(f"Error in analyze_image: {str(e)}")
            return None, f"Error analyzing image: {str(e)}"

    segment_button.click(
        analyze_image,
        inputs=[
            input_image,
            method_select,
            input_size_slider,
            iou_threshold_slider,
            conf_threshold_slider,
            better_quality_checkbox,
            withContours_checkbox,
            min_area_percentile_slider,
            max_area_percentile_slider,
            circularity_threshold_slider
        ],
        outputs=[output_image, colony_count_text]
    )

    # ìˆ˜ë™ í¬ì¸íŠ¸ ì¶”ê°€/ì œê±° ì´ë²¤íŠ¸
    output_image.select(
        counter.add_or_remove_point,
        inputs=[output_image],
        outputs=[output_image, colony_count_text]
    )

    remove_point_button.click(
        counter.remove_last_point,
        inputs=[output_image],
        outputs=[output_image, colony_count_text]
    )

    remove_mode_button.click(
        counter.toggle_remove_mode,
        inputs=[],
        outputs=[output_image, remove_mode_text]
    )

    undo_removal_button.click(
        counter.undo_last_removal,
        inputs=[output_image],
        outputs=[output_image, colony_count_text]
    )

    # ì´ë¯¸ì§€ ì €ì¥ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì¶”ê°€
    def save_current_results(output_image, output_dir):
        if output_image is None:
            return "ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
        
        if not output_dir or output_dir.strip() == "":
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            output_dir = os.path.join(os.getcwd(), "results")
        
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # í˜„ì¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"result_{timestamp}"
        
        try:
            # ColonyCounterì˜ save_results ë©”ì„œë“œ ì‚¬ìš©
            save_result = counter.save_results(output_dir, filename)
            return f"ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_result}"
        except Exception as e:
            return f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    # ì €ì¥ ë²„íŠ¼ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
    save_button.click(
        save_current_results,
        inputs=[output_image, save_dir],
        outputs=[save_result_text]
    )

    # ë°°ì¹˜ ì²˜ë¦¬ ì´ë²¤íŠ¸
    def handle_batch_process(
        input_dir, output_dir, method, input_size, iou_threshold,
        conf_threshold, better_quality, withContours,
        min_area_percentile, max_area_percentile, circularity_threshold
    ):
        # batch_process ë‚´ë¶€ì—ì„œ mask_random_color=Trueë¡œ í•˜ë“œì½”ë”©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ
        # ì›í•˜ë©´ inputsì—ì„œ mask_random_colorë¥¼ ì¸ìë¡œ ë°›ì•„ ì—°ë™ ê°€ëŠ¥
        progress, gallery = batch_process(
            input_dir=input_dir,
            output_dir=output_dir,
            method=method,
            input_size=input_size,
            iou_threshold=iou_threshold,
            conf_threshold=conf_threshold,
            better_quality=better_quality,
            withContours=withContours,
            mask_random_color=True,
            min_area_percentile=min_area_percentile,
            max_area_percentile=max_area_percentile,
            circularity_threshold=circularity_threshold
        )
        return progress, gallery

    batch_process_btn.click(
        handle_batch_process,
        inputs=[
            input_dir,
            output_dir,
            method_select,
            input_size_slider,
            iou_threshold_slider,
            conf_threshold_slider,
            better_quality_checkbox,
            withContours_checkbox,
            min_area_percentile_slider,
            max_area_percentile_slider,
            circularity_threshold_slider
        ],
        outputs=[batch_progress, batch_gallery]
    )

    def open_output_folder(folder_to_open):
        try:
            if os.path.exists(folder_to_open):
                if os.name == 'nt':  # Windows
                    os.startfile(folder_to_open)
                elif os.name == 'posix':  # macOS, Linux
                    if sys.platform == 'darwin':
                        os.system(f'open "{folder_to_open}"')
                    else:
                        os.system(f'xdg-open "{folder_to_open}"')
                return f"Opened output folder: {folder_to_open}"
            else:
                return f"Output folder does not exist: {folder_to_open}"
        except Exception as e:
            return f"Error opening folder: {str(e)}"

    open_folder_btn.click(
        open_output_folder,
        inputs=[output_dir],
        outputs=[batch_progress]
    )

if __name__ == "__main__":
    demo.launch()